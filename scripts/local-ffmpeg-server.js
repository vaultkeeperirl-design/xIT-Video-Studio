import http from 'http';
import { spawn, execSync } from 'child_process';
import { createWriteStream, createReadStream, unlinkSync, mkdirSync, existsSync, writeFileSync, readFileSync, readdirSync, statSync } from 'fs';
import { join } from 'path';
import { tmpdir } from 'os';
import { randomUUID } from 'crypto';
import formidable from 'formidable';
import { GoogleGenAI } from '@google/genai';
import { fal } from '@fal-ai/client';

// Detect if running in Electron
const IS_ELECTRON = process.env.ELECTRON_RUN === 'true';

// In Electron/production, resources might be in a different place
const BASE_DIR = process.env.CWD_OVERRIDE || process.cwd();

// Get the writable directory for user data/settings
// In production (Electron), this is passed via env var USER_DATA_PATH
// In dev, we use the project root
const USER_DATA_DIR = process.env.USER_DATA_PATH || BASE_DIR;

// Load environment variables from .dev.vars
function loadEnvVars() {
  try {
    // Try user data dir first (production/writable), then base dir (dev/readonly fallback)
    let envPath = join(USER_DATA_DIR, '.dev.vars');
    let loaded = false;

    // If it doesn't exist in user data, try base dir (initial read)
    if (!existsSync(envPath) && USER_DATA_DIR !== BASE_DIR) {
      const baseEnvPath = join(BASE_DIR, '.dev.vars');
      if (existsSync(baseEnvPath)) {
        // Copy to user data dir so it becomes writable
        try {
          const content = readFileSync(baseEnvPath, 'utf-8');
          writeFileSync(envPath, content);
          console.log('[Server] Copied .dev.vars to writable user data directory');
        } catch (copyErr) {
          console.warn('[Server] Failed to copy .dev.vars to user data:', copyErr.message);
          envPath = baseEnvPath; // Fallback to reading from base
        }
      }
    }

    // Try loading from primary path
    if (existsSync(envPath)) {
      console.log('[Server] Loading settings from:', envPath);
      const content = readFileSync(envPath, 'utf-8');
      for (const line of content.split('\n')) {
        const [key, ...valueParts] = line.split('=');
        if (key && valueParts.length > 0) {
          process.env[key.trim()] = valueParts.join('=').trim();
        }
      }
      loaded = true;
    }

    // If failed and paths are different, try base path as readonly fallback
    if (!loaded && USER_DATA_DIR !== BASE_DIR) {
        const baseEnvPath = join(BASE_DIR, '.dev.vars');
        if (existsSync(baseEnvPath)) {
            console.log('[Server] Loading settings from base fallback:', baseEnvPath);
            const content = readFileSync(baseEnvPath, 'utf-8');
            for (const line of content.split('\n')) {
                const [key, ...valueParts] = line.split('=');
                if (key && valueParts.length > 0) {
                process.env[key.trim()] = valueParts.join('=').trim();
                }
            }
        } else {
            console.log('[Server] No .dev.vars found at:', envPath, 'or', baseEnvPath);
        }
    } else if (!loaded) {
        console.log('[Server] No .dev.vars found at:', envPath);
    }

  } catch (e) {
    console.warn('Could not load .dev.vars:', e.message);
  }
}
loadEnvVars();

// Configure fal.ai client - SDK expects FAL_KEY env var or credentials config
// Map FAL_API_KEY to FAL_KEY for backward compatibility
if (process.env.FAL_API_KEY && !process.env.FAL_KEY) {
  process.env.FAL_KEY = process.env.FAL_API_KEY;
}

const PORT = 3333;
const TEMP_DIR = join(tmpdir(), 'hyperedit-ffmpeg');
const SESSIONS_DIR = join(TEMP_DIR, 'sessions');

// Active video sessions - keeps videos on disk between edits
const sessions = new Map();

// Ensure temp directories exist
if (!existsSync(TEMP_DIR)) {
  mkdirSync(TEMP_DIR, { recursive: true });
}
if (!existsSync(SESSIONS_DIR)) {
  mkdirSync(SESSIONS_DIR, { recursive: true });
}

// Restore sessions from disk on server start
function restoreSessionsFromDisk() {
  console.log('[Server] Restoring sessions from disk...');
  const sessionDirs = readdirSync(SESSIONS_DIR, { withFileTypes: true })
    .filter(dirent => dirent.isDirectory())
    .map(dirent => dirent.name);

  for (const sessionId of sessionDirs) {
    const sessionDir = join(SESSIONS_DIR, sessionId);
    const assetsDir = join(sessionDir, 'assets');
    const rendersDir = join(sessionDir, 'renders');

    // Skip if assets directory doesn't exist
    if (!existsSync(assetsDir)) {
      console.log(`[Session] Skipping ${sessionId} - no assets directory`);
      continue;
    }

    // Restore project state from disk if it exists
    const projectPath = join(sessionDir, 'project.json');
    let projectState = {
      tracks: [
        { id: 'T1', type: 'text', name: 'T1', order: 0 },
        { id: 'V3', type: 'video', name: 'V3', order: 1 },
        { id: 'V2', type: 'video', name: 'V2', order: 2 },
        { id: 'V1', type: 'video', name: 'V1', order: 3 },
        { id: 'A1', type: 'audio', name: 'A1', order: 4 },
        { id: 'A2', type: 'audio', name: 'A2', order: 5 },
      ],
      clips: [],
      settings: { width: 1920, height: 1080, fps: 30 },
    };

    if (existsSync(projectPath)) {
      try {
        projectState = JSON.parse(readFileSync(projectPath, 'utf-8'));
      } catch (e) {
        console.log(`[Session] Could not read project.json for ${sessionId}`);
      }
    }

    // Restore assets from disk
    const assets = new Map();

    // Try to load saved asset metadata first
    const assetsMetaPath = join(sessionDir, 'assets-meta.json');
    let savedAssetsMeta = {};
    if (existsSync(assetsMetaPath)) {
      try {
        savedAssetsMeta = JSON.parse(readFileSync(assetsMetaPath, 'utf-8'));
        console.log(`[Session] Found saved metadata for ${Object.keys(savedAssetsMeta).length} assets`);
      } catch (e) {
        console.log(`[Session] Could not read assets-meta.json for ${sessionId}`);
      }
    }

    const assetFiles = readdirSync(assetsDir, { withFileTypes: true })
      .filter(dirent => dirent.isFile() && !dirent.name.includes('_thumb'));

    for (const assetFile of assetFiles) {
      const assetPath = join(assetsDir, assetFile.name);
      const assetId = assetFile.name.replace(/\.[^/.]+$/, ''); // Remove extension
      const ext = assetFile.name.split('.').pop().toLowerCase();

      // Determine asset type from extension
      let type = 'video';
      if (['jpg', 'jpeg', 'png', 'gif', 'webp'].includes(ext)) {
        type = 'image';
      } else if (['mp3', 'wav', 'aac', 'm4a'].includes(ext)) {
        type = 'audio';
      }

      try {
        const stats = statSync(assetPath);
        const thumbPath = join(assetsDir, `${assetId}_thumb.jpg`);

        // Merge with saved metadata if available
        const savedMeta = savedAssetsMeta[assetId] || {};

        assets.set(assetId, {
          id: assetId,
          type: savedMeta.type || type,
          filename: savedMeta.filename || assetFile.name,
          path: assetPath,
          thumbPath: existsSync(thumbPath) ? thumbPath : null,
          size: stats.size,
          createdAt: savedMeta.createdAt || stats.mtimeMs,
          // Restore AI-generated metadata
          aiGenerated: savedMeta.aiGenerated || false,
          description: savedMeta.description,
          sceneCount: savedMeta.sceneCount,
          sceneDataPath: savedMeta.sceneDataPath,
          editCount: savedMeta.editCount || 0,
          duration: savedMeta.duration,
          width: savedMeta.width,
          height: savedMeta.height,
        });

        if (savedMeta.aiGenerated) {
          console.log(`[Session] Restored AI-generated asset: ${assetFile.name}`);
        }
      } catch (e) {
        console.log(`[Session] Could not stat asset ${assetFile.name}`);
      }
    }

    if (assets.size === 0) {
      console.log(`[Session] Skipping ${sessionId} - no assets found`);
      continue;
    }

    const session = {
      id: sessionId,
      dir: sessionDir,
      assetsDir,
      rendersDir,
      currentVideo: join(sessionDir, 'current.mp4'), // Legacy
      originalName: 'Restored Project',
      createdAt: Date.now(),
      editCount: 0,
      assets,
      project: projectState,
      transcriptCache: new Map(),
    };

    sessions.set(sessionId, session);
    console.log(`[Session] Restored: ${sessionId} (${assets.size} assets)`);
  }

  console.log(`[Server] Restored ${sessions.size} sessions from disk`);
}

// Save asset metadata to disk (preserves aiGenerated flag, etc.)
function saveAssetMetadata(session) {
  if (!session || !session.dir) return;

  const assetsMetaPath = join(session.dir, 'assets-meta.json');
  const metadata = {};

  for (const [assetId, asset] of session.assets) {
    // Only save metadata that needs to persist (not paths which are reconstructed)
    metadata[assetId] = {
      type: asset.type,
      filename: asset.filename,
      createdAt: asset.createdAt,
      duration: asset.duration,
      width: asset.width,
      height: asset.height,
      // AI-generated specific metadata
      aiGenerated: asset.aiGenerated || false,
      description: asset.description,
      sceneCount: asset.sceneCount,
      sceneDataPath: asset.sceneDataPath,
      editCount: asset.editCount || 0,
    };
  }

  try {
    writeFileSync(assetsMetaPath, JSON.stringify(metadata, null, 2));
  } catch (e) {
    console.log(`[Session] Could not save assets metadata: ${e.message}`);
  }
}

// Run restoration on module load
restoreSessionsFromDisk();

// Session management
function createSession(originalName) {
  const sessionId = randomUUID();
  const sessionDir = join(SESSIONS_DIR, sessionId);
  const assetsDir = join(sessionDir, 'assets');
  const rendersDir = join(sessionDir, 'renders');

  mkdirSync(sessionDir, { recursive: true });
  mkdirSync(assetsDir, { recursive: true });
  mkdirSync(rendersDir, { recursive: true });

  // Initialize project state with all 6 tracks
  const projectState = {
    tracks: [
      { id: 'T1', type: 'text', name: 'T1', order: 0 },    // Captions/text track (top)
      { id: 'V3', type: 'video', name: 'V3', order: 1 },   // Top overlay (B-roll)
      { id: 'V2', type: 'video', name: 'V2', order: 2 },   // Overlay (GIFs)
      { id: 'V1', type: 'video', name: 'V1', order: 3 },   // Base video track
      { id: 'A1', type: 'audio', name: 'A1', order: 4 },   // Audio track 1
      { id: 'A2', type: 'audio', name: 'A2', order: 5 },   // Audio track 2
    ],
    clips: [],
    settings: {
      width: 1920,
      height: 1080,
      fps: 30,
    },
  };

  const session = {
    id: sessionId,
    dir: sessionDir,
    assetsDir,
    rendersDir,
    currentVideo: join(sessionDir, 'current.mp4'), // Legacy support
    originalName,
    createdAt: Date.now(),
    editCount: 0,
    assets: new Map(), // assetId -> asset info
    project: projectState,
    transcriptCache: new Map(), // assetId -> { text, words, cachedAt }
  };
  sessions.set(sessionId, session);
  console.log(`[Session] Created: ${sessionId}`);
  return session;
}

function getSession(sessionId) {
  return sessions.get(sessionId);
}

function cleanupSession(sessionId) {
  const session = sessions.get(sessionId);
  if (session) {
    try {
      const { rmSync } = require('fs');
      rmSync(session.dir, { recursive: true, force: true });
      sessions.delete(sessionId);
      console.log(`[Session] Cleaned up: ${sessionId}`);
    } catch (e) {
      console.error(`[Session] Cleanup error for ${sessionId}:`, e.message);
    }
  }
}

// Clean up old sessions (older than 2 hours)
setInterval(() => {
  const twoHoursAgo = Date.now() - (2 * 60 * 60 * 1000);
  for (const [id, session] of sessions) {
    if (session.createdAt < twoHoursAgo) {
      console.log(`[Session] Auto-cleaning old session: ${id}`);
      cleanupSession(id);
    }
  }
}, 30 * 60 * 1000); // Check every 30 minutes

// Run FFmpeg command and return a promise
function runFFmpeg(args, jobId) {
  return new Promise((resolve, reject) => {
    const ffmpegPath = process.env.FFMPEG_PATH || 'ffmpeg';
    const ffmpeg = spawn(ffmpegPath, args);
    let stderr = '';

    ffmpeg.stderr.on('data', (data) => {
      stderr += data.toString();
      const lines = data.toString().split('\n');
      for (const line of lines) {
        if (line.includes('time=') || line.includes('frame=')) {
          process.stdout.write(`\r[${jobId}] ${line.trim()}`);
        }
      }
    });

    ffmpeg.on('close', (code) => {
      if (code === 0) {
        resolve(stderr);
      } else {
        reject(new Error(`FFmpeg failed with code ${code}: ${stderr.slice(-500)}`));
      }
    });
    ffmpeg.on('error', reject);
  });
}

// Run FFprobe command and return stdout
function runFFmpegProbe(args, jobId) {
  return new Promise((resolve, reject) => {
    const ffprobePath = process.env.FFPROBE_PATH || 'ffprobe';
    const ffprobe = spawn(ffprobePath, args);
    let stdout = '';
    let stderr = '';

    ffprobe.stdout.on('data', (data) => {
      stdout += data.toString();
    });

    ffprobe.stderr.on('data', (data) => {
      stderr += data.toString();
    });

    ffprobe.on('close', (code) => {
      if (code === 0) {
        resolve(stdout);
      } else {
        reject(new Error(`FFprobe failed with code ${code}: ${stderr.slice(-500)}`));
      }
    });
    ffprobe.on('error', reject);
  });
}

// Detect silence in video and return silence periods
async function detectSilence(inputPath, jobId, options = {}) {
  const {
    silenceThreshold = -40, // dB
    minSilenceDuration = 0.5, // seconds
  } = options;

  console.log(`[${jobId}] Detecting silence (threshold: ${silenceThreshold}dB, min duration: ${minSilenceDuration}s)...`);

  const args = [
    '-i', inputPath,
    '-af', `silencedetect=noise=${silenceThreshold}dB:d=${minSilenceDuration}`,
    '-f', 'null',
    '-'
  ];

  const stderr = await runFFmpeg(args, jobId);

  // Parse silence detection output
  const silencePeriods = [];
  const lines = stderr.split('\n');

  let currentStart = null;
  for (const line of lines) {
    const startMatch = line.match(/silence_start:\s*([\d.]+)/);
    const endMatch = line.match(/silence_end:\s*([\d.]+)/);

    if (startMatch) {
      currentStart = parseFloat(startMatch[1]);
    }
    if (endMatch && currentStart !== null) {
      silencePeriods.push({
        start: currentStart,
        end: parseFloat(endMatch[1])
      });
      currentStart = null;
    }
  }

  console.log(`\n[${jobId}] Found ${silencePeriods.length} silence periods`);
  return silencePeriods;
}

// Get video/audio duration (returns 0 for images)
async function getVideoDuration(inputPath) {
  try {
    const ffprobePath = process.env.FFPROBE_PATH || 'ffprobe';
    const result = execSync(
      `"${ffprobePath}" -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 "${inputPath}"`,
      { encoding: 'utf-8' }
    );
    const duration = parseFloat(result.trim());
    return isNaN(duration) ? 0 : duration;
  } catch {
    return 0;
  }
}

// Calculate segments to keep (inverse of silence periods)
function calculateKeepSegments(silencePeriods, totalDuration, minSegmentDuration = 0.1) {
  if (silencePeriods.length === 0) {
    return [{ start: 0, end: totalDuration }];
  }

  const keepSegments = [];
  let lastEnd = 0;

  for (const silence of silencePeriods) {
    if (silence.start > lastEnd + minSegmentDuration) {
      keepSegments.push({
        start: lastEnd,
        end: silence.start
      });
    }
    lastEnd = silence.end;
  }

  // Add final segment if there's content after last silence
  if (lastEnd < totalDuration - minSegmentDuration) {
    keepSegments.push({
      start: lastEnd,
      end: totalDuration
    });
  }

  return keepSegments;
}

// Remove dead air from video
async function handleRemoveDeadAir(req, res) {
  const jobId = randomUUID();
  const inputPath = join(TEMP_DIR, `${jobId}-input.mp4`);
  const outputPath = join(TEMP_DIR, `${jobId}-output.mp4`);
  const concatListPath = join(TEMP_DIR, `${jobId}-concat.txt`);
  const segmentPaths = [];

  try {
    // Parse the multipart form
    const form = formidable({
      maxFileSize: 2 * 1024 * 1024 * 1024, // 2GB
      uploadDir: TEMP_DIR,
      keepExtensions: true,
    });

    const [fields, files] = await form.parse(req);

    const videoFile = files.video?.[0];
    // More aggressive defaults for "magical" dead air removal
    // -30dB catches more pauses, 0.3s cuts shorter gaps
    const silenceThreshold = parseFloat(fields.silenceThreshold?.[0] || '-30');
    const minSilenceDuration = parseFloat(fields.minSilenceDuration?.[0] || '0.3');

    if (!videoFile) {
      res.writeHead(400, { 'Content-Type': 'application/json' });
      res.end(JSON.stringify({ error: 'Missing video file' }));
      return;
    }

    // Rename uploaded file to our input path
    const { rename, stat } = await import('fs/promises');
    await rename(videoFile.filepath, inputPath);

    console.log(`\n[${jobId}] === DEAD AIR REMOVAL ===`);
    console.log(`[${jobId}] Input file size: ${(videoFile.size / 1024 / 1024).toFixed(1)} MB`);

    // Step 1: Get video duration
    const totalDuration = await getVideoDuration(inputPath);
    console.log(`[${jobId}] Video duration: ${totalDuration.toFixed(2)}s`);

    // Step 2: Detect silence
    const silencePeriods = await detectSilence(inputPath, jobId, {
      silenceThreshold,
      minSilenceDuration,
    });

    if (silencePeriods.length === 0) {
      console.log(`[${jobId}] No silence detected, returning original video`);
      // Return original video
      const outputStats = await stat(inputPath);
      res.writeHead(200, {
        'Content-Type': 'video/mp4',
        'Content-Length': outputStats.size,
        'Access-Control-Allow-Origin': '*',
      });
      createReadStream(inputPath).pipe(res);
      return;
    }

    // Step 3: Calculate segments to keep
    const keepSegments = calculateKeepSegments(silencePeriods, totalDuration);
    console.log(`[${jobId}] Keeping ${keepSegments.length} segments:`);
    keepSegments.forEach((seg, i) => {
      console.log(`[${jobId}]   Segment ${i + 1}: ${seg.start.toFixed(2)}s - ${seg.end.toFixed(2)}s (${(seg.end - seg.start).toFixed(2)}s)`);
    });

    const totalKeptDuration = keepSegments.reduce((sum, seg) => sum + (seg.end - seg.start), 0);
    const removedDuration = totalDuration - totalKeptDuration;
    console.log(`[${jobId}] Removing ${removedDuration.toFixed(2)}s of dead air (${((removedDuration / totalDuration) * 100).toFixed(1)}%)`);

    // Single-pass trim+concat filter to keep audio and video in sync
    console.log(`[${jobId}] Building filter chain for ${keepSegments.length} segments...`);

    const filterParts = [];
    const videoStreams = [];
    const audioStreams = [];

    for (let i = 0; i < keepSegments.length; i++) {
      const seg = keepSegments[i];
      filterParts.push(`[0:v]trim=start=${seg.start}:end=${seg.end},setpts=PTS-STARTPTS[v${i}]`);
      filterParts.push(`[0:a]atrim=start=${seg.start}:end=${seg.end},asetpts=PTS-STARTPTS[a${i}]`);
      videoStreams.push(`[v${i}]`);
      audioStreams.push(`[a${i}]`);
    }

    filterParts.push(`${videoStreams.join('')}concat=n=${keepSegments.length}:v=1:a=0[outv]`);
    filterParts.push(`${audioStreams.join('')}concat=n=${keepSegments.length}:v=0:a=1[outa]`);

    const filterComplex = filterParts.join(';');

    const args = [
      '-y', '-i', inputPath,
      '-filter_complex', filterComplex,
      '-map', '[outv]', '-map', '[outa]',
      '-c:v', 'libx264', '-preset', 'ultrafast', '-crf', '18',
      '-c:a', 'aac', '-b:a', '192k',
      '-movflags', '+faststart',
      outputPath
    ];

    await runFFmpeg(args, jobId);
    console.log(`\n[${jobId}] Dead air removal complete`);

    // Read output file and send it back
    const outputStats = await stat(outputPath);
    console.log(`[${jobId}] Output file size: ${(outputStats.size / 1024 / 1024).toFixed(1)} MB`);
    console.log(`[${jobId}] === DEAD AIR REMOVAL COMPLETE ===\n`);

    res.writeHead(200, {
      'Content-Type': 'video/mp4',
      'Content-Length': outputStats.size,
      'Access-Control-Allow-Origin': '*',
      'X-Removed-Duration': removedDuration.toFixed(2),
      'X-Original-Duration': totalDuration.toFixed(2),
      'X-New-Duration': totalKeptDuration.toFixed(2),
    });

    const readStream = createReadStream(outputPath);
    readStream.pipe(res);

    readStream.on('close', () => {
      // Cleanup temp files
      try {
        unlinkSync(inputPath);
        unlinkSync(outputPath);
        unlinkSync(concatListPath);
        segmentPaths.forEach(p => { try { unlinkSync(p); } catch {} });
        console.log(`[${jobId}] Cleaned up temp files`);
      } catch (e) {
        console.error(`[${jobId}] Cleanup error:`, e.message);
      }
    });

  } catch (error) {
    console.error(`[${jobId}] Error:`, error.message);

    // Cleanup on error
    try { unlinkSync(inputPath); } catch {}
    try { unlinkSync(outputPath); } catch {}
    try { unlinkSync(concatListPath); } catch {}
    segmentPaths.forEach(p => { try { unlinkSync(p); } catch {} });

    res.writeHead(500, {
      'Content-Type': 'application/json',
      'Access-Control-Allow-Origin': '*',
    });
    res.end(JSON.stringify({ error: error.message }));
  }
}

function parseFFmpegArgs(command) {
  const args = [];
  let current = '';
  let inQuotes = false;
  let quoteChar = '';

  // Remove 'ffmpeg' prefix if present
  command = command.replace(/^ffmpeg\s+/, '');

  for (let i = 0; i < command.length; i++) {
    const char = command[i];

    if ((char === '"' || char === "'") && !inQuotes) {
      inQuotes = true;
      quoteChar = char;
    } else if (char === quoteChar && inQuotes) {
      inQuotes = false;
      quoteChar = '';
    } else if (char === ' ' && !inQuotes) {
      if (current) {
        args.push(current);
        current = '';
      }
    } else {
      current += char;
    }
  }

  if (current) {
    args.push(current);
  }

  return args;
}

async function handleProcess(req, res) {
  const jobId = randomUUID();
  const inputPath = join(TEMP_DIR, `${jobId}-input.mp4`);
  const outputPath = join(TEMP_DIR, `${jobId}-output.mp4`);

  try {
    // Parse the multipart form
    const form = formidable({
      maxFileSize: 2 * 1024 * 1024 * 1024, // 2GB
      uploadDir: TEMP_DIR,
      keepExtensions: true,
    });

    const [fields, files] = await form.parse(req);

    const videoFile = files.video?.[0];
    const command = fields.command?.[0];

    if (!videoFile || !command) {
      res.writeHead(400, { 'Content-Type': 'application/json' });
      res.end(JSON.stringify({ error: 'Missing video or command' }));
      return;
    }

    // Rename uploaded file to our input path
    const { rename } = await import('fs/promises');
    await rename(videoFile.filepath, inputPath);

    console.log(`[${jobId}] Processing video with command: ${command}`);
    console.log(`[${jobId}] Input file size: ${(videoFile.size / 1024 / 1024).toFixed(1)} MB`);

    // Parse the FFmpeg command and replace input/output placeholders
    let args = parseFFmpegArgs(command);
    args = args.map(arg => {
      if (arg.match(/input\.[a-z0-9]+/i)) return inputPath;
      if (arg.match(/output\.[a-z0-9]+/i)) return outputPath;
      return arg;
    });

    // Add -y flag to overwrite output if not present
    if (!args.includes('-y')) {
      args.unshift('-y');
    }

    console.log(`[${jobId}] FFmpeg args:`, args);

    // Run FFmpeg
    const ffmpeg = spawn('ffmpeg', args);

    let stderr = '';

    ffmpeg.stderr.on('data', (data) => {
      stderr += data.toString();
      // Log progress lines
      const lines = data.toString().split('\n');
      for (const line of lines) {
        if (line.includes('time=') || line.includes('frame=')) {
          process.stdout.write(`\r[${jobId}] ${line.trim()}`);
        }
      }
    });

    await new Promise((resolve, reject) => {
      ffmpeg.on('close', (code) => {
        console.log(`\n[${jobId}] FFmpeg exited with code ${code}`);
        if (code === 0) {
          resolve();
        } else {
          reject(new Error(`FFmpeg failed with code ${code}: ${stderr.slice(-500)}`));
        }
      });
      ffmpeg.on('error', reject);
    });

    // Read output file and send it back
    const { stat } = await import('fs/promises');
    const outputStats = await stat(outputPath);
    console.log(`[${jobId}] Output file size: ${(outputStats.size / 1024 / 1024).toFixed(1)} MB`);

    res.writeHead(200, {
      'Content-Type': 'video/mp4',
      'Content-Length': outputStats.size,
      'Access-Control-Allow-Origin': '*',
    });

    const readStream = createReadStream(outputPath);
    readStream.pipe(res);

    readStream.on('close', () => {
      // Cleanup temp files
      try {
        unlinkSync(inputPath);
        unlinkSync(outputPath);
        console.log(`[${jobId}] Cleaned up temp files`);
      } catch (e) {
        console.error(`[${jobId}] Cleanup error:`, e.message);
      }
    });

  } catch (error) {
    console.error(`[${jobId}] Error:`, error.message);

    // Cleanup on error
    try { unlinkSync(inputPath); } catch {}
    try { unlinkSync(outputPath); } catch {}

    res.writeHead(500, {
      'Content-Type': 'application/json',
      'Access-Control-Allow-Origin': '*',
    });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Format seconds to YouTube timestamp format (MM:SS or HH:MM:SS)
function formatTimestamp(seconds) {
  const hrs = Math.floor(seconds / 3600);
  const mins = Math.floor((seconds % 3600) / 60);
  const secs = Math.floor(seconds % 60);

  if (hrs > 0) {
    return `${hrs}:${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
  }
  return `${mins}:${secs.toString().padStart(2, '0')}`;
}

// Generate chapters from video using AI
async function handleGenerateChapters(req, res) {
  const jobId = randomUUID();
  const inputPath = join(TEMP_DIR, `${jobId}-input.mp4`);
  const audioPath = join(TEMP_DIR, `${jobId}-audio.mp3`);

  try {
    // Check for API key
    const apiKey = process.env.GEMINI_API_KEY;
    if (!apiKey) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'GEMINI_API_KEY not configured in .dev.vars' }));
      return;
    }

    // Parse the multipart form
    const form = formidable({
      maxFileSize: 2 * 1024 * 1024 * 1024, // 2GB
      uploadDir: TEMP_DIR,
      keepExtensions: true,
    });

    const [fields, files] = await form.parse(req);

    const videoFile = files.video?.[0];
    if (!videoFile) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'Missing video file' }));
      return;
    }

    // Rename uploaded file to our input path
    const { rename, stat } = await import('fs/promises');
    await rename(videoFile.filepath, inputPath);

    console.log(`\n[${jobId}] === CHAPTER GENERATION ===`);
    console.log(`[${jobId}] Input file size: ${(videoFile.size / 1024 / 1024).toFixed(1)} MB`);

    // Step 1: Get video duration
    const totalDuration = await getVideoDuration(inputPath);
    console.log(`[${jobId}] Video duration: ${totalDuration.toFixed(2)}s`);

    // Step 2: Extract audio as MP3 (compressed for faster upload to Gemini)
    console.log(`[${jobId}] Extracting audio...`);
    const extractArgs = [
      '-y',
      '-i', inputPath,
      '-vn',                    // No video
      '-acodec', 'libmp3lame',  // MP3 codec
      '-ab', '64k',             // Lower bitrate for smaller file (speech doesn't need high quality)
      '-ar', '16000',           // 16kHz sample rate (good for speech)
      '-ac', '1',               // Mono
      audioPath
    ];
    await runFFmpeg(extractArgs, jobId);

    const audioStats = await stat(audioPath);
    console.log(`\n[${jobId}] Audio extracted: ${(audioStats.size / 1024 / 1024).toFixed(1)} MB`);

    // Step 3: Read audio file as base64
    console.log(`[${jobId}] Sending to Gemini for analysis...`);
    const audioBuffer = readFileSync(audioPath);
    const audioBase64 = audioBuffer.toString('base64');

    // Step 4: Send to Gemini for transcription and chapter analysis
    const ai = new GoogleGenAI({ apiKey });

    const response = await ai.models.generateContent({
      model: 'gemini-2.0-flash',
      contents: [
        {
          role: 'user',
          parts: [
            {
              inlineData: {
                mimeType: 'audio/mp3',
                data: audioBase64
              }
            },
            {
              text: `Analyze this audio from a video that is ${totalDuration.toFixed(1)} seconds long.

Your task is to identify logical chapter breaks based on topic changes, new sections, or natural transitions in the content.

For each chapter:
1. Identify the START timestamp (in seconds from the beginning)
2. Create a concise, descriptive title (2-6 words)

Guidelines:
- First chapter should always start at 0 seconds
- Aim for 3-8 chapters depending on content length and topic diversity
- Chapters should be at least 30 seconds apart
- Titles should be engaging and descriptive (good for YouTube)
- If the content is a tutorial, use action-oriented titles
- If it's a discussion, summarize the main topic of each section

Return your response as valid JSON with exactly this structure:
{
  "chapters": [
    { "start": 0, "title": "Introduction" },
    { "start": 45.5, "title": "Getting Started" },
    { "start": 120, "title": "Main Topic" }
  ],
  "summary": "Brief 1-2 sentence summary of the video content"
}

Only return the JSON, no other text.`
            }
          ]
        }
      ],
      config: {
        responseMimeType: 'application/json',
      }
    });

    const responseText = response.text || '{}';
    console.log(`[${jobId}] Gemini response received`);

    let result;
    try {
      result = JSON.parse(responseText);
    } catch {
      // Try to extract JSON from response
      const jsonMatch = responseText.match(/\{[\s\S]*\}/);
      result = jsonMatch ? JSON.parse(jsonMatch[0]) : { chapters: [], summary: 'Failed to parse response' };
    }

    // Format chapters for YouTube
    const youtubeChapters = (result.chapters || [])
      .sort((a, b) => a.start - b.start)
      .map(ch => `${formatTimestamp(ch.start)} ${ch.title}`)
      .join('\n');

    console.log(`[${jobId}] Generated ${result.chapters?.length || 0} chapters`);
    console.log(`[${jobId}] === CHAPTER GENERATION COMPLETE ===\n`);

    // Return the chapters
    res.writeHead(200, {
      'Content-Type': 'application/json',
      'Access-Control-Allow-Origin': '*',
    });
    res.end(JSON.stringify({
      success: true,
      chapters: result.chapters || [],
      youtubeFormat: youtubeChapters,
      summary: result.summary || '',
      videoDuration: totalDuration,
    }));

    // Cleanup
    try {
      unlinkSync(inputPath);
      unlinkSync(audioPath);
      console.log(`[${jobId}] Cleaned up temp files`);
    } catch (e) {
      console.error(`[${jobId}] Cleanup error:`, e.message);
    }

  } catch (error) {
    console.error(`[${jobId}] Error:`, error.message);

    // Cleanup on error
    try { unlinkSync(inputPath); } catch {}
    try { unlinkSync(audioPath); } catch {}

    res.writeHead(500, {
      'Content-Type': 'application/json',
      'Access-Control-Allow-Origin': '*',
    });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// ============== SESSION-BASED HANDLERS ==============
// These keep videos on disk between edits for efficient large file handling

// Create a new empty session (for multi-asset workflow)
async function handleSessionCreate(req, res) {
  try {
    const session = createSession('Untitled Project');

    console.log(`[${session.id}] Empty session created`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      sessionId: session.id,
    }));

  } catch (error) {
    console.error('[Create] Error:', error.message);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Upload video and create a session
async function handleSessionUpload(req, res) {
  try {
    const form = formidable({
      maxFileSize: 10 * 1024 * 1024 * 1024, // 10GB limit
      uploadDir: TEMP_DIR,
      keepExtensions: true,
    });

    const [fields, files] = await form.parse(req);
    const videoFile = files.video?.[0];

    if (!videoFile) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'Missing video file' }));
      return;
    }

    // Create session and move file
    const session = createSession(videoFile.originalFilename || 'video.mp4');
    const { rename, stat } = await import('fs/promises');
    await rename(videoFile.filepath, session.currentVideo);

    const duration = await getVideoDuration(session.currentVideo);
    const stats = await stat(session.currentVideo);

    console.log(`[${session.id}] Video uploaded: ${(stats.size / 1024 / 1024).toFixed(1)} MB, ${duration.toFixed(2)}s`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      sessionId: session.id,
      duration,
      size: stats.size,
      name: session.originalName,
    }));

  } catch (error) {
    console.error('[Upload] Error:', error.message);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Stream video for preview (supports range requests for seeking)
async function handleSessionStream(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  try {
    const { stat } = await import('fs/promises');
    const stats = await stat(session.currentVideo);
    const fileSize = stats.size;

    const range = req.headers.range;

    if (range) {
      // Handle range request for video seeking
      const parts = range.replace(/bytes=/, '').split('-');
      const start = parseInt(parts[0], 10);
      const end = parts[1] ? parseInt(parts[1], 10) : fileSize - 1;
      const chunkSize = end - start + 1;

      res.writeHead(206, {
        'Content-Range': `bytes ${start}-${end}/${fileSize}`,
        'Accept-Ranges': 'bytes',
        'Content-Length': chunkSize,
        'Content-Type': 'video/mp4',
        'Access-Control-Allow-Origin': '*',
      });

      createReadStream(session.currentVideo, { start, end }).pipe(res);
    } else {
      res.writeHead(200, {
        'Content-Length': fileSize,
        'Content-Type': 'video/mp4',
        'Access-Control-Allow-Origin': '*',
      });
      createReadStream(session.currentVideo).pipe(res);
    }
  } catch (error) {
    console.error(`[${sessionId}] Stream error:`, error.message);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Get session info
async function handleSessionInfo(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  try {
    const { stat } = await import('fs/promises');
    const stats = await stat(session.currentVideo);
    const duration = await getVideoDuration(session.currentVideo);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      sessionId: session.id,
      duration,
      size: stats.size,
      name: session.originalName,
      editCount: session.editCount,
      createdAt: session.createdAt,
    }));
  } catch (error) {
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Process video within a session (edit in place)
async function handleSessionProcess(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  try {
    // Parse JSON body
    let body = '';
    for await (const chunk of req) body += chunk;
    const { command } = JSON.parse(body);

    if (!command) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'Missing command' }));
      return;
    }

    const outputPath = join(session.dir, `output-${Date.now()}.mp4`);

    console.log(`\n[${sessionId}] Processing: ${command}`);

    // Parse and prepare FFmpeg command
    let args = parseFFmpegArgs(command);
    args = args.map(arg => {
      if (arg.match(/input\.[a-z0-9]+/i)) return session.currentVideo;
      if (arg.match(/output\.[a-z0-9]+/i)) return outputPath;
      return arg;
    });

    if (!args.includes('-y')) args.unshift('-y');

    console.log(`[${sessionId}] FFmpeg args:`, args.slice(0, 10).join(' '), '...');

    await runFFmpeg(args, sessionId);

    // Replace current video with output
    const { rename, stat } = await import('fs/promises');
    unlinkSync(session.currentVideo);
    await rename(outputPath, session.currentVideo);

    const newStats = await stat(session.currentVideo);
    const newDuration = await getVideoDuration(session.currentVideo);
    session.editCount++;

    console.log(`\n[${sessionId}] Edit complete. New duration: ${newDuration.toFixed(2)}s, Size: ${(newStats.size / 1024 / 1024).toFixed(1)} MB`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      duration: newDuration,
      size: newStats.size,
      editCount: session.editCount,
    }));

  } catch (error) {
    console.error(`[${sessionId}] Process error:`, error.message);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Remove dead air within a session
async function handleSessionRemoveDeadAir(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  const jobId = sessionId;
  const outputPath = join(session.dir, `deadair-output-${Date.now()}.mp4`);
  const concatListPath = join(session.dir, `concat-${Date.now()}.txt`);
  const segmentPaths = [];

  try {
    // Parse options from body
    let body = '';
    for await (const chunk of req) body += chunk;
    const options = body ? JSON.parse(body) : {};

    const silenceThreshold = options.silenceThreshold || -30;
    const minSilenceDuration = options.minSilenceDuration || 0.3;

    console.log(`\n[${jobId}] === DEAD AIR REMOVAL (Session) ===`);

    // Find the original (non-AI-generated) video asset
    let videoAsset = null;
    for (const [assetId, asset] of session.assets) {
      if (asset.type === 'video' && !asset.aiGenerated) {
        videoAsset = asset;
        break;
      }
    }
    // Fallback to any video if no original found
    if (!videoAsset) {
      for (const [assetId, asset] of session.assets) {
        if (asset.type === 'video') {
          videoAsset = asset;
          break;
        }
      }
    }

    if (!videoAsset) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'No video asset found in session. Please upload a video first.' }));
      return;
    }

    // Verify the video file exists on disk
    if (!existsSync(videoAsset.path)) {
      console.error(`[${jobId}] Video file missing: ${videoAsset.path}`);
      res.writeHead(410, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({
        error: 'Video file no longer exists. Your session may have expired. Please re-upload your video.',
        code: 'VIDEO_FILE_MISSING'
      }));
      return;
    }

    console.log(`[${jobId}] Using video asset: ${videoAsset.filename} (${videoAsset.path})`);

    const totalDuration = await getVideoDuration(videoAsset.path);
    console.log(`[${jobId}] Video duration: ${totalDuration.toFixed(2)}s`);

    const silencePeriods = await detectSilence(videoAsset.path, jobId, {
      silenceThreshold,
      minSilenceDuration,
    });

    if (silencePeriods.length === 0) {
      console.log(`[${jobId}] No silence detected`);
      res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({
        success: true,
        duration: totalDuration,
        removedDuration: 0,
        message: 'No silence detected',
      }));
      return;
    }

    const keepSegments = calculateKeepSegments(silencePeriods, totalDuration);
    console.log(`[${jobId}] Keeping ${keepSegments.length} segments`);

    const totalKeptDuration = keepSegments.reduce((sum, seg) => sum + (seg.end - seg.start), 0);
    const removedDuration = totalDuration - totalKeptDuration;
    console.log(`[${jobId}] Removing ${removedDuration.toFixed(2)}s of dead air (${((removedDuration / totalDuration) * 100).toFixed(1)}%)`);

    // Extract segments
    console.log(`[${jobId}] Extracting segments...`);
    for (let i = 0; i < keepSegments.length; i++) {
      const seg = keepSegments[i];
      const segmentPath = join(session.dir, `segment-${Date.now()}-${i}.mp4`);
      segmentPaths.push(segmentPath);

      const args = [
        '-y', '-i', videoAsset.path,
        '-ss', seg.start.toString(),
        '-t', (seg.end - seg.start).toString(),
        '-c:v', 'libx264', '-preset', 'ultrafast', '-crf', '18',
        '-c:a', 'aac', '-b:a', '192k',
        segmentPath
      ];

      await runFFmpeg(args, jobId);
      console.log(`\n[${jobId}] Segment ${i + 1}/${keepSegments.length}`);
    }

    // Concatenate
    const concatList = segmentPaths.map(p => `file '${p}'`).join('\n');
    writeFileSync(concatListPath, concatList);

    console.log(`[${jobId}] Concatenating...`);
    await runFFmpeg(['-y', '-f', 'concat', '-safe', '0', '-i', concatListPath, '-c', 'copy', '-movflags', '+faststart', outputPath], jobId);

    console.log(`\n[${jobId}] Dead air removal complete`);

    // Verify output has audio before replacing original
    const ffprobePath = process.env.FFPROBE_PATH || 'ffprobe';
    const probeResult = execSync(
      `"${ffprobePath}" -v error -show_entries stream=codec_type -of csv=p=0 "${outputPath}"`,
      { encoding: 'utf-8' }
    );
    const streams = probeResult.trim().split('\n');
    console.log(`\nðŸ” [${jobId}] OUTPUT FILE PROBE:`);
    console.log(`ðŸ” [${jobId}]   Streams: ${streams.join(', ')}`);
    console.log(`ðŸ” [${jobId}]   Has video: ${streams.includes('video')}`);
    console.log(`ðŸ” [${jobId}]   Has audio: ${streams.includes('audio')}`);
    console.log(`ðŸ” [${jobId}]   Output path: ${outputPath}`);

    // Also probe the ORIGINAL file for comparison
    const origProbe = execSync(
      `"${ffprobePath}" -v error -show_entries stream=codec_type -of csv=p=0 "${videoAsset.path}"`,
      { encoding: 'utf-8' }
    );
    console.log(`ðŸ” [${jobId}]   Original streams: ${origProbe.trim().split('\n').join(', ')}`);

    // Cleanup segments
    segmentPaths.forEach(p => { try { unlinkSync(p); } catch {} });
    try { unlinkSync(concatListPath); } catch {}

    // Replace the video asset file
    const { rename, stat } = await import('fs/promises');
    unlinkSync(videoAsset.path);
    await rename(outputPath, videoAsset.path);

    // Cleanup segments
    segmentPaths.forEach(p => { try { unlinkSync(p); } catch {} });
    try { unlinkSync(concatListPath); } catch {}

    const newStats = await stat(videoAsset.path);

    // Update the video asset metadata
    videoAsset.duration = totalKeptDuration;
    videoAsset.size = newStats.size;

    session.editCount++;

    console.log(`\n[${jobId}] === DEAD AIR REMOVAL COMPLETE ===`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      duration: totalKeptDuration,
      originalDuration: totalDuration,
      removedDuration,
      size: newStats.size,
      editCount: session.editCount,
    }));

  } catch (error) {
    console.error(`[${jobId}] Error:`, error.message);
    segmentPaths.forEach(p => { try { unlinkSync(p); } catch {} });
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Generate chapters for a session
async function handleSessionChapters(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  const jobId = sessionId;
  const audioPath = join(session.dir, `audio-${Date.now()}.mp3`);

  try {
    const apiKey = process.env.GEMINI_API_KEY;
    if (!apiKey) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'GEMINI_API_KEY not configured' }));
      return;
    }

    console.log(`\n[${jobId}] === CHAPTER GENERATION (Session) ===`);

    // Find video path - check both legacy currentVideo and new assets system
    let videoPath = session.currentVideo;
    if (!videoPath || !existsSync(videoPath)) {
      // Try to find original (non-AI) video from assets
      if (session.assets && session.assets.size > 0) {
        for (const [, asset] of session.assets) {
          if (asset.type === 'video' && !asset.aiGenerated && existsSync(asset.path)) {
            videoPath = asset.path;
            console.log(`[${jobId}] Using video asset: ${asset.filename}`);
            break;
          }
        }
        // Fallback to any video
        if (!videoPath || !existsSync(videoPath)) {
          for (const [, asset] of session.assets) {
            if (asset.type === 'video' && existsSync(asset.path)) {
              videoPath = asset.path;
              console.log(`[${jobId}] Using video asset (fallback): ${asset.filename}`);
              break;
            }
          }
        }
      }
    }

    if (!videoPath || !existsSync(videoPath)) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'No video found in session. Please upload a video first.' }));
      return;
    }

    const totalDuration = await getVideoDuration(videoPath);

    // Extract audio
    console.log(`[${jobId}] Extracting audio from: ${videoPath}`);
    await runFFmpeg(['-y', '-i', videoPath, '-vn', '-acodec', 'libmp3lame', '-ab', '64k', '-ar', '16000', '-ac', '1', audioPath], jobId);

    const { stat } = await import('fs/promises');
    const audioStats = await stat(audioPath);
    console.log(`\n[${jobId}] Audio: ${(audioStats.size / 1024 / 1024).toFixed(1)} MB`);

    // Send to Gemini
    console.log(`[${jobId}] Analyzing with Gemini...`);
    const audioBuffer = readFileSync(audioPath);
    const audioBase64 = audioBuffer.toString('base64');

    const ai = new GoogleGenAI({ apiKey });
    const response = await ai.models.generateContent({
      model: 'gemini-2.0-flash',
      contents: [{
        role: 'user',
        parts: [
          { inlineData: { mimeType: 'audio/mp3', data: audioBase64 } },
          { text: `Analyze this audio from a video that is ${totalDuration.toFixed(1)} seconds long.

Identify logical chapter breaks based on topic changes or natural transitions.

For each chapter:
1. START timestamp (seconds from beginning)
2. Concise, descriptive title (2-6 words)

Guidelines:
- First chapter starts at 0
- Aim for 3-8 chapters
- At least 30 seconds apart
- Engaging titles for YouTube

Return JSON: {"chapters": [{"start": 0, "title": "Introduction"}], "summary": "Brief summary"}` }
        ]
      }],
      config: { responseMimeType: 'application/json' }
    });

    // Get the response text - handle different SDK versions
    let responseText = '';
    if (typeof response.text === 'function') {
      responseText = await response.text();
    } else if (response.text) {
      responseText = response.text;
    } else if (response.candidates && response.candidates[0]?.content?.parts?.[0]?.text) {
      responseText = response.candidates[0].content.parts[0].text;
    }

    console.log(`[${jobId}] Gemini response:`, responseText.substring(0, 500));

    let result;
    try {
      result = JSON.parse(responseText || '{}');
    } catch {
      const match = (responseText || '').match(/\{[\s\S]*\}/);
      result = match ? JSON.parse(match[0]) : { chapters: [], summary: '' };
    }

    // If no chapters detected, create automatic chapters based on duration
    if (!result.chapters || result.chapters.length === 0) {
      console.log(`[${jobId}] No chapters from AI, creating automatic chapters...`);

      // Create chapters every ~60 seconds, or split into 4-6 sections
      const chapterInterval = Math.max(30, Math.min(90, totalDuration / 5));
      const autoChapters = [];

      for (let time = 0; time < totalDuration - 10; time += chapterInterval) {
        const chapterNum = autoChapters.length + 1;
        autoChapters.push({
          start: Math.round(time * 10) / 10,
          title: time === 0 ? 'Introduction' : `Part ${chapterNum}`
        });
      }

      result.chapters = autoChapters;
      result.summary = 'Auto-generated chapters based on video duration';
      console.log(`[${jobId}] Created ${autoChapters.length} automatic chapters`);
    }

    const youtubeChapters = (result.chapters || [])
      .sort((a, b) => a.start - b.start)
      .map(ch => `${formatTimestamp(ch.start)} ${ch.title}`)
      .join('\n');

    // Cleanup
    try { unlinkSync(audioPath); } catch {}

    console.log(`[${jobId}] Generated ${result.chapters?.length || 0} chapters`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      chapters: result.chapters || [],
      youtubeFormat: youtubeChapters,
      summary: result.summary || '',
      videoDuration: totalDuration,
    }));

  } catch (error) {
    console.error(`[${jobId}] Error:`, error.message);
    try { unlinkSync(audioPath); } catch {}
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Download final video
async function handleSessionDownload(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  try {
    const { stat } = await import('fs/promises');
    const stats = await stat(session.currentVideo);

    const filename = session.originalName.replace(/\.[^.]+$/, '-edited.mp4');

    res.writeHead(200, {
      'Content-Type': 'video/mp4',
      'Content-Length': stats.size,
      'Content-Disposition': `attachment; filename="${filename}"`,
      'Access-Control-Allow-Origin': '*',
    });

    createReadStream(session.currentVideo).pipe(res);
    console.log(`[${sessionId}] Downloading: ${filename}`);

  } catch (error) {
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Delete session
function handleSessionDelete(req, res, sessionId) {
  cleanupSession(sessionId);
  res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
  res.end(JSON.stringify({ success: true }));
}

// ============== MULTI-ASSET HANDLERS ==============

// Generate thumbnail for video/image asset
async function generateThumbnail(inputPath, outputPath, isImage = false) {
  if (isImage) {
    // For images, just resize
    const args = [
      '-y', '-i', inputPath,
      '-vf', 'scale=160:90:force_original_aspect_ratio=decrease,pad=160:90:(ow-iw)/2:(oh-ih)/2',
      '-frames:v', '1',
      outputPath
    ];
    await runFFmpeg(args, 'thumb');
  } else {
    // For videos, extract frame at 1 second or 10% of duration
    const duration = await getVideoDuration(inputPath);
    const seekTime = Math.min(1, duration * 0.1);
    const args = [
      '-y', '-ss', seekTime.toString(),
      '-i', inputPath,
      '-vf', 'scale=160:90:force_original_aspect_ratio=decrease,pad=160:90:(ow-iw)/2:(oh-ih)/2',
      '-frames:v', '1',
      outputPath
    ];
    await runFFmpeg(args, 'thumb');
  }
}

// Get video/image dimensions
async function getMediaInfo(inputPath) {
  try {
    const ffprobePath = process.env.FFPROBE_PATH || 'ffprobe';
    const result = execSync(
      `"${ffprobePath}" -v error -select_streams v:0 -show_entries stream=width,height,duration -of json "${inputPath}"`,
      { encoding: 'utf-8' }
    );
    const info = JSON.parse(result);
    const stream = info.streams?.[0] || {};
    return {
      width: stream.width || 0,
      height: stream.height || 0,
      duration: parseFloat(stream.duration) || 0,
    };
  } catch {
    return { width: 0, height: 0, duration: 0 };
  }
}

// Upload asset to session
async function handleAssetUpload(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  try {
    const form = formidable({
      maxFileSize: 10 * 1024 * 1024 * 1024, // 10GB
      uploadDir: session.assetsDir,
      keepExtensions: true,
    });

    const [fields, files] = await form.parse(req);
    const uploadedFile = files.file?.[0] || files.video?.[0];

    if (!uploadedFile) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'Missing file' }));
      return;
    }

    const assetId = randomUUID();
    const originalName = uploadedFile.originalFilename || 'file';
    const ext = originalName.split('.').pop()?.toLowerCase() || 'mp4';
    const isImage = ['jpg', 'jpeg', 'png', 'gif', 'webp'].includes(ext);
    const isAudio = ['mp3', 'wav', 'aac', 'm4a', 'ogg'].includes(ext);
    const type = isImage ? 'image' : isAudio ? 'audio' : 'video';

    // Move file to proper location
    const assetPath = join(session.assetsDir, `${assetId}.${ext}`);
    const thumbPath = join(session.assetsDir, `${assetId}_thumb.jpg`);

    const { rename, stat } = await import('fs/promises');
    await rename(uploadedFile.filepath, assetPath);

    // Get media info
    let duration = 0;
    let width = 0;
    let height = 0;

    if (!isAudio) {
      const info = await getMediaInfo(assetPath);
      duration = info.duration;
      width = info.width;
      height = info.height;
    } else {
      duration = await getVideoDuration(assetPath);
    }

    // Generate thumbnail (for video/image)
    if (!isAudio) {
      try {
        await generateThumbnail(assetPath, thumbPath, isImage);
      } catch (e) {
        console.warn(`[${sessionId}] Thumbnail generation failed:`, e.message);
      }
    }

    const stats = await stat(assetPath);

    const asset = {
      id: assetId,
      type,
      filename: originalName,
      path: assetPath,
      thumbPath: existsSync(thumbPath) ? thumbPath : null,
      duration: isImage ? 5 : duration, // Default 5s for images
      size: stats.size,
      width,
      height,
      createdAt: Date.now(),
    };

    session.assets.set(assetId, asset);
    saveAssetMetadata(session); // Persist asset metadata to disk

    console.log(`[${sessionId}] Asset uploaded: ${assetId} (${type}, ${(stats.size / 1024 / 1024).toFixed(1)} MB)`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      asset: {
        id: asset.id,
        type: asset.type,
        filename: asset.filename,
        duration: asset.duration,
        size: asset.size,
        width: asset.width,
        height: asset.height,
        thumbnailUrl: asset.thumbPath ? `/session/${sessionId}/assets/${assetId}/thumbnail` : null,
      },
    }));

  } catch (error) {
    console.error(`[${sessionId}] Asset upload error:`, error.message);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// List all assets in session
function handleAssetList(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  const assets = Array.from(session.assets.values()).map(asset => ({
    id: asset.id,
    type: asset.type,
    filename: asset.filename,
    duration: asset.duration,
    size: asset.size,
    width: asset.width,
    height: asset.height,
    thumbnailUrl: asset.thumbPath ? `/session/${sessionId}/assets/${asset.id}/thumbnail` : null,
    aiGenerated: asset.aiGenerated || false, // True for Remotion-generated animations
  }));

  res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
  res.end(JSON.stringify({ assets }));
}

// Delete asset
function handleAssetDelete(req, res, sessionId, assetId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  const asset = session.assets.get(assetId);
  if (!asset) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Asset not found' }));
    return;
  }

  // Remove files
  try {
    if (existsSync(asset.path)) unlinkSync(asset.path);
    if (asset.thumbPath && existsSync(asset.thumbPath)) unlinkSync(asset.thumbPath);
  } catch (e) {
    console.warn(`[${sessionId}] Asset file cleanup failed:`, e.message);
  }

  // Remove from session
  session.assets.delete(assetId);
  saveAssetMetadata(session); // Update metadata file

  // Remove any clips using this asset
  session.project.clips = session.project.clips.filter(clip => clip.assetId !== assetId);

  console.log(`[${sessionId}] Asset deleted: ${assetId}`);

  res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
  res.end(JSON.stringify({ success: true }));
}

// Get asset thumbnail
async function handleAssetThumbnail(req, res, sessionId, assetId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  const asset = session.assets.get(assetId);
  if (!asset || !asset.thumbPath || !existsSync(asset.thumbPath)) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Thumbnail not found' }));
    return;
  }

  const { stat } = await import('fs/promises');
  const stats = await stat(asset.thumbPath);

  res.writeHead(200, {
    'Content-Type': 'image/jpeg',
    'Content-Length': stats.size,
    'Cache-Control': 'public, max-age=3600',
    'Access-Control-Allow-Origin': '*',
  });

  createReadStream(asset.thumbPath).pipe(res);
}

// Stream asset
async function handleAssetStream(req, res, sessionId, assetId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  const asset = session.assets.get(assetId);
  if (!asset || !existsSync(asset.path)) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Asset not found' }));
    return;
  }

  const { stat } = await import('fs/promises');
  const stats = await stat(asset.path);
  const fileSize = stats.size;

  // Get proper MIME type for the asset
  const getContentType = () => {
    if (asset.type === 'image') {
      const ext = asset.path.split('.').pop()?.toLowerCase();
      const mimeTypes = {
        'jpg': 'image/jpeg',
        'jpeg': 'image/jpeg',
        'png': 'image/png',
        'gif': 'image/gif',
        'webp': 'image/webp',
        'svg': 'image/svg+xml',
      };
      return mimeTypes[ext] || 'image/jpeg';
    }
    if (asset.type === 'audio') {
      const ext = asset.path.split('.').pop()?.toLowerCase();
      const mimeTypes = {
        'mp3': 'audio/mpeg',
        'wav': 'audio/wav',
        'ogg': 'audio/ogg',
        'm4a': 'audio/mp4',
        'aac': 'audio/aac',
      };
      return mimeTypes[ext] || 'audio/mpeg';
    }
    return 'video/mp4';
  };
  const contentType = getContentType();

  const range = req.headers.range;

  if (range) {
    const parts = range.replace(/bytes=/, '').split('-');
    let start = parseInt(parts[0], 10);
    let end = parts[1] ? parseInt(parts[1], 10) : fileSize - 1;

    // Clamp values to valid range (prevents crash if file size changed)
    if (start >= fileSize) {
      // Requested range is completely outside file - return 416
      res.writeHead(416, {
        'Content-Range': `bytes */${fileSize}`,
        'Access-Control-Allow-Origin': '*',
      });
      res.end();
      return;
    }
    if (end >= fileSize) {
      end = fileSize - 1;
    }
    if (start > end) {
      start = end;
    }

    const chunkSize = end - start + 1;

    res.writeHead(206, {
      'Content-Range': `bytes ${start}-${end}/${fileSize}`,
      'Accept-Ranges': 'bytes',
      'Content-Length': chunkSize,
      'Content-Type': contentType,
      'Access-Control-Allow-Origin': '*',
    });

    createReadStream(asset.path, { start, end }).pipe(res);
  } else {
    res.writeHead(200, {
      'Content-Length': fileSize,
      'Content-Type': contentType,
      'Access-Control-Allow-Origin': '*',
    });
    createReadStream(asset.path).pipe(res);
  }
}

// Get project state
function handleProjectGet(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  // Verify the session directory still exists on disk
  if (!existsSync(session.dir)) {
    console.log(`[Session] Directory missing for ${sessionId}, cleaning up`);
    sessions.delete(sessionId);
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session files no longer exist' }));
    return;
  }

  res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
  res.end(JSON.stringify({
    tracks: session.project.tracks,
    clips: session.project.clips,
    settings: session.project.settings,
  }));
}

// Save project state
async function handleProjectSave(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  try {
    let body = '';
    for await (const chunk of req) body += chunk;
    const data = JSON.parse(body);

    if (data.tracks) session.project.tracks = data.tracks;
    if (data.clips) session.project.clips = data.clips;
    if (data.settings) session.project.settings = { ...session.project.settings, ...data.settings };

    // Save to disk for persistence
    const projectPath = join(session.dir, 'project.json');
    writeFileSync(projectPath, JSON.stringify(session.project, null, 2));

    console.log(`[${sessionId}] Project saved: ${session.project.clips.length} clips`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ success: true }));

  } catch (error) {
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Render project to video
async function handleProjectRender(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  try {
    let body = '';
    for await (const chunk of req) body += chunk;
    const options = body ? JSON.parse(body) : {};
    const isPreview = options.preview === true;

    const clips = session.project.clips;
    const settings = session.project.settings;

    if (clips.length === 0) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'No clips in timeline' }));
      return;
    }

    console.log(`\n[${sessionId}] === RENDER ${isPreview ? 'PREVIEW' : 'EXPORT'} ===`);
    console.log(`[${sessionId}] ${clips.length} clips, ${settings.width}x${settings.height}`);

    // Sort clips by track for layering (V1 first, then V2, etc.)
    const videoClips = clips
      .filter(c => session.assets.get(c.assetId)?.type !== 'audio')
      .sort((a, b) => {
        const trackOrder = { 'V1': 0, 'V2': 1, 'V3': 2 };
        return (trackOrder[a.trackId] || 0) - (trackOrder[b.trackId] || 0);
      });

    const audioClips = clips
      .filter(c => session.assets.get(c.assetId)?.type === 'audio');

    // Calculate total duration from all clips
    const totalDuration = Math.max(
      ...clips.map(c => c.start + c.duration),
      0.1
    );

    // Build FFmpeg filter_complex
    const inputs = [];
    const filterParts = [];
    let inputIndex = 0;

    // Create black background
    filterParts.push(`color=black:s=${settings.width}x${settings.height}:d=${totalDuration}:r=${settings.fps}[base]`);
    let lastVideo = 'base';

    // Process video clips
    for (const clip of videoClips) {
      const asset = session.assets.get(clip.assetId);
      if (!asset) continue;

      inputs.push('-i', asset.path);
      const idx = inputIndex++;

      // Apply trim and scale
      const inPoint = clip.inPoint || 0;
      const outPoint = clip.outPoint || asset.duration;
      const trimDuration = outPoint - inPoint;

      let clipFilter = `[${idx}:v]`;

      // Trim
      clipFilter += `trim=${inPoint}:${outPoint},setpts=PTS-STARTPTS,`;

      // Scale/fit to canvas
      clipFilter += `scale=${settings.width}:${settings.height}:force_original_aspect_ratio=decrease,`;
      clipFilter += `pad=${settings.width}:${settings.height}:(ow-iw)/2:(oh-ih)/2`;

      // Apply transform if present
      if (clip.transform) {
        const { x = 0, y = 0, scale = 1, opacity = 1 } = clip.transform;
        if (scale !== 1) {
          clipFilter += `,scale=iw*${scale}:ih*${scale}`;
        }
        // Opacity is handled in overlay
      }

      clipFilter += `[v${idx}]`;
      filterParts.push(clipFilter);

      // Overlay onto base
      const overlayX = clip.transform?.x || `(W-w)/2`;
      const overlayY = clip.transform?.y || `(H-h)/2`;
      const enable = `between(t,${clip.start},${clip.start + trimDuration})`;

      filterParts.push(`[${lastVideo}][v${idx}]overlay=x=${overlayX}:y=${overlayY}:enable='${enable}'[out${idx}]`);
      lastVideo = `out${idx}`;
    }

    // Rename final output
    filterParts.push(`[${lastVideo}]copy[vout]`);

    // Audio mixing
    let audioFilter = '';
    if (audioClips.length > 0) {
      const audioInputs = [];
      for (const clip of audioClips) {
        const asset = session.assets.get(clip.assetId);
        if (!asset) continue;

        inputs.push('-i', asset.path);
        const idx = inputIndex++;
        const inPoint = clip.inPoint || 0;
        const outPoint = clip.outPoint || asset.duration;

        audioInputs.push(`[${idx}:a]atrim=${inPoint}:${outPoint},asetpts=PTS-STARTPTS,adelay=${Math.floor(clip.start * 1000)}|${Math.floor(clip.start * 1000)}[a${idx}]`);
      }

      if (audioInputs.length > 0) {
        filterParts.push(...audioInputs);
        const audioMix = audioInputs.map((_, i) => `[a${clips.indexOf(audioClips[i]) + videoClips.length}]`).join('');
        filterParts.push(`${audioMix}amix=inputs=${audioInputs.length}[aout]`);
        audioFilter = '-map [aout]';
      }
    }

    // Handle vertical export (9:16)
    // Add a final scaling filter to the output chain
    let finalFilterComplex = filterParts.join(';');
    if (options.isVertical) {
      // Add scale and crop filters to the video output
      // Scale to cover 1080x1920 (force_original_aspect_ratio=increase)
      // Then crop to exactly 1080x1920
      finalFilterComplex += `;[vout]scale=1080:1920:force_original_aspect_ratio=increase,crop=1080:1920[vout_vertical]`;
    }

    // Build final command
    const outputPath = join(session.rendersDir, isPreview ? 'preview.mp4' : `export-${Date.now()}.mp4`);

    const ffmpegArgs = [
      '-y',
      ...inputs,
      '-filter_complex', finalFilterComplex,
      '-map', options.isVertical ? '[vout_vertical]' : '[vout]',
    ];

    if (audioFilter) {
      ffmpegArgs.push('-map', '[aout]');
    }

    // Encoding settings
    if (isPreview) {
      ffmpegArgs.push('-c:v', 'libx264', '-preset', 'ultrafast', '-crf', '28');
    } else {
      ffmpegArgs.push('-c:v', 'libx264', '-preset', 'medium', '-crf', '18');
    }

    // Add audio mapping if present
    if (audioFilter) {
      ffmpegArgs.push('-c:a', 'aac', '-b:a', '192k');
    } else {
      // If no audio filter, still need codec args if we're doing complex filter?
      // Actually, if no audioFilter, we probably have no audio output stream mapped above
      // But let's keep the codec args just in case, though usually they go with the mapping.
      // Wait, the original code had it unconditional. Let's just make it conditional to avoid duplicates if audioFilter is true.
      // Actually, looking at the code, we pushed '-map', '[aout]' inside the `if (audioFilter)` block earlier.
      // So we should only add audio codec args if audioFilter is true OR if we're not doing complex filter (which we always are here).
      // Let's just remove the duplicate unconditional one.
    }
    ffmpegArgs.push('-movflags', '+faststart');
    ffmpegArgs.push('-t', totalDuration.toString());
    ffmpegArgs.push(outputPath);

    console.log(`[${sessionId}] FFmpeg render command prepared`);

    await runFFmpeg(ffmpegArgs, sessionId);

    const { stat } = await import('fs/promises');
    const outputStats = await stat(outputPath);

    console.log(`[${sessionId}] Render complete: ${(outputStats.size / 1024 / 1024).toFixed(1)} MB`);
    console.log(`[${sessionId}] === RENDER COMPLETE ===\n`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      path: outputPath,
      size: outputStats.size,
      duration: totalDuration,
      downloadUrl: `/session/${sessionId}/renders/${isPreview ? 'preview' : 'export'}`,
    }));

  } catch (error) {
    console.error(`[${sessionId}] Render error:`, error.message);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Download rendered video
async function handleRenderDownload(req, res, sessionId, renderType) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  // Find the render file
  const { readdirSync } = require('fs');
  const files = readdirSync(session.rendersDir);

  let renderFile;
  if (renderType === 'preview') {
    renderFile = files.find(f => f === 'preview.mp4');
  } else {
    // Get most recent export
    renderFile = files
      .filter(f => f.startsWith('export-'))
      .sort()
      .pop();
  }

  if (!renderFile) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Render not found' }));
    return;
  }

  const renderPath = join(session.rendersDir, renderFile);
  const { stat } = await import('fs/promises');
  const stats = await stat(renderPath);

  const filename = renderType === 'preview' ? 'preview.mp4' : `${session.originalName.replace(/\.[^.]+$/, '')}-export.mp4`;

  res.writeHead(200, {
    'Content-Type': 'video/mp4',
    'Content-Length': stats.size,
    'Content-Disposition': `attachment; filename="${filename}"`,
    'Access-Control-Allow-Origin': '*',
  });

  createReadStream(renderPath).pipe(res);
}

// Create animated GIF from an image
async function handleCreateGif(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  try {
    let body = '';
    for await (const chunk of req) body += chunk;
    const options = body ? JSON.parse(body) : {};

    const {
      sourceAssetId,
      effect = 'pulse', // pulse, zoom, rotate, bounce, fade
      duration = 2,      // seconds
      fps = 15,
      width = 400,
      height = 400,
    } = options;

    const sourceAsset = session.assets.get(sourceAssetId);
    if (!sourceAsset) {
      res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'Source asset not found' }));
      return;
    }

    if (sourceAsset.type !== 'image') {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'Source must be an image' }));
      return;
    }

    const jobId = randomUUID();
    console.log(`\n[${jobId}] === CREATE ANIMATED GIF ===`);
    console.log(`[${jobId}] Source: ${sourceAsset.filename}, Effect: ${effect}, Duration: ${duration}s`);

    // Generate GIF output path
    const gifId = randomUUID();
    const gifPath = join(session.assetsDir, `${gifId}.gif`);
    const thumbPath = join(session.assetsDir, `${gifId}_thumb.jpg`);

    // Build FFmpeg filter based on effect
    let filter;
    const totalFrames = duration * fps;

    switch (effect) {
      case 'pulse':
        // Pulsing scale effect (breathe in/out)
        filter = `scale=${width}:${height}:force_original_aspect_ratio=decrease,pad=${width}:${height}:(ow-iw)/2:(oh-ih)/2,` +
          `zoompan=z='1+0.1*sin(on*PI*2/${totalFrames})':x='iw/2-(iw/zoom/2)':y='ih/2-(ih/zoom/2)':d=${totalFrames}:s=${width}x${height}:fps=${fps}`;
        break;

      case 'zoom':
        // Ken Burns zoom in effect
        filter = `scale=${width * 2}:${height * 2}:force_original_aspect_ratio=decrease,` +
          `zoompan=z='min(zoom+0.002,1.5)':x='iw/2-(iw/zoom/2)':y='ih/2-(ih/zoom/2)':d=${totalFrames}:s=${width}x${height}:fps=${fps}`;
        break;

      case 'rotate':
        // Gentle rotation effect
        filter = `scale=${width}:${height}:force_original_aspect_ratio=decrease,pad=${width}:${height}:(ow-iw)/2:(oh-ih)/2,` +
          `rotate=t*PI/8:c=none:ow=${width}:oh=${height},fps=${fps}`;
        break;

      case 'bounce':
        // Bouncing effect (up and down)
        filter = `scale=${width}:${height - 40}:force_original_aspect_ratio=decrease,` +
          `pad=${width}:${height}:(ow-iw)/2:'(oh-ih)/2+20*sin(t*PI*2)':color=transparent,fps=${fps}`;
        break;

      case 'fade':
        // Fade in and out
        filter = `scale=${width}:${height}:force_original_aspect_ratio=decrease,pad=${width}:${height}:(ow-iw)/2:(oh-ih)/2,` +
          `fade=t=in:st=0:d=${duration / 4},fade=t=out:st=${duration * 3 / 4}:d=${duration / 4},fps=${fps}`;
        break;

      case 'shake':
        // Shake/vibrate effect
        filter = `scale=${width}:${height}:force_original_aspect_ratio=decrease,pad=${width + 20}:${height + 20}:(ow-iw)/2:(oh-ih)/2,` +
          `crop=${width}:${height}:'10+5*sin(t*30)':'10+5*cos(t*25)',fps=${fps}`;
        break;

      default:
        // Simple loop with no animation
        filter = `scale=${width}:${height}:force_original_aspect_ratio=decrease,pad=${width}:${height}:(ow-iw)/2:(oh-ih)/2,fps=${fps}`;
    }

    // FFmpeg command to create animated GIF
    const ffmpegArgs = [
      '-y',
      '-loop', '1',
      '-i', sourceAsset.path,
      '-t', duration.toString(),
      '-vf', filter,
      '-gifflags', '+transdiff',
      gifPath
    ];

    console.log(`[${jobId}] Running FFmpeg...`);
    await runFFmpeg(ffmpegArgs, jobId);

    // Generate thumbnail from first frame
    try {
      await runFFmpeg([
        '-y',
        '-i', gifPath,
        '-vf', 'scale=160:90:force_original_aspect_ratio=decrease,pad=160:90:(ow-iw)/2:(oh-ih)/2',
        '-frames:v', '1',
        thumbPath
      ], jobId);
    } catch (e) {
      console.warn(`[${jobId}] Thumbnail generation failed:`, e.message);
    }

    const { stat } = await import('fs/promises');
    const stats = await stat(gifPath);

    // Create asset entry
    const gifAsset = {
      id: gifId,
      type: 'image',
      filename: `${sourceAsset.filename.replace(/\.[^.]+$/, '')}-${effect}.gif`,
      path: gifPath,
      thumbPath: existsSync(thumbPath) ? thumbPath : null,
      duration: duration, // GIFs have duration
      size: stats.size,
      width,
      height,
      createdAt: Date.now(),
    };

    session.assets.set(gifId, gifAsset);

    console.log(`[${jobId}] GIF created: ${(stats.size / 1024).toFixed(1)} KB`);
    console.log(`[${jobId}] === GIF CREATION COMPLETE ===\n`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      asset: {
        id: gifAsset.id,
        type: gifAsset.type,
        filename: gifAsset.filename,
        duration: gifAsset.duration,
        size: gifAsset.size,
        width: gifAsset.width,
        height: gifAsset.height,
        thumbnailUrl: gifAsset.thumbPath ? `/session/${sessionId}/assets/${gifId}/thumbnail` : null,
      },
    }));

  } catch (error) {
    console.error(`[${sessionId}] GIF creation error:`, error.message);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// ============== TRANSCRIPTION & KEYWORD EXTRACTION ==============

// Known keywords/brands to detect in transcripts
const KNOWN_KEYWORDS = [
  // Tech companies
  'anthropic', 'claude', 'openai', 'chatgpt', 'gpt', 'google', 'gemini', 'bard',
  'microsoft', 'copilot', 'meta', 'llama', 'apple', 'siri', 'amazon', 'alexa',
  'nvidia', 'tesla', 'spacex', 'neuralink', 'twitter', 'x',
  // Social media
  'youtube', 'tiktok', 'instagram', 'facebook', 'snapchat', 'linkedin', 'reddit',
  'discord', 'twitch', 'spotify',
  // People
  'elon musk', 'sam altman', 'mark zuckerberg', 'sundar pichai', 'satya nadella',
  'tim cook', 'jensen huang', 'dario amodei', 'trump', 'biden',
  // General tech terms
  'artificial intelligence', 'machine learning', 'neural network', 'blockchain',
  'cryptocurrency', 'bitcoin', 'ethereum', 'nft', 'metaverse', 'virtual reality',
  'augmented reality', 'robotics', 'automation',
  // Products
  'iphone', 'android', 'windows', 'macbook', 'playstation', 'xbox', 'nintendo',
  'airpods', 'vision pro',
];

// Extract keywords from transcript with timestamps
function extractKeywordsFromTranscript(transcript, words) {
  const foundKeywords = [];
  const lowerTranscript = transcript.toLowerCase();

  for (const keyword of KNOWN_KEYWORDS) {
    const lowerKeyword = keyword.toLowerCase();
    let searchIndex = 0;

    while (true) {
      const index = lowerTranscript.indexOf(lowerKeyword, searchIndex);
      if (index === -1) break;

      // Find the timestamp for this occurrence
      // We need to count characters to find which word this belongs to
      let charCount = 0;
      let timestamp = 0;
      let confidence = 0.9;

      for (const word of words) {
        const wordEnd = charCount + word.word.length + 1; // +1 for space
        if (index >= charCount && index < wordEnd) {
          timestamp = word.start;
          confidence = word.confidence || 0.9;
          break;
        }
        charCount = wordEnd;
      }

      // Avoid duplicates within 5 seconds
      const isDuplicate = foundKeywords.some(
        k => k.keyword === keyword && Math.abs(k.timestamp - timestamp) < 5
      );

      if (!isDuplicate) {
        foundKeywords.push({
          keyword,
          timestamp,
          confidence,
        });
      }

      searchIndex = index + keyword.length;
    }
  }

  // Sort by timestamp
  foundKeywords.sort((a, b) => a.timestamp - b.timestamp);

  return foundKeywords;
}

// Transcribe video using OpenAI Whisper API
async function transcribeVideo(videoPath, jobId) {
  const audioPath = join(TEMP_DIR, `${jobId}-audio-whisper.mp3`);

  try {
    // Extract audio
    console.log(`[${jobId}] Extracting audio for transcription...`);
    await runFFmpeg([
      '-y', '-i', videoPath,
      '-vn', '-acodec', 'libmp3lame',
      '-ab', '64k', '-ar', '16000', '-ac', '1',
      audioPath
    ], jobId);

    const { stat } = await import('fs/promises');
    const audioStats = await stat(audioPath);
    console.log(`\n[${jobId}] Audio extracted: ${(audioStats.size / 1024 / 1024).toFixed(1)} MB`);

    // Check for OpenAI API key
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) {
      throw new Error('OPENAI_API_KEY not configured in .dev.vars');
    }

    // Send to Whisper API
    console.log(`[${jobId}] Sending to Whisper API...`);
    const audioBuffer = readFileSync(audioPath);
    const audioBlob = new Blob([audioBuffer], { type: 'audio/mp3' });

    const formData = new FormData();
    formData.append('file', audioBlob, 'audio.mp3');
    formData.append('model', 'whisper-1');
    formData.append('response_format', 'verbose_json');
    formData.append('timestamp_granularities[]', 'word');

    const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
      },
      body: formData,
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Whisper API error: ${response.status} - ${errorText}`);
    }

    const result = await response.json();
    console.log(`[${jobId}] Transcription complete: ${result.text?.length || 0} characters`);

    // Cleanup
    try { unlinkSync(audioPath); } catch {}

    return {
      text: result.text || '',
      words: result.words || [],
      duration: result.duration || 0,
    };

  } catch (error) {
    try { unlinkSync(audioPath); } catch {}
    throw error;
  }
}

// Search GIPHY for a keyword
async function searchGiphy(keyword, limit = 1) {
  const apiKey = process.env.GIPHY_API_KEY;
  if (!apiKey || apiKey === 'YOUR_GIPHY_API_KEY_HERE') {
    throw new Error('GIPHY_API_KEY not configured. Get a free key at https://developers.giphy.com/');
  }

  const url = `https://api.giphy.com/v1/gifs/search?api_key=${apiKey}&q=${encodeURIComponent(keyword)}&limit=${limit}&rating=g&lang=en`;

  const response = await fetch(url);
  if (!response.ok) {
    throw new Error(`GIPHY API error: ${response.status}`);
  }

  const data = await response.json();
  return data.data || [];
}

// Download GIF and save as asset
async function downloadGifAsAsset(session, gifUrl, keyword, timestamp) {
  const jobId = randomUUID();
  const gifId = randomUUID();
  const gifPath = join(session.assetsDir, `${gifId}.gif`);
  const thumbPath = join(session.assetsDir, `${gifId}_thumb.jpg`);

  try {
    console.log(`[${jobId}] Downloading GIF for "${keyword}"...`);

    const response = await fetch(gifUrl);
    if (!response.ok) {
      throw new Error(`Failed to download GIF: ${response.status}`);
    }

    const buffer = await response.arrayBuffer();
    writeFileSync(gifPath, Buffer.from(buffer));

    // Generate thumbnail
    try {
      await runFFmpeg([
        '-y', '-i', gifPath,
        '-vf', 'scale=160:90:force_original_aspect_ratio=decrease,pad=160:90:(ow-iw)/2:(oh-ih)/2',
        '-frames:v', '1',
        thumbPath
      ], jobId);
    } catch (e) {
      console.warn(`[${jobId}] Thumbnail generation failed:`, e.message);
    }

    const { stat } = await import('fs/promises');
    const stats = await stat(gifPath);

    // Get GIF dimensions
    const info = await getMediaInfo(gifPath);

    const asset = {
      id: gifId,
      type: 'image',
      filename: `${keyword.replace(/\s+/g, '-')}.gif`,
      path: gifPath,
      thumbPath: existsSync(thumbPath) ? thumbPath : null,
      duration: 3, // Default 3 seconds for GIFs
      size: stats.size,
      width: info.width || 200,
      height: info.height || 200,
      createdAt: Date.now(),
      // Extra metadata for auto-placement
      keyword,
      timestamp,
    };

    session.assets.set(gifId, asset);

    console.log(`[${jobId}] GIF saved: ${(stats.size / 1024).toFixed(1)} KB`);

    return asset;

  } catch (error) {
    try { unlinkSync(gifPath); } catch {}
    try { unlinkSync(thumbPath); } catch {}
    throw error;
  }
}

// Search GIPHY for trending GIFs
async function searchGiphyTrending(limit = 20) {
  const apiKey = process.env.GIPHY_API_KEY;
  if (!apiKey || apiKey === 'YOUR_GIPHY_API_KEY_HERE') {
    throw new Error('GIPHY_API_KEY not configured. Get a free key at https://developers.giphy.com/');
  }

  const url = `https://api.giphy.com/v1/gifs/trending?api_key=${apiKey}&limit=${limit}&rating=g`;

  const response = await fetch(url);
  if (!response.ok) {
    throw new Error(`GIPHY API error: ${response.status}`);
  }

  const data = await response.json();
  return data.data || [];
}

// Handle GIPHY search endpoint
async function handleGiphySearch(req, res, sessionId, url) {
  const session = sessions.get(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  try {
    const query = url.searchParams.get('q') || '';
    const limit = parseInt(url.searchParams.get('limit') || '20', 10);

    if (!query.trim()) {
      res.writeHead(400, { 'Content-Type': 'application/json' });
      res.end(JSON.stringify({ error: 'Search query (q) is required' }));
      return;
    }

    const gifs = await searchGiphy(query, limit);

    // Format response
    const results = gifs.map(gif => ({
      id: gif.id,
      title: gif.title,
      url: gif.images.original.url,
      previewUrl: gif.images.fixed_width.url,
      thumbnailUrl: gif.images.fixed_width_still?.url || gif.images.fixed_width.url,
      width: parseInt(gif.images.original.width, 10),
      height: parseInt(gif.images.original.height, 10),
      source: 'giphy',
    }));

    res.writeHead(200, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({ gifs: results }));
  } catch (error) {
    console.error('GIPHY search error:', error);
    res.writeHead(500, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Handle GIPHY trending endpoint
async function handleGiphyTrending(req, res, sessionId, url) {
  const session = sessions.get(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  try {
    const limit = parseInt(url.searchParams.get('limit') || '20', 10);
    const gifs = await searchGiphyTrending(limit);

    // Format response
    const results = gifs.map(gif => ({
      id: gif.id,
      title: gif.title,
      url: gif.images.original.url,
      previewUrl: gif.images.fixed_width.url,
      thumbnailUrl: gif.images.fixed_width_still?.url || gif.images.fixed_width.url,
      width: parseInt(gif.images.original.width, 10),
      height: parseInt(gif.images.original.height, 10),
      source: 'giphy',
    }));

    res.writeHead(200, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({ gifs: results }));
  } catch (error) {
    console.error('GIPHY trending error:', error);
    res.writeHead(500, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Handle adding a GIPHY GIF to assets
async function handleGiphyAdd(req, res, sessionId) {
  const session = sessions.get(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  try {
    // Parse request body
    const body = await new Promise((resolve, reject) => {
      let data = '';
      req.on('data', chunk => { data += chunk; });
      req.on('end', () => {
        try {
          resolve(JSON.parse(data));
        } catch (e) {
          reject(new Error('Invalid JSON'));
        }
      });
      req.on('error', reject);
    });

    const { gifUrl, title } = body;
    if (!gifUrl) {
      res.writeHead(400, { 'Content-Type': 'application/json' });
      res.end(JSON.stringify({ error: 'gifUrl is required' }));
      return;
    }

    // Download and add to assets
    const asset = await downloadGifAsAsset(session, gifUrl, title || 'GIF', Date.now());

    res.writeHead(200, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({
      success: true,
      asset: {
        id: asset.id,
        filename: asset.filename,
        type: asset.type,
        duration: asset.duration,
        width: asset.width,
        height: asset.height,
        thumbnailUrl: `/session/${sessionId}/assets/${asset.id}/thumbnail`,
        streamUrl: `/session/${sessionId}/assets/${asset.id}/stream`,
      }
    }));
  } catch (error) {
    console.error('GIPHY add error:', error);
    res.writeHead(500, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Handle simple transcription for captions using Gemini (returns word-level timestamps)
// Check if local Whisper is available
async function checkLocalWhisper() {
  return new Promise((resolve) => {
    const check = spawn('python3', ['-c', 'import whisper; print("ok")']);
    let output = '';
    check.stdout.on('data', (data) => { output += data.toString(); });
    check.on('close', (code) => {
      resolve(code === 0 && output.includes('ok'));
    });
    check.on('error', () => resolve(false));
  });
}

// Run local Whisper transcription
async function runLocalWhisper(audioPath, jobId) {
  const scriptPath = join(BASE_DIR, 'scripts', 'whisper-transcribe.py');

  return new Promise((resolve, reject) => {
    console.log(`[${jobId}] Running local Whisper...`);
    const whisperProcess = spawn('python3', [scriptPath, audioPath, 'base']);

    let stdout = '';
    let stderr = '';

    whisperProcess.stdout.on('data', (data) => { stdout += data.toString(); });
    whisperProcess.stderr.on('data', (data) => {
      stderr += data.toString();
      // Log progress messages
      const lines = data.toString().split('\n').filter(l => l.trim());
      lines.forEach(line => console.log(`[${jobId}] Whisper: ${line}`));
    });

    whisperProcess.on('close', (code) => {
      if (code !== 0) {
        // Try to parse JSON error from stdout first
        try {
          const result = JSON.parse(stdout);
          if (result.error) {
            reject(new Error(`Whisper error: ${result.error}`));
            return;
          }
        } catch (e) {
          // stdout wasn't valid JSON, fall through to stderr
        }
        reject(new Error(`Whisper failed (exit code ${code}): ${stderr.slice(-500)}`));
        return;
      }
      try {
        const result = JSON.parse(stdout);
        if (result.error) {
          reject(new Error(result.error));
        } else {
          resolve(result);
        }
      } catch (e) {
        reject(new Error(`Failed to parse Whisper output: ${stdout.slice(0, 200)}`));
      }
    });

    whisperProcess.on('error', (err) => reject(err));
  });
}

// Cached transcription helper - avoids re-transcribing the same video
// Returns { text: string, words: Array<{text, start, end}> }
async function getOrTranscribeVideo(session, videoAsset, jobId) {
  // Check cache first
  if (session.transcriptCache.has(videoAsset.id)) {
    const cached = session.transcriptCache.get(videoAsset.id);
    console.log(`[${jobId}] Using cached transcript for ${videoAsset.filename} (cached ${Math.round((Date.now() - cached.cachedAt) / 1000)}s ago)`);
    return { text: cached.text, words: cached.words };
  }

  console.log(`[${jobId}] Transcribing ${videoAsset.filename}...`);

  // Check available transcription methods
  const hasLocalWhisper = await checkLocalWhisper();
  const openaiKey = process.env.OPENAI_API_KEY;
  const geminiKey = process.env.GEMINI_API_KEY;

  if (!hasLocalWhisper && !openaiKey && !geminiKey) {
    throw new Error('No transcription method available. Install local Whisper or set OPENAI_API_KEY/GEMINI_API_KEY');
  }

  // Extract audio from video
  const audioPath = join(TEMP_DIR, `${jobId}-transcript-audio.mp3`);
  await runFFmpeg([
    '-y', '-i', videoAsset.path,
    '-vn', '-acodec', 'libmp3lame', '-q:a', '4',
    audioPath
  ], jobId);

  let transcription = { text: '', words: [] };

  // Helper to transcribe with Gemini (always available as fallback if geminiKey exists)
  const transcribeWithGeminiLocal = async () => {
    if (!geminiKey) throw new Error('No transcription method available');
    console.log(`[${jobId}] Using Gemini for transcription...`);
    const ai = new GoogleGenAI({ apiKey: geminiKey });
    const audioBuffer = readFileSync(audioPath);
    const audioBase64 = audioBuffer.toString('base64');

    const result = await ai.models.generateContent({
      model: 'gemini-2.0-flash',
      contents: [{
        role: 'user',
        parts: [
          { inlineData: { mimeType: 'audio/mp3', data: audioBase64 } },
          { text: `Transcribe this audio with word timestamps. Duration: ${videoAsset.duration}s. Return JSON: {"text": "full transcript", "words": [{"text": "word", "start": 0.0, "end": 0.5}]}` }
        ]
      }],
    });

    const respText = result.candidates[0].content.parts[0].text || '';
    try {
      return JSON.parse(respText);
    } catch {
      const match = respText.match(/\{[\s\S]*\}/);
      return match ? JSON.parse(match[0]) : { text: respText, words: [] };
    }
  };

  if (hasLocalWhisper) {
    try {
      console.log(`[${jobId}] Using local Whisper...`);
      transcription = await runLocalWhisper(audioPath, jobId);
    } catch (whisperError) {
      console.log(`[${jobId}] Local Whisper failed: ${whisperError.message}`);
      console.log(`[${jobId}] Falling back to Gemini...`);
      transcription = await transcribeWithGeminiLocal();
    }
  } else if (openaiKey) {
    console.log(`[${jobId}] Using OpenAI Whisper API...`);
    const { FormData, File } = await import('formdata-node');
    const audioBuffer = readFileSync(audioPath);
    const formData = new FormData();
    formData.append('file', new File([audioBuffer], 'audio.mp3', { type: 'audio/mp3' }));
    formData.append('model', 'whisper-1');
    formData.append('response_format', 'verbose_json');
    formData.append('timestamp_granularities[]', 'word');

    const whisperResponse = await fetch('https://api.openai.com/v1/audio/transcriptions', {
      method: 'POST',
      headers: { 'Authorization': `Bearer ${openaiKey}` },
      body: formData,
    });

    if (!whisperResponse.ok) {
      throw new Error(`Whisper API error: ${whisperResponse.status}`);
    }

    const whisperResult = await whisperResponse.json();
    transcription = {
      text: whisperResult.text || '',
      words: (whisperResult.words || []).map(w => ({
        text: w.word,
        start: w.start,
        end: w.end,
      })),
    };
  } else if (geminiKey) {
    transcription = await transcribeWithGeminiLocal();
  }

  // Clean up audio file
  try { unlinkSync(audioPath); } catch {}

  // Cache the transcript
  session.transcriptCache.set(videoAsset.id, {
    text: transcription.text,
    words: transcription.words || [],
    cachedAt: Date.now(),
  });

  console.log(`[${jobId}] Transcription cached: ${transcription.text.substring(0, 100)}...`);
  return transcription;
}

// Get transcript segment for a specific time range
function getTranscriptSegment(transcription, startTime, endTime) {
  if (!transcription.words || transcription.words.length === 0) {
    return transcription.text;
  }

  const segmentWords = transcription.words.filter(w =>
    w.end >= startTime && w.start <= endTime
  );

  if (segmentWords.length === 0) {
    // Fall back to full transcript if no words in range
    return transcription.text;
  }

  return segmentWords.map(w => w.text).join(' ');
}

// Extract numeric value from stat strings like "$10K+", "50%", "2.5M", "10,000", etc.
// Returns { numericValue, prefix, suffix } where numericValue is the number to count TO
function extractNumericValue(valueStr) {
  if (!valueStr || typeof valueStr !== 'string') return null;

  const str = valueStr.trim();
  console.log(`[extractNumericValue] Input: "${str}"`);

  // Extract prefix (currency symbols and other leading non-numeric chars)
  let prefix = '';
  const prefixMatch = str.match(/^([Â£$â‚¬Â¥â‚¹#@~]+)/);
  if (prefixMatch) {
    prefix = prefixMatch[1];
  }

  // Extract the number part (including decimals and commas)
  const numberMatch = str.match(/[\d,]+\.?\d*/);
  if (!numberMatch || numberMatch[0] === '') {
    console.log(`[extractNumericValue] No number found in "${str}"`);
    return null;
  }

  let numericValue = parseFloat(numberMatch[0].replace(/,/g, ''));
  if (isNaN(numericValue)) {
    console.log(`[extractNumericValue] Could not parse number from "${numberMatch[0]}"`);
    return null;
  }

  // Extract suffix - everything after the number
  let suffix = '';
  const numberEndIndex = str.indexOf(numberMatch[0]) + numberMatch[0].length;
  const afterNumber = str.substring(numberEndIndex).trim();
  console.log(`[extractNumericValue] Number: ${numericValue}, After: "${afterNumber}"`);

  // Check for multiplier suffixes and apply them
  if (/^k\b/i.test(afterNumber) || /^thousand/i.test(afterNumber)) {
    numericValue *= 1000;
    suffix = afterNumber.replace(/^k\b/i, '').replace(/^thousand/i, '').trim();
  } else if (/^m\b/i.test(afterNumber) || /^million/i.test(afterNumber)) {
    numericValue *= 1000000;
    suffix = afterNumber.replace(/^m\b/i, '').replace(/^million/i, '').trim();
  } else if (/^b\b/i.test(afterNumber) || /^billion/i.test(afterNumber)) {
    numericValue *= 1000000000;
    suffix = afterNumber.replace(/^b\b/i, '').replace(/^billion/i, '').trim();
  } else {
    suffix = afterNumber;
  }

  // Clean up suffix - keep only common suffix chars
  // But preserve % and + which are important
  if (suffix.includes('%')) {
    suffix = '%';
  } else if (suffix.includes('+')) {
    suffix = '+';
  } else {
    suffix = suffix.replace(/[^%+\-KMB]/gi, '').trim();
  }

  const result = {
    numericValue: Math.round(numericValue),
    prefix,
    suffix,
  };

  console.log(`[extractNumericValue] Result: ${JSON.stringify(result)}`);
  return result;
}

async function handleTranscribe(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  const jobId = sessionId.substring(0, 8);
  const audioPath = join(TEMP_DIR, `${jobId}-caption-audio.mp3`);

  try {
    // Check for transcription options in order of preference:
    // 1. Local Whisper (free, accurate)
    // 2. OpenAI Whisper API (paid, accurate)
    // 3. Gemini (paid, less accurate timestamps)
    const hasLocalWhisper = await checkLocalWhisper();
    const openaiKey = process.env.OPENAI_API_KEY;
    const geminiKey = process.env.GEMINI_API_KEY;

    if (!hasLocalWhisper && !openaiKey && !geminiKey) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'No transcription method available. Install local Whisper (pip3 install openai-whisper) or set GEMINI_API_KEY in .dev.vars' }));
      return;
    }

    // Parse request body
    let body = '';
    for await (const chunk of req) {
      body += chunk;
    }
    const { assetId } = JSON.parse(body || '{}');

    // Determine which method to use
    const useLocalWhisper = hasLocalWhisper;
    const useOpenAIWhisper = !hasLocalWhisper && !!openaiKey;
    const useGemini = !hasLocalWhisper && !openaiKey && !!geminiKey;

    const method = useLocalWhisper ? 'Local Whisper' : useOpenAIWhisper ? 'OpenAI Whisper' : 'Gemini';
    console.log(`\n[${jobId}] === TRANSCRIBE FOR CAPTIONS (${method}) ===`);

    if (useLocalWhisper) {
      console.log(`[${jobId}] Using local Whisper for accurate word-level timestamps (free)`);
    } else if (useOpenAIWhisper) {
      console.log(`[${jobId}] Using OpenAI Whisper API for accurate word-level timestamps`);
    } else {
      console.log(`[${jobId}] Using Gemini (timestamps may drift - install local Whisper for accurate sync)`);
    }

    // Find the video asset
    let videoAsset = null;
    if (assetId) {
      videoAsset = session.assets.get(assetId);
    } else {
      // If no assetId, prefer the original (non-AI-generated) video asset
      for (const asset of session.assets.values()) {
        if (asset.type === 'video' && !asset.aiGenerated) {
          videoAsset = asset;
          break;
        }
      }
      // Fallback to any video if no non-AI video found
      if (!videoAsset) {
        for (const asset of session.assets.values()) {
          if (asset.type === 'video') {
            videoAsset = asset;
            break;
          }
        }
      }
    }

    if (!videoAsset) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'No video asset found' }));
      return;
    }

    console.log(`[${jobId}] Transcribing: ${videoAsset.filename}`);

    // Get video duration
    const totalDuration = await getVideoDuration(videoAsset.path);
    console.log(`[${jobId}] Video duration: ${totalDuration.toFixed(2)}s`);

    // Extract audio as MP3
    console.log(`[${jobId}] Extracting audio...`);
    await runFFmpeg([
      '-y', '-i', videoAsset.path,
      '-vn', '-acodec', 'libmp3lame',
      '-ab', '64k', '-ar', '16000', '-ac', '1',
      audioPath
    ], jobId);

    const { stat } = await import('fs/promises');
    const audioStats = await stat(audioPath);
    console.log(`[${jobId}] Audio extracted: ${(audioStats.size / 1024 / 1024).toFixed(1)} MB`);

    // Transcribe using the available method
    let transcription;

    if (useLocalWhisper) {
      // === Local Whisper - Free and accurate word-level timestamps ===
      try {
        transcription = await runLocalWhisper(audioPath, jobId);
        console.log(`[${jobId}] Local Whisper complete: ${transcription.words?.length || 0} words`);
      } catch (whisperError) {
        console.log(`[${jobId}] Local Whisper failed: ${whisperError.message}`);
        if (geminiKey) {
          console.log(`[${jobId}] Falling back to Gemini for transcription...`);
          // Fall through to Gemini transcription below by setting useGemini-like behavior
          const audioBuffer = readFileSync(audioPath);
          const audioBase64 = audioBuffer.toString('base64');
          const ai = new GoogleGenAI({ apiKey: geminiKey });

          const response = await ai.models.generateContent({
            model: 'gemini-2.0-flash',
            contents: [{ role: 'user', parts: [
              { inlineData: { mimeType: 'audio/mp3', data: audioBase64 } },
              { text: `Transcribe this audio with word-level timestamps. Duration: ${totalDuration.toFixed(1)}s. Return JSON: {"text": "full text", "words": [{"text": "word", "start": 0.0, "end": 0.5}]}` }
            ]}]
          });

          const responseText = response.text || '';
          try {
            transcription = JSON.parse(responseText);
          } catch {
            const jsonMatch = responseText.match(/\{[\s\S]*\}/);
            transcription = jsonMatch ? JSON.parse(jsonMatch[0]) : { text: responseText, words: [] };
          }
        } else {
          throw whisperError;
        }
      }

    } else if (useOpenAIWhisper) {
      // === OpenAI Whisper API - Accurate word-level timestamps ===
      console.log(`[${jobId}] Sending to OpenAI Whisper for transcription...`);
      const audioBuffer = readFileSync(audioPath);

      // Create FormData for multipart upload
      const FormData = (await import('formdata-node')).FormData;
      const { Blob } = await import('buffer');

      const formData = new FormData();
      formData.append('file', new Blob([audioBuffer], { type: 'audio/mp3' }), 'audio.mp3');
      formData.append('model', 'whisper-1');
      formData.append('response_format', 'verbose_json');
      formData.append('timestamp_granularities[]', 'word');

      const whisperResponse = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${openaiKey}`,
        },
        body: formData,
      });

      if (!whisperResponse.ok) {
        const errorText = await whisperResponse.text();
        console.error(`[${jobId}] Whisper API error:`, errorText);
        throw new Error(`Whisper API error: ${whisperResponse.status} - ${errorText}`);
      }

      const whisperResult = await whisperResponse.json();
      console.log(`[${jobId}] Whisper transcription complete: ${whisperResult.words?.length || 0} words`);

      transcription = {
        text: whisperResult.text || '',
        words: (whisperResult.words || []).map(w => ({
          text: w.word || '',
          start: w.start || 0,
          end: w.end || 0,
        }))
      };

    } else if (useGemini) {
      // === Gemini - Estimated timestamps (less accurate) ===
      console.log(`[${jobId}] Sending to Gemini for transcription...`);
      const audioBuffer = readFileSync(audioPath);
      const audioBase64 = audioBuffer.toString('base64');

      const ai = new GoogleGenAI({ apiKey: geminiKey });

      const response = await ai.models.generateContent({
        model: 'gemini-2.0-flash',
        contents: [
          {
            role: 'user',
            parts: [
              {
                inlineData: {
                  mimeType: 'audio/mp3',
                  data: audioBase64
                }
              },
              {
                text: `Transcribe this audio with word-level timestamps. The audio is ${totalDuration.toFixed(1)} seconds long.

IMPORTANT: Return ONLY valid JSON, no markdown, no explanation. The response must be parseable JSON.

Return this exact JSON structure:
{
  "text": "full transcript text here",
  "words": [
    {"text": "word1", "start": 0.0, "end": 0.5},
    {"text": "word2", "start": 0.5, "end": 1.0}
  ]
}

Guidelines:
- Include every spoken word
- Timestamps should be in seconds (decimals allowed)
- "start" is when the word begins, "end" is when it ends
- Words should be in order
- Estimate timing based on natural speech patterns if exact timing is unclear
- Do not include filler sounds like "um" or "uh" unless they're clearly intentional`
              }
            ]
          }
        ]
      });

      const responseText = response.text || '';
      console.log(`[${jobId}] Gemini response length: ${responseText.length} chars`);
      console.log(`[${jobId}] Gemini raw response:`, responseText.substring(0, 1000));

      // Parse the JSON response
      try {
        // First try direct parse
        transcription = JSON.parse(responseText);
      } catch (e1) {
        try {
          // Try to extract JSON from markdown code blocks
          const codeBlockMatch = responseText.match(/```(?:json)?\s*([\s\S]*?)```/);
          if (codeBlockMatch) {
            transcription = JSON.parse(codeBlockMatch[1].trim());
          } else {
            // Try to extract any JSON object
            const jsonMatch = responseText.match(/\{[\s\S]*\}/);
            if (jsonMatch) {
              transcription = JSON.parse(jsonMatch[0]);
            } else {
              throw new Error('No JSON found in response');
            }
          }
        } catch (e2) {
          console.error(`[${jobId}] Failed to parse Gemini response:`, responseText);

          // Last resort: try to create a simple transcription from the text
          // If Gemini just returned plain text, use that as the transcript
          if (responseText && responseText.length > 10 && !responseText.startsWith('{')) {
            console.log(`[${jobId}] Falling back to plain text transcription`);
            const plainText = responseText.replace(/```[\s\S]*?```/g, '').trim();
            const wordsArray = plainText.split(/\s+/).filter(w => w.length > 0);
            const avgWordDuration = totalDuration / wordsArray.length;

            transcription = {
              text: plainText,
              words: wordsArray.map((word, i) => ({
                text: word.replace(/[.,!?;:'"]/g, ''),
                start: i * avgWordDuration,
                end: (i + 1) * avgWordDuration,
              }))
            };
          } else {
            throw new Error('Failed to parse transcription response from Gemini');
          }
        }
      }
    }

    // Cleanup
    try { unlinkSync(audioPath); } catch {}

    const words = (transcription.words || []).map(w => ({
      text: w.text || '',
      start: parseFloat(w.start) || 0,
      end: parseFloat(w.end) || 0,
    })).filter(w => w.text.trim().length > 0); // Filter out empty words

    console.log(`[${jobId}] Transcription complete: ${words.length} words`);
    console.log(`[${jobId}] Text: "${(transcription.text || '').substring(0, 200)}..."`);

    // Check if transcription is empty
    if (words.length === 0 && (!transcription.text || transcription.text.trim().length === 0)) {
      console.error(`[${jobId}] Empty transcription - Gemini returned no words`);
      console.error(`[${jobId}] This could mean: no speech in video, audio too quiet, or unsupported language`);

      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({
        error: 'No speech detected. Make sure the video has clear, audible speech.',
        debug: {
          transcriptionText: (transcription.text || '').substring(0, 200),
          wordCount: (transcription.words || []).length
        }
      }));
      return;
    }

    console.log(`[${jobId}] === TRANSCRIPTION DONE ===\n`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      text: transcription.text || '',
      words: words,
      duration: totalDuration,
    }));

  } catch (error) {
    console.error(`[${jobId}] Error:`, error.message);
    try { unlinkSync(audioPath); } catch {}
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Handle transcribe and extract keywords endpoint
async function handleTranscribeAndExtract(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  const jobId = sessionId.substring(0, 8);

  try {
    console.log(`\n[${jobId}] === TRANSCRIBE & EXTRACT KEYWORDS ===`);

    // Find the original (non-AI-generated) video asset
    let videoAsset = null;
    for (const asset of session.assets.values()) {
      if (asset.type === 'video' && !asset.aiGenerated) {
        videoAsset = asset;
        break;
      }
    }
    if (!videoAsset) {
      for (const asset of session.assets.values()) {
        if (asset.type === 'video') { videoAsset = asset; break; }
      }
    }

    if (!videoAsset) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'No video asset found in session' }));
      return;
    }

    console.log(`[${jobId}] Using video: ${videoAsset.filename}`);

    // Step 1: Transcribe
    const transcription = await transcribeVideo(videoAsset.path, jobId);
    console.log(`[${jobId}] Transcript: "${transcription.text.substring(0, 100)}..."`);

    // Step 2: Extract keywords
    const keywords = extractKeywordsFromTranscript(transcription.text, transcription.words);
    console.log(`[${jobId}] Found ${keywords.length} keywords`);

    // Step 3: Fetch GIFs from GIPHY for each keyword
    const gifAssets = [];
    for (const kw of keywords) {
      try {
        console.log(`[${jobId}] Searching GIPHY for "${kw.keyword}"...`);
        const gifs = await searchGiphy(kw.keyword, 1);

        if (gifs.length > 0) {
          // Get the fixed height small GIF URL
          const gifUrl = gifs[0].images?.fixed_height?.url ||
                         gifs[0].images?.original?.url;

          if (gifUrl) {
            const asset = await downloadGifAsAsset(session, gifUrl, kw.keyword, kw.timestamp);
            gifAssets.push({
              assetId: asset.id,
              keyword: kw.keyword,
              timestamp: kw.timestamp,
              confidence: kw.confidence,
              filename: asset.filename,
              thumbnailUrl: `/session/${sessionId}/assets/${asset.id}/thumbnail`,
            });
          }
        }
      } catch (error) {
        console.warn(`[${jobId}] Failed to get GIF for "${kw.keyword}":`, error.message);
      }
    }

    console.log(`[${jobId}] Downloaded ${gifAssets.length} GIFs`);
    console.log(`[${jobId}] === TRANSCRIPTION COMPLETE ===\n`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      transcript: transcription.text,
      keywords: keywords,
      gifAssets: gifAssets,
    }));

  } catch (error) {
    console.error(`[${jobId}] Error:`, error.message);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// ============== B-ROLL IMAGE GENERATION ==============

// Helper to parse JSON body from request
async function parseBody(req) {
  let body = '';
  for await (const chunk of req) body += chunk;
  return body ? JSON.parse(body) : {};
}

// Analyze transcript for B-roll opportunities using Gemini
async function analyzeBrollOpportunities(transcript, words, totalDuration, apiKey) {
  const ai = new GoogleGenAI({ apiKey });

  const response = await ai.models.generateContent({
    model: 'gemini-2.0-flash',
    contents: [{
      role: 'user',
      parts: [{
        text: `Analyze this video transcript and identify 3-5 key moments that would benefit from a visual B-roll image overlay. Consider:
- Keywords or products mentioned (e.g., "iPhone", "Claude AI", "Tesla")
- Funny or emphatic moments
- Important concepts being explained
- Brand names or people mentioned
- Abstract concepts that could use visual reinforcement

The video is ${totalDuration.toFixed(1)} seconds long.

Transcript: "${transcript}"

Word timings (for reference): ${JSON.stringify(words.slice(0, 50))}${words.length > 50 ? '...' : ''}

Return a JSON array with this exact structure:
[
  {
    "timestamp": 15.2,
    "prompt": "minimalist icon of iPhone floating on clean white background, simple flat design",
    "reason": "product mention",
    "keyword": "iPhone"
  }
]

Guidelines for prompts:
- Keep prompts concise (10-20 words)
- Request clean, iconic, simple images suitable for video overlay
- Use "minimalist", "icon", "simple", "flat design" style descriptors
- Avoid complex scenes - prefer single subjects with clean backgrounds
- Images will be 1:1 square format

IMPORTANT: Return ONLY valid JSON array, no markdown, no explanation.`
      }]
    }],
    config: { responseMimeType: 'application/json' }
  });

  const responseText = response.text || '[]';

  try {
    // Try to parse directly
    const parsed = JSON.parse(responseText);
    return Array.isArray(parsed) ? parsed : [];
  } catch {
    // Try to extract JSON from response
    const match = responseText.match(/\[[\s\S]*\]/);
    if (match) {
      try {
        return JSON.parse(match[0]);
      } catch {
        return [];
      }
    }
    return [];
  }
}

// Generate image using Gemini Imagen (Nano Banana)
async function generateImageWithGemini(prompt, apiKey, outputPath) {
  const ai = new GoogleGenAI({ apiKey });

  console.log(`    Generating image: "${prompt.substring(0, 50)}..."`);

  try {
    // Use Gemini's image generation model
    const response = await ai.models.generateContent({
      model: 'gemini-2.0-flash-exp-image-generation',
      contents: [{
        role: 'user',
        parts: [{
          text: `Generate a clean, simple image: ${prompt}.
Style: minimalist, iconic, suitable for video overlay.
Format: 1:1 square aspect ratio.
Background: clean, uncluttered.`
        }]
      }],
      config: {
        responseModalities: ['image', 'text'],
      }
    });

    // Extract image from response
    const parts = response.candidates?.[0]?.content?.parts || [];
    console.log(`    Response has ${parts.length} parts`);

    for (const part of parts) {
      if (part.inlineData?.data) {
        const imageBuffer = Buffer.from(part.inlineData.data, 'base64');
        writeFileSync(outputPath, imageBuffer);
        console.log(`    âœ“ Image saved: ${(imageBuffer.length / 1024).toFixed(1)} KB`);
        return true;
      }
      if (part.text) {
        console.log(`    Part contains text: "${part.text.substring(0, 100)}..."`);
      }
    }

    console.warn(`    âš ï¸ No image data in response. Model may not support image generation.`);
    console.warn(`    Response structure:`, JSON.stringify(response.candidates?.[0]?.content || {}).substring(0, 200));
    return false;
  } catch (error) {
    console.error(`    âœ— Image generation failed: ${error.message}`);
    if (error.message.includes('not found') || error.message.includes('404')) {
      console.error(`    The model 'gemini-2.0-flash-exp-image-generation' may not be available.`);
    }
    return false;
  }
}

// Handle B-roll generation endpoint
async function handleGenerateBroll(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  const jobId = sessionId.substring(0, 8);

  try {
    console.log(`\n[${jobId}] === GENERATE B-ROLL IMAGES ===`);

    // Check for Gemini API key
    const apiKey = process.env.GEMINI_API_KEY;
    if (!apiKey) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'GEMINI_API_KEY not configured in .dev.vars' }));
      return;
    }

    // Find the original (non-AI-generated) video asset
    let videoAsset = null;
    for (const asset of session.assets.values()) {
      if (asset.type === 'video' && !asset.aiGenerated) {
        videoAsset = asset;
        break;
      }
    }
    if (!videoAsset) {
      for (const asset of session.assets.values()) {
        if (asset.type === 'video') { videoAsset = asset; break; }
      }
    }

    if (!videoAsset) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'No video asset found in session' }));
      return;
    }

    console.log(`[${jobId}] Using video: ${videoAsset.filename}`);

    // Step 1: Transcribe the video
    console.log(`[${jobId}] Step 1: Transcribing video...`);
    const audioPath = join(TEMP_DIR, `${jobId}-broll-audio.mp3`);
    const totalDuration = await getVideoDuration(videoAsset.path);

    // Check for transcription method
    const hasLocalWhisper = await checkLocalWhisper();
    const openaiKey = process.env.OPENAI_API_KEY;

    // Extract audio
    await runFFmpeg([
      '-y', '-i', videoAsset.path,
      '-vn', '-acodec', 'libmp3lame',
      '-ab', '64k', '-ar', '16000', '-ac', '1',
      audioPath
    ], jobId);

    let transcription;
    if (hasLocalWhisper) {
      try {
        console.log(`[${jobId}]    Using local Whisper...`);
        transcription = await runLocalWhisper(audioPath, jobId);
      } catch (whisperError) {
        console.log(`[${jobId}]    Local Whisper failed: ${whisperError.message}`);
        console.log(`[${jobId}]    Falling back to Gemini...`);
        const audioBuffer = readFileSync(audioPath);
        const audioBase64 = audioBuffer.toString('base64');
        const ai = new GoogleGenAI({ apiKey });
        const response = await ai.models.generateContent({
          model: 'gemini-2.0-flash',
          contents: [{ role: 'user', parts: [
            { inlineData: { mimeType: 'audio/mp3', data: audioBase64 } },
            { text: `Transcribe this audio with word timestamps. Duration: ${totalDuration}s. Return JSON: {"text": "...", "words": [{"text": "word", "start": 0.0, "end": 0.5}]}` }
          ]}]
        });
        const respText = response.text || '';
        try {
          transcription = JSON.parse(respText);
        } catch {
          const match = respText.match(/\{[\s\S]*\}/);
          transcription = match ? JSON.parse(match[0]) : { text: respText, words: [] };
        }
      }
    } else if (openaiKey) {
      console.log(`[${jobId}]    Using OpenAI Whisper API...`);
      const audioBuffer = readFileSync(audioPath);
      const FormData = (await import('formdata-node')).FormData;
      const { Blob } = await import('buffer');

      const formData = new FormData();
      formData.append('file', new Blob([audioBuffer], { type: 'audio/mp3' }), 'audio.mp3');
      formData.append('model', 'whisper-1');
      formData.append('response_format', 'verbose_json');
      formData.append('timestamp_granularities[]', 'word');

      const whisperResponse = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: { 'Authorization': `Bearer ${openaiKey}` },
        body: formData,
      });

      if (!whisperResponse.ok) {
        throw new Error(`Whisper API error: ${whisperResponse.status}`);
      }

      const whisperResult = await whisperResponse.json();
      transcription = {
        text: whisperResult.text || '',
        words: (whisperResult.words || []).map(w => ({
          text: w.word || '',
          start: w.start || 0,
          end: w.end || 0,
        }))
      };
    } else {
      // Use Gemini for transcription
      console.log(`[${jobId}]    Using Gemini for transcription...`);
      const audioBuffer = readFileSync(audioPath);
      const audioBase64 = audioBuffer.toString('base64');

      const ai = new GoogleGenAI({ apiKey });
      const response = await ai.models.generateContent({
        model: 'gemini-2.0-flash',
        contents: [{
          role: 'user',
          parts: [
            { inlineData: { mimeType: 'audio/mp3', data: audioBase64 } },
            { text: `Transcribe this audio with word timestamps. Duration: ${totalDuration}s. Return JSON: {"text": "...", "words": [{"text": "word", "start": 0.0, "end": 0.5}]}` }
          ]
        }]
      });

      const respText = response.text || '';
      try {
        transcription = JSON.parse(respText);
      } catch {
        const match = respText.match(/\{[\s\S]*\}/);
        transcription = match ? JSON.parse(match[0]) : { text: respText, words: [] };
      }
    }

    try { unlinkSync(audioPath); } catch {}

    console.log(`[${jobId}]    Transcript: "${transcription.text.substring(0, 100)}..."`);
    console.log(`[${jobId}]    Words: ${transcription.words?.length || 0}`);

    // Step 2: Analyze transcript for B-roll opportunities
    console.log(`[${jobId}] Step 2: Analyzing for B-roll opportunities...`);
    const opportunities = await analyzeBrollOpportunities(
      transcription.text,
      transcription.words || [],
      totalDuration,
      apiKey
    );

    console.log(`[${jobId}]    Found ${opportunities.length} B-roll opportunities`);
    opportunities.forEach((opp, i) => {
      console.log(`[${jobId}]    ${i + 1}. @${opp.timestamp.toFixed(1)}s: "${opp.keyword}" - ${opp.reason}`);
    });

    // Step 3: Generate images for each opportunity
    console.log(`[${jobId}] Step 3: Generating B-roll images...`);
    const brollAssets = [];

    for (let i = 0; i < opportunities.length; i++) {
      const opp = opportunities[i];
      const assetId = randomUUID();
      const imagePath = join(session.assetsDir, `${assetId}.png`);
      const thumbPath = join(session.assetsDir, `${assetId}_thumb.jpg`);

      console.log(`[${jobId}]    [${i + 1}/${opportunities.length}] Generating for "${opp.keyword}"...`);

      const success = await generateImageWithGemini(opp.prompt, apiKey, imagePath);

      if (success && existsSync(imagePath)) {
        // Generate thumbnail
        try {
          await runFFmpeg([
            '-y', '-i', imagePath,
            '-vf', 'scale=160:90:force_original_aspect_ratio=decrease,pad=160:90:(ow-iw)/2:(oh-ih)/2',
            '-frames:v', '1',
            thumbPath
          ], jobId);
        } catch (e) {
          console.warn(`[${jobId}]    Thumbnail generation failed:`, e.message);
        }

        const { stat } = await import('fs/promises');
        const stats = await stat(imagePath);
        const info = await getMediaInfo(imagePath);

        // Create asset entry
        const asset = {
          id: assetId,
          type: 'image',
          filename: `broll-${opp.keyword.replace(/\s+/g, '-')}.png`,
          path: imagePath,
          thumbPath: existsSync(thumbPath) ? thumbPath : null,
          duration: 3, // Default 3 seconds for B-roll images
          size: stats.size,
          width: info.width || 1024,
          height: info.height || 1024,
          createdAt: Date.now(),
          // B-roll metadata
          keyword: opp.keyword,
          timestamp: opp.timestamp,
          reason: opp.reason,
        };

        session.assets.set(assetId, asset);
        saveAssetMetadata(session); // Persist asset metadata to disk

        brollAssets.push({
          assetId: asset.id,
          keyword: opp.keyword,
          timestamp: opp.timestamp,
          reason: opp.reason,
          filename: asset.filename,
          thumbnailUrl: `/session/${sessionId}/assets/${asset.id}/thumbnail`,
        });

        console.log(`[${jobId}]    âœ“ Generated: ${asset.filename}`);
      } else {
        console.log(`[${jobId}]    âœ— Failed to generate image for "${opp.keyword}"`);
      }
    }

    console.log(`[${jobId}] Generated ${brollAssets.length}/${opportunities.length} B-roll images`);
    console.log(`[${jobId}] === B-ROLL GENERATION COMPLETE ===\n`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      transcript: transcription.text,
      opportunities: opportunities,
      brollAssets: brollAssets,
    }));

  } catch (error) {
    console.error(`[${jobId}] Error:`, error.message);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// ============== MOTION GRAPHICS RENDERING ==============

// Handle motion graphics rendering
// NOTE: This is a placeholder that creates a simple text overlay video using FFmpeg
// For proper Remotion rendering, you'd need to set up @remotion/renderer with bundling
async function handleRenderMotionGraphic(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  try {
    const body = await parseBody(req);
    const { templateId, props, duration, fps = 30, width = 1920, height = 1080 } = body;

    const jobId = randomUUID();
    const assetId = randomUUID();
    const outputPath = join(session.assetsDir, `${assetId}.mp4`);
    const thumbPath = join(session.assetsDir, `${assetId}_thumb.jpg`);

    console.log(`\n[${jobId}] === RENDER MOTION GRAPHIC ===`);
    console.log(`[${jobId}] Template: ${templateId}`);
    console.log(`[${jobId}] Duration: ${duration}s`);

    // Get text and styling from props
    const text = props.text || props.name || templateId;
    const color = (props.color || props.primaryColor || '#ffffff').replace('#', '');
    const bgColor = props.backgroundColor || '000000';
    const fontSize = props.fontSize || 64;

    // Create a video with text overlay using FFmpeg
    // This is a placeholder - proper Remotion rendering would generate much nicer animations
    const fontFile = '/System/Library/Fonts/Helvetica.ttc'; // macOS system font

    // FFmpeg command to create a video with text
    const ffmpegArgs = [
      '-y',
      '-f', 'lavfi',
      '-i', `color=c=0x${bgColor}:s=${width}x${height}:d=${duration}:r=${fps}`,
      '-vf', `drawtext=text='${text.replace(/'/g, "\\'")}':fontfile=${fontFile}:fontsize=${fontSize}:fontcolor=0x${color}:x=(w-text_w)/2:y=(h-text_h)/2`,
      '-c:v', 'libx264',
      '-pix_fmt', 'yuv420p',
      '-preset', 'fast',
      outputPath
    ];

    await runFFmpeg(ffmpegArgs, jobId);

    // Generate thumbnail
    await runFFmpeg([
      '-y', '-i', outputPath,
      '-vf', 'scale=160:90:force_original_aspect_ratio=decrease,pad=160:90:(ow-iw)/2:(oh-ih)/2',
      '-frames:v', '1',
      thumbPath
    ], jobId);

    const { stat } = await import('fs/promises');
    const stats = await stat(outputPath);

    // Create asset entry
    const asset = {
      id: assetId,
      type: 'video',
      filename: `motion-${templateId}-${Date.now()}.mp4`,
      path: outputPath,
      thumbPath: existsSync(thumbPath) ? thumbPath : null,
      duration: duration,
      size: stats.size,
      width,
      height,
      createdAt: Date.now(),
      // Metadata
      templateId,
      props,
    };

    session.assets.set(assetId, asset);
    saveAssetMetadata(session); // Persist asset metadata to disk

    console.log(`[${jobId}] Motion graphic rendered: ${assetId}`);
    console.log(`[${jobId}] === RENDER COMPLETE ===\n`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      assetId,
      filename: asset.filename,
      duration,
      thumbnailUrl: `/session/${sessionId}/assets/${assetId}/thumbnail`,
      streamUrl: `/session/${sessionId}/assets/${assetId}/stream`,
    }));

  } catch (error) {
    console.error('Motion graphic render error:', error);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// AI-generated animation using Gemini + Remotion
async function handleGenerateAnimation(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  const apiKey = process.env.GEMINI_API_KEY;
  if (!apiKey) {
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'GEMINI_API_KEY not configured' }));
    return;
  }

  try {
    const body = await parseBody(req);
    const { description, videoAssetId, startTime, endTime, attachedAssetIds, fps = 30, width = 1920, height = 1080, durationSeconds } = body;

    if (!description) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'description is required' }));
      return;
    }

    const jobId = randomUUID();
    const assetId = randomUUID();
    const outputPath = join(session.assetsDir, `${assetId}.mp4`);
    const thumbPath = join(session.assetsDir, `${assetId}_thumb.jpg`);
    const propsPath = join(session.dir, `${jobId}-props.json`);

    console.log(`\n[${jobId}] === GENERATE AI ANIMATION ===`);
    console.log(`[${jobId}] Description: ${description}`);
    if (attachedAssetIds?.length) {
      console.log(`[${jobId}] Attached assets: ${attachedAssetIds.length}`);
    }

    // Step 0: Get video transcript context if a video is provided
    let transcriptContext = '';
    let relevantSegment = '';
    let detectedTimeRange = null;

    if (videoAssetId) {
      const videoAsset = session.assets.get(videoAssetId);
      if (videoAsset && videoAsset.type === 'video') {
        console.log(`[${jobId}] Getting transcript context from ${videoAsset.filename}...`);

        try {
          const transcription = await getOrTranscribeVideo(session, videoAsset, jobId);

          if (transcription.text) {
            // If time range provided, get that segment
            if (startTime !== undefined && endTime !== undefined) {
              relevantSegment = getTranscriptSegment(transcription, startTime, endTime);
              detectedTimeRange = { start: startTime, end: endTime };
              console.log(`[${jobId}] â±ï¸ Using USER-SPECIFIED time range: ${startTime}s - ${endTime}s`);
              console.log(`[${jobId}] ðŸ“ Extracted transcript segment (${relevantSegment.split(' ').length} words):`);
              console.log(`[${jobId}]    "${relevantSegment.substring(0, 200)}${relevantSegment.length > 200 ? '...' : ''}"`);
            } else {
              // Use AI to identify the relevant part of the video based on the description
              console.log(`[${jobId}] Using AI to identify relevant video segment...`);

              const ai = new GoogleGenAI({ apiKey });
              const segmentResult = await ai.models.generateContent({
                model: 'gemini-2.0-flash',
                contents: [{
                  role: 'user',
                  parts: [{
                    text: `Given this video transcript and an animation request, identify the most relevant time segment.

VIDEO TRANSCRIPT (with word timestamps):
${transcription.words?.slice(0, 200).map(w => `[${w.start.toFixed(1)}s] ${w.text}`).join(' ') || transcription.text.substring(0, 2000)}

ANIMATION REQUEST: "${description}"

VIDEO DURATION: ${videoAsset.duration}s

Analyze the request and determine:
1. Which part of the video is most relevant to this animation
2. The start and end times of the relevant segment

Return ONLY JSON (no markdown):
{
  "startTime": <seconds>,
  "endTime": <seconds>,
  "reasoning": "brief explanation of why this segment is relevant"
}

If the animation seems to be for the intro (beginning), use startTime: 0.
If it's for the outro (ending), use times near the end.
If it's about a specific topic mentioned in the transcript, find where that topic is discussed.
If unclear or general, use the middle third of the video.`
                  }]
                }],
              });

              try {
                const segmentText = segmentResult.candidates[0].content.parts[0].text;
                const cleanedSegment = segmentText.replace(/```json\n?/g, '').replace(/```\n?/g, '').trim();
                const segmentData = JSON.parse(cleanedSegment);

                if (segmentData.startTime !== undefined && segmentData.endTime !== undefined) {
                  detectedTimeRange = {
                    start: Math.max(0, segmentData.startTime),
                    end: Math.min(videoAsset.duration, segmentData.endTime)
                  };
                  relevantSegment = getTranscriptSegment(transcription, detectedTimeRange.start, detectedTimeRange.end);
                  console.log(`[${jobId}] AI detected relevant segment: ${detectedTimeRange.start}s - ${detectedTimeRange.end}s`);
                  console.log(`[${jobId}] Reasoning: ${segmentData.reasoning}`);
                }
              } catch (e) {
                console.log(`[${jobId}] Could not parse segment detection, using full transcript`);
                relevantSegment = transcription.text;
              }
            }

            // Build transcript context for the animation prompt
            if (relevantSegment) {
              const timeRangeNote = detectedTimeRange
                ? `\nThis segment is from ${detectedTimeRange.start.toFixed(1)}s to ${detectedTimeRange.end.toFixed(1)}s in the video.`
                : '';

              transcriptContext = `

VIDEO CONTEXT (from the transcript):
"${relevantSegment.substring(0, 1500)}"
${timeRangeNote}

IMPORTANT: The animation content should be relevant to and inspired by this video context. Use specific terms, concepts, and themes from the transcript to make the animation feel connected to the video content.`;

              console.log(`[${jobId}] ðŸŽ¯ Transcript context built for Gemini (${relevantSegment.length} chars)`);
            }
          }
        } catch (transcriptError) {
          console.log(`[${jobId}] Could not get transcript: ${transcriptError.message}`);
          // Continue without transcript context
        }
      }
    }

    // Build context for attached assets (images/videos to include in animation)
    let attachedAssetsContext = '';
    const attachedAssetPaths = [];
    if (attachedAssetIds?.length) {
      const attachedAssetInfo = [];
      for (const attachedId of attachedAssetIds) {
        const attachedAsset = session.assets.get(attachedId);
        if (attachedAsset) {
          // Build HTTP URL for the asset (served by FFmpeg server)
          const assetUrl = `http://localhost:${PORT}/session/${sessionId}/assets/${attachedAsset.id}/stream`;
          attachedAssetInfo.push({
            id: attachedAsset.id,
            filename: attachedAsset.filename,
            type: attachedAsset.type,
            url: assetUrl,
          });
          attachedAssetPaths.push({
            id: attachedAsset.id,
            path: attachedAsset.path,  // Keep file path for server-side operations
            url: assetUrl,              // HTTP URL for Remotion rendering
            type: attachedAsset.type,
            filename: attachedAsset.filename,
          });
        }
      }
      if (attachedAssetInfo.length > 0) {
        attachedAssetsContext = `

ATTACHED MEDIA ASSETS (MUST be included in the animation):
${attachedAssetInfo.map((a, i) => `Asset ${i + 1}:
  - id: "${a.id}"
  - type: "${a.type}"
  - filename: "${a.filename}"`).join('\n')}

CRITICAL REQUIREMENTS:
1. You MUST create at least one "media" type scene for each attached asset above
2. In each media scene, set "mediaAssetId" to the EXACT id value shown above (copy/paste it exactly)
3. Use "mediaStyle": "framed" for a nicely presented image, or "fullscreen" for dramatic impact
4. Example media scene:
   {
     "id": "show-image",
     "type": "media",
     "duration": 90,
     "content": {
       "title": "Optional title over the image",
       "mediaAssetId": "${attachedAssetInfo[0].id}",
       "mediaStyle": "framed",
       "color": "#f97316"
     }
   }`;
        console.log(`[${jobId}] Including ${attachedAssetInfo.length} attached assets in animation`);
      }
    }

    // Step 1: Use Gemini to generate scene data
    console.log(`[${jobId}] Generating scenes with Gemini...`);

    const ai = new GoogleGenAI({ apiKey });

    const prompt = `You are a motion graphics designer. Create a JSON scene structure for an animated video based on this description:

"${description}"
${transcriptContext}${attachedAssetsContext}
Return ONLY valid JSON (no markdown, no code blocks) with this structure:
{
  "scenes": [
    {
      "id": "unique-id",
      "type": "title" | "steps" | "features" | "stats" | "text" | "transition" | "media" | "chart" | "comparison" | "countdown" | "shapes" | "emoji" | "gif" | "lottie",
      "duration": <number of frames at 30fps, typically 45-90 (1.5-3 seconds per scene). Keep scenes SHORT and punchy!>,
      "content": {
        "title": "optional title text",
        "subtitle": "optional subtitle",
        "items": [{"icon": "emoji or number", "label": "text", "description": "optional", "value": 75, "color": "#hex"}],
        "stats": [{"value": "10K+", "label": "Users", "numericValue": 10000, "prefix": "", "suffix": "+"}],  // IMPORTANT: numericValue must be a NUMBER (not string) for counting animation!
        "color": "#hex color for accent",
        "backgroundColor": "#hex for bg or null for transparent",
        // MEDIA SCENE OPTIONS:
        "mediaAssetId": "id of attached image/video to display",
        "mediaStyle": "fullscreen" | "framed" | "pip" | "background" | "split-left" | "split-right" | "circle" | "phone-frame",
        // VIDEO CONTROLS (for video assets):
        "videoStartFrom": 0,  // frame to start playing from
        "videoEndAt": 90,     // frame to stop at (for trimming)
        "videoVolume": 1,     // 0-1
        "videoPlaybackRate": 1, // 0.5 = slow-mo, 2 = fast forward
        "videoLoop": false,
        "videoMuted": false,
        // MEDIA ANIMATION (ken-burns, zoom, pan on the media itself):
        "mediaAnimation": {"type": "ken-burns" | "zoom-in" | "zoom-out" | "pan-left" | "pan-right" | "parallax", "intensity": 0.3},
        // TEXT OVERLAY ON MEDIA:
        "overlayText": "Text to show over media",
        "overlayPosition": "top" | "center" | "bottom",
        "overlayStyle": "minimal" | "bold" | "gradient-bar",
        // SHAPES SCENE OPTIONS:
        "shapes": [
          {
            "type": "circle" | "rect" | "triangle" | "star" | "polygon" | "ellipse",
            "fill": "#hex color",
            "stroke": "#hex outline color",
            "strokeWidth": 2,
            "x": 50, "y": 50,  // position as percentage (0-100)
            "scale": 1,
            "rotation": 0,
            "delay": 0,  // animation delay in frames
            "animation": "pop" | "spin" | "bounce" | "float" | "pulse" | "none",
            // Shape-specific: radius (circle/polygon), width/height (rect), length/direction (triangle), points/innerRadius/outerRadius (star), rx/ry (ellipse)
          }
        ],
        "shapesLayout": "scattered" | "grid" | "circle" | "custom",
        // EMOJI SCENE OPTIONS:
        "emojis": [
          {
            "emoji": "ðŸ”¥",  // Use actual emoji characters
            "x": 50, "y": 50,  // position as percentage
            "scale": 0.2,  // size (0.1 = small, 0.3 = large)
            "delay": 0,  // animation delay in frames
            "animation": "pop" | "bounce" | "float" | "pulse" | "spin" | "shake" | "wave" | "none"
          }
        ],
        "emojiLayout": "scattered" | "grid" | "circle" | "row" | "custom",
        // OTHER SCENE OPTIONS:
        "chartType": "bar" | "progress" | "pie",
        "chartData": [{"label": "Category", "value": 75, "color": "#hex"}],
        "maxValue": 100,
        "beforeLabel": "BEFORE", "afterLabel": "AFTER",
        "beforeValue": "50%", "afterValue": "95%",
        "countFrom": 3, "countTo": 0,
        "camera": {"type": "zoom-in" | "zoom-out" | "pan-left" | "pan-right" | "ken-burns" | "shake", "intensity": 0.3}
      },
      "transition": {"type": "swipe-left" | "swipe-right" | "swipe-up" | "swipe-down" | "fade" | "zoom-in" | "zoom-out" | "wipe-left" | "wipe-right" | "blur" | "flip", "duration": 15}
    }
  ],
  "backgroundColor": "#0a0a0a",
  "totalDuration": <sum of all scene durations>,
  "attachedAssets": [{"id": "asset-id", "path": "will be filled by server"}]
}

Scene types:
- "title": Big centered title with optional subtitle (for intros/outros)
- "steps": Numbered steps or process flow (1, 2, 3...)
- "features": Feature showcase with icons
- "stats": Animated statistics/numbers with COUNTING animation (numbers count from 0 to target). CRITICAL: You MUST include "numericValue" as an INTEGER (e.g., 10000, not "10000") for the counting animation to work! Example: {"value": "$10K+", "label": "Revenue", "numericValue": 10000, "prefix": "$", "suffix": "+"}. Without numericValue, numbers will NOT animate!
- "text": Simple text message
- "transition": Brief transition between scenes
- "media": Display an attached image/video with ADVANCED controls:
  * mediaStyle: "fullscreen" (edge-to-edge), "framed" (bordered), "pip" (small corner), "background" (dimmed behind text), "split-left"/"split-right" (half screen), "circle" (circular crop), "phone-frame" (mobile mockup)
  * mediaAnimation: Apply ken-burns, zoom, or pan DIRECTLY on the media for dynamic effect
  * overlayText: Add text over the media (great with "background" style)
  * For videos: Use videoStartFrom/videoEndAt to trim, videoPlaybackRate for slow-mo (0.5) or speed-up (2)
- "chart": Data visualization with chartType: "bar" (vertical bars), "progress" (horizontal progress bars), "pie" (pie chart). Use chartData array with label/value/color.
- "comparison": Before/after comparison. Use beforeLabel, afterLabel, beforeValue, afterValue.
- "countdown": Animated countdown. Use countFrom and countTo (e.g., 3 to 0).
- "shapes": Animated SVG shapes scene! Create eye-catching visuals with:
  * Shape types: "circle", "rect", "triangle", "star", "polygon", "ellipse"
  * Animations: "pop" (scale up), "spin" (rotate), "bounce" (vertical movement), "float" (gentle hover), "pulse" (breathing effect)
  * Layout: "scattered" (random positions), "grid" (organized), "circle" (arranged in circle), "custom" (use x/y)
  * Example shapes: [{"type": "star", "fill": "#f97316", "points": 5, "outerRadius": 60, "x": 50, "y": 50, "animation": "spin"}]
- "emoji": Animated emoji scene! Fun and expressive visuals:
  * Use actual emoji characters: "ðŸ”¥", "â­", "ðŸš€", "ðŸ’¯", "â¤ï¸", "ðŸŽ‰", "âœ¨", "ðŸ‘", "ðŸŽ¯", "ðŸ’¡", etc.
  * Animations: "pop", "bounce", "float", "pulse", "spin", "shake", "wave"
  * Layout: "scattered", "grid", "circle" (arranged around center), "row" (horizontal line), "custom"
  * Example: [{"emoji": "ðŸ”¥", "x": 30, "y": 50, "scale": 0.2, "animation": "bounce"}, {"emoji": "ðŸš€", "x": 70, "y": 50, "animation": "float"}]
  * Great for reactions, celebrations, emphasis!
- "gif": Animated GIF scene! GIPHY integration for memes, reactions, and B-roll:
  * Use "gifSearch" to search GIPHY for GIFs by keyword (the server will fetch actual URLs automatically!)
  * Example: {"gifSearch": "mind blown", "gifLayout": "fullscreen"} - searches GIPHY for "mind blown" GIFs
  * Can also use "gifSearches" array for multiple GIFs: {"gifSearches": ["fire", "celebration", "thumbs up"]}
  * Properties for each GIF: x, y (position 0-100), width, height, scale, playbackRate (0.5=slow, 2=fast)
  * Animations: "pop", "bounce", "float", "pulse", "spin", "shake" (applied to the GIF container)
  * Layout: "fullscreen" (single GIF fills screen), "scattered", "grid", "circle", "row", "pip" (corner)
  * Use "gifBackground": true for a looping GIF as the scene background (with dark overlay for readability)
  * POPULAR SEARCHES: "reaction", "funny", "meme", "celebration", "mind blown", "shocked", "laughing", "applause", "fire", "thumbs up", "yes", "no", "thinking", "dancing"
  * Great for: adding humor, emphasizing points, meme-style content, reaction clips!
- "lottie": Professional After Effects animations! Smooth vector animations:
  * Provide Lottie JSON URLs in the "lotties" array (from LottieFiles.com or similar)
  * Properties: src (URL to JSON), x, y (position 0-100), width, height, scale, playbackRate, direction ("forward"/"backward")
  * Layout: "fullscreen", "scattered", "grid", "circle", "row", "custom"
  * Use "lottieBackground" for animated background (with dark overlay)
  * Great for: loading spinners, confetti, celebrations, transitions, icons, illustrations
  * Example: {"lotties": [{"src": "https://assets.lottiefiles.com/...", "width": 400, "height": 400}], "lottieLayout": "fullscreen"}

Camera movement (add to any scene's content):
- "zoom-in": Slowly zoom into the content
- "zoom-out": Start zoomed, pull back
- "pan-left" / "pan-right": Horizontal movement
- "pan-up" / "pan-down": Vertical movement
- "ken-burns": Classic documentary style (slow zoom + slight pan)
- "intensity": 0.1 to 0.5 (subtle to dramatic)

Scene transitions (add to scene to animate entry/exit):
- "swipe-left" / "swipe-right": Slide in/out horizontally (most popular)
- "swipe-up" / "swipe-down": Slide in/out vertically
- "fade": Fade in/out (subtle, professional)
- "zoom-in" / "zoom-out": Scale in/out with fade
- "wipe-left" / "wipe-right": Reveal effect (like a curtain)
- "blur": Blur transition (dreamy effect)
- "flip": 3D flip effect (dramatic)
- "duration": frames for transition (default 15, use 20-30 for dramatic)

Guidelines:
- Use MORE scenes with SHORTER durations (1.5-3 seconds each, 45-90 frames). Fast cuts feel dynamic and engaging!
- For a 5s animation use 3-4 scenes, for 10s use 5-7 scenes, for 15s use 7-10 scenes, for 30s use 12-18 scenes. Scale up proportionally.
- NO scene should exceed 120 frames (4 seconds) unless it's a countdown or media showcase.
- Total duration: ${durationSeconds ? `EXACTLY ${durationSeconds} seconds (${Math.round(durationSeconds * fps)} frames) - the user specifically requested this duration!` : '5-15 seconds (150-450 frames)'}
- Use vibrant colors: #f97316 (orange), #3b82f6 (blue), #22c55e (green), #8b5cf6 (purple), #ec4899 (pink)
- Make it visually engaging with good pacing

IMPORTANT - ADD CAMERA MOVEMENTS to make scenes dynamic:
- ADD "camera" to at least 2-3 scenes (especially title, stats, and media scenes)
- Example: "content": { "title": "Hello", "camera": {"type": "zoom-in", "intensity": 0.25} }
- Use "zoom-in" for focus and impact (intensity 0.2-0.3)
- Use "ken-burns" for media/photos (intensity 0.25-0.35)
- Use "pan-left" or "pan-right" for text reveals (intensity 0.2)
- Use "shake" sparingly for energy (intensity 0.1-0.15)

- When showing numbers/stats, use numericValue for animated counting effect
- ADD TRANSITIONS between scenes! Use "swipe-left" or "swipe-right" for dynamic flow, "fade" for elegance, or "zoom-in" for impact
- Mix transition types for variety (e.g., first scene: swipe-right, second: fade, third: swipe-left)

IMPORTANT - ADD GIF SCENES for humor and engagement:
- ALWAYS include at least 1-2 "gif" type scenes in every animation for comedic/reaction effects!
- Use "gifSearch" with funny, relevant search terms that match the topic (e.g., "mind blown", "excited", "wait what", "money rain", "mic drop")
- Place GIF scenes BETWEEN informational scenes as punchlines or reactions to what was just shown
- Use "gifLayout": "fullscreen" for maximum impact, or "pip" for a subtle corner reaction
- GIFs make animations feel fun, relatable, and meme-worthy - lean into humor!
- Example: After a stats scene showing impressive numbers, add a "gif" scene with "gifSearch": "mind blown" or "impressed"
- For intros, try "lets go" or "hype". For outros, try "mic drop" or "thats all folks"
${attachedAssetIds?.length ? `- IMPORTANT: Include media scenes to showcase the attached images/videos!
- Use "mediaAnimation": {"type": "ken-burns", "intensity": 0.3} to add dynamic movement to images/videos
- Use "background" mediaStyle with "overlayText" for cinematic text-over-video effect
- For product shots, use "phone-frame" or "circle" mediaStyle
- For videos, consider using slow-mo (videoPlaybackRate: 0.5) for dramatic moments` : ''}`;

    const result = await ai.models.generateContent({
      model: 'gemini-2.0-flash',
      contents: [{ role: 'user', parts: [{ text: prompt }] }],
    });

    let sceneData;
    try {
      const responseText = result.candidates[0].content.parts[0].text;
      // Clean up response - remove markdown code blocks if present
      const cleanedResponse = responseText
        .replace(/```json\n?/g, '')
        .replace(/```\n?/g, '')
        .trim();
      sceneData = JSON.parse(cleanedResponse);
    } catch (parseError) {
      console.error(`[${jobId}] Failed to parse Gemini response:`, parseError);
      throw new Error('Failed to parse AI-generated scene data');
    }

    console.log(`[${jobId}] Generated ${sceneData.scenes.length} scenes`);

    // Log camera movements for debugging
    const scenesWithCamera = sceneData.scenes.filter(s => s.content?.camera?.type);
    if (scenesWithCamera.length > 0) {
      console.log(`[${jobId}] ðŸŽ¥ Camera movements: ${scenesWithCamera.map(s => `${s.id}: ${s.content.camera.type}`).join(', ')}`);
    } else {
      console.log(`[${jobId}] âš ï¸ No camera movements in any scene`);
    }

    let totalDuration = sceneData.totalDuration || sceneData.scenes.reduce((sum, s) => sum + s.duration, 0);

    // Enforce user-requested duration by scaling scene durations proportionally
    if (durationSeconds) {
      const targetFrames = Math.round(durationSeconds * fps);
      if (totalDuration !== targetFrames && totalDuration > 0) {
        const scale = targetFrames / totalDuration;
        console.log(`[${jobId}] â±ï¸ Adjusting duration: Gemini gave ${totalDuration} frames (${(totalDuration / fps).toFixed(1)}s), user requested ${durationSeconds}s (${targetFrames} frames). Scale: ${scale.toFixed(2)}x`);
        for (const scene of sceneData.scenes) {
          const oldDuration = scene.duration;
          scene.duration = Math.max(1, Math.round(scene.duration * scale));
          console.log(`[${jobId}]   Scene "${scene.id}": ${oldDuration} â†’ ${scene.duration} frames`);
        }
        totalDuration = sceneData.scenes.reduce((sum, s) => sum + s.duration, 0);
        sceneData.totalDuration = totalDuration;
        console.log(`[${jobId}] â±ï¸ Adjusted total: ${totalDuration} frames (${(totalDuration / fps).toFixed(1)}s)`);
      }
    }

    const durationInSeconds = totalDuration / fps;

    // Inject actual asset file paths for attached media (use absolute file paths for Remotion CLI)
    if (attachedAssetPaths.length > 0) {
      sceneData.attachedAssets = attachedAssetPaths;
      console.log(`[${jobId}] Available attached assets:`, attachedAssetPaths.map(a => ({ id: a.id, filename: a.filename, type: a.type })));

      // Also update any media scenes with the correct file paths
      let mediaSceneCount = 0;
      for (const scene of sceneData.scenes) {
        console.log(`[${jobId}] Checking scene: type=${scene.type}, hasMediaAssetId=${!!scene.content?.mediaAssetId}`);

        if (scene.type === 'media' && scene.content?.mediaAssetId) {
          const matchedAsset = attachedAssetPaths.find(a => a.id === scene.content.mediaAssetId);
          if (matchedAsset) {
            // Use HTTP URL for Remotion CLI rendering - more reliable than file:// paths
            scene.content.mediaPath = matchedAsset.url;
            scene.content.mediaType = matchedAsset.type;
            mediaSceneCount++;
            console.log(`[${jobId}] âœ“ Linked media asset to scene: ${matchedAsset.filename} -> ${matchedAsset.url}`);
          } else {
            console.log(`[${jobId}] âœ— No matching asset found for mediaAssetId: ${scene.content.mediaAssetId}`);
            console.log(`[${jobId}]   Available IDs: ${attachedAssetPaths.map(a => a.id).join(', ')}`);
          }
        } else if (scene.type === 'media' && !scene.content?.mediaAssetId) {
          console.log(`[${jobId}] âœ— Media scene without mediaAssetId - will show placeholder`);
          // If Gemini created a media scene but didn't set mediaAssetId, try to assign the first attached asset
          if (attachedAssetPaths.length > 0) {
            const firstAsset = attachedAssetPaths[0];
            scene.content.mediaAssetId = firstAsset.id;
            scene.content.mediaPath = firstAsset.url;  // Use HTTP URL
            scene.content.mediaType = firstAsset.type;
            mediaSceneCount++;
            console.log(`[${jobId}] âœ“ Auto-assigned first attached asset: ${firstAsset.filename} -> ${firstAsset.url}`);
          }
        }
      }

      // If Gemini didn't create any media scenes but we have attached assets, add one
      if (mediaSceneCount === 0 && attachedAssetPaths.length > 0) {
        console.log(`[${jobId}] âš  No media scenes found! Adding a media scene for the attached asset(s)`);
        const firstAsset = attachedAssetPaths[0];
        const mediaScene = {
          id: `media-${firstAsset.id}`,
          type: 'media',
          duration: 90, // 3 seconds at 30fps
          content: {
            title: firstAsset.filename.replace(/\.[^/.]+$/, ''), // filename without extension
            mediaAssetId: firstAsset.id,
            mediaPath: firstAsset.url,  // Use HTTP URL
            mediaType: firstAsset.type,
            mediaStyle: 'framed',
            color: '#f97316',
          }
        };
        // Insert media scene near the beginning (after the first scene if there is one)
        if (sceneData.scenes.length > 1) {
          sceneData.scenes.splice(1, 0, mediaScene);
        } else {
          sceneData.scenes.push(mediaScene);
        }
        sceneData.totalDuration = sceneData.scenes.reduce((sum, s) => sum + s.duration, 0);
        console.log(`[${jobId}] âœ“ Added media scene for: ${firstAsset.filename} -> ${firstAsset.url}`);
      }
    }

    // Post-process GIF scenes - search GIPHY and inject actual URLs
    const giphyKey = process.env.GIPHY_API_KEY;
    for (const scene of sceneData.scenes) {
      if (scene.type === 'gif' && scene.content) {
        const { gifSearch, gifSearches } = scene.content;
        const searchTerms = gifSearches || (gifSearch ? [gifSearch] : []);

        if (searchTerms.length > 0 && giphyKey) {
          console.log(`[${jobId}] ðŸŽ¬ Fetching GIFs from GIPHY for: ${searchTerms.join(', ')}`);
          scene.content.gifs = [];

          for (const term of searchTerms) {
            try {
              const gifs = await searchGiphy(term, 1);
              if (gifs.length > 0) {
                const gif = gifs[0];
                const gifUrl = gif.images?.fixed_height?.url || gif.images?.original?.url;
                if (gifUrl) {
                  scene.content.gifs.push({
                    src: gifUrl,
                    width: parseInt(gif.images?.fixed_height?.width) || 400,
                    height: parseInt(gif.images?.fixed_height?.height) || 300,
                    title: gif.title || term,
                    searchTerm: term,
                  });
                  console.log(`[${jobId}]    âœ“ Found GIF for "${term}": ${gif.title || 'untitled'}`);
                }
              } else {
                console.log(`[${jobId}]    âœ— No GIF found for "${term}"`);
              }
            } catch (err) {
              console.log(`[${jobId}]    âœ— GIPHY search failed for "${term}": ${err.message}`);
            }
          }

          // Set default layout if not specified
          if (!scene.content.gifLayout && scene.content.gifs.length === 1) {
            scene.content.gifLayout = 'fullscreen';
          } else if (!scene.content.gifLayout) {
            scene.content.gifLayout = 'scattered';
          }

          console.log(`[${jobId}]    Total GIFs fetched: ${scene.content.gifs.length}`);
        } else if (searchTerms.length > 0 && !giphyKey) {
          console.log(`[${jobId}] âš  GIPHY_API_KEY not configured - skipping GIF search`);
        }
      }
    }

    // Post-process stats to ensure numericValue is set for counting animation
    for (const scene of sceneData.scenes) {
      if (scene.type === 'stats' && scene.content?.stats) {
        console.log(`[${jobId}] ðŸ“Š Processing stats scene with ${scene.content.stats.length} stats...`);
        for (const stat of scene.content.stats) {
          console.log(`[${jobId}]    Raw stat: value="${stat.value}", numericValue=${stat.numericValue} (type: ${typeof stat.numericValue}), prefix="${stat.prefix || ''}", suffix="${stat.suffix || ''}"`);

          // Convert numericValue to number if it's a string
          if (typeof stat.numericValue === 'string') {
            const parsed = parseFloat(stat.numericValue);
            if (!isNaN(parsed)) {
              stat.numericValue = parsed;
              console.log(`[${jobId}]    âœ“ Converted string numericValue to number: ${stat.numericValue}`);
            } else {
              stat.numericValue = undefined; // Clear invalid string so we can extract from value
            }
          }

          // If numericValue is not a valid positive number, try to extract from value string
          const hasValidNumericValue = typeof stat.numericValue === 'number' && !isNaN(stat.numericValue) && stat.numericValue > 0;

          if (!hasValidNumericValue && stat.value) {
            const extracted = extractNumericValue(stat.value);
            if (extracted && extracted.numericValue > 0) {
              stat.numericValue = extracted.numericValue;
              stat.prefix = stat.prefix || extracted.prefix;
              stat.suffix = stat.suffix || extracted.suffix;
              console.log(`[${jobId}]    âœ“ Extracted: "${stat.value}" â†’ prefix="${stat.prefix}" numericValue=${stat.numericValue} suffix="${stat.suffix}"`);
            } else {
              console.log(`[${jobId}]    âœ— Could not extract numeric value from "${stat.value}"`);
            }
          } else if (hasValidNumericValue) {
            console.log(`[${jobId}]    âœ“ Already has valid numericValue: ${stat.numericValue}`);
          }

          // Final check: log what will be used for rendering
          const finalHasNumeric = typeof stat.numericValue === 'number' && !isNaN(stat.numericValue) && stat.numericValue > 0;
          console.log(`[${jobId}]    â†’ Final: numericValue=${stat.numericValue}, will animate: ${finalHasNumeric}`);
        }
      }
    }

    // Step 2: Write props to JSON file for Remotion
    // Log final scene data for debugging
    console.log(`[${jobId}] Final scene data:`);
    for (const scene of sceneData.scenes) {
      const hasMedia = scene.content?.mediaPath ? `mediaPath: ${scene.content.mediaPath}` : 'no media';
      const hasStats = scene.content?.stats ? `stats: ${scene.content.stats.map(s => s.numericValue || s.value).join(', ')}` : '';
      console.log(`[${jobId}]   - ${scene.type}: ${scene.content?.title || '(no title)'} | ${hasMedia} ${hasStats}`);
    }
    writeFileSync(propsPath, JSON.stringify(sceneData, null, 2));
    console.log(`[${jobId}] Props written to ${propsPath}`);

    // Step 3: Render with Remotion CLI
    console.log(`[${jobId}] Rendering with Remotion...`);

    const remotionArgs = [
      'remotion', 'render',
      'src/remotion/index.tsx',
      'DynamicAnimation',
      outputPath,
      '--props', propsPath,
      '--frames', `0-${totalDuration - 1}`,
      '--fps', String(fps),
      '--width', String(width),
      '--height', String(height),
      '--codec', 'h264',
      '--overwrite',
      '--gl=angle', // Use Metal GPU acceleration on macOS
    ];

    await new Promise((resolve, reject) => {
      const proc = spawn('npx', remotionArgs, {
        cwd: process.cwd(),
        stdio: ['pipe', 'pipe', 'pipe'],
      });

      let stderr = '';
      proc.stderr.on('data', (data) => {
        stderr += data.toString();
        console.log(`[${jobId}] Remotion: ${data.toString().trim()}`);
      });

      proc.on('close', (code) => {
        if (code === 0) {
          resolve();
        } else {
          reject(new Error(`Remotion render failed with code ${code}: ${stderr}`));
        }
      });

      proc.on('error', (err) => {
        reject(new Error(`Failed to start Remotion: ${err.message}`));
      });
    });

    // Step 4: Generate thumbnail
    await runFFmpeg([
      '-y', '-i', outputPath,
      '-vf', 'scale=160:90:force_original_aspect_ratio=decrease,pad=160:90:(ow-iw)/2:(oh-ih)/2',
      '-frames:v', '1',
      thumbPath
    ], jobId);

    // Store the scene data for future editing (don't delete props)
    const sceneDataPath = join(session.dir, `${assetId}-scenes.json`);
    writeFileSync(sceneDataPath, JSON.stringify(sceneData, null, 2));

    // Clean up temporary props file (but keep scene data)
    try {
      unlinkSync(propsPath);
    } catch (e) {
      // Ignore cleanup errors
    }

    const { stat } = await import('fs/promises');
    const stats = await stat(outputPath);

    // Create asset entry with scene data for re-editing
    const asset = {
      id: assetId,
      type: 'video',
      filename: `animation-${Date.now()}.mp4`,
      path: outputPath,
      thumbPath: existsSync(thumbPath) ? thumbPath : null,
      duration: durationInSeconds,
      size: stats.size,
      width,
      height,
      createdAt: Date.now(),
      // Metadata for AI animations
      aiGenerated: true,
      description,
      sceneCount: sceneData.scenes.length,
      sceneDataPath, // Store path to scene data for re-editing
      sceneData, // Also keep in memory for quick access
    };

    session.assets.set(assetId, asset);
    saveAssetMetadata(session); // Persist AI-generated flag to disk

    console.log(`[${jobId}] AI animation rendered: ${assetId} (${durationInSeconds}s)`);
    console.log(`[${jobId}] === GENERATION COMPLETE ===\n`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      assetId,
      filename: asset.filename,
      duration: durationInSeconds,
      sceneCount: sceneData.scenes.length,
      thumbnailUrl: `/session/${sessionId}/assets/${assetId}/thumbnail`,
      streamUrl: `/session/${sessionId}/assets/${assetId}/stream`,
    }));

  } catch (error) {
    console.error('AI animation generation error:', error);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Edit an existing animation with a new prompt
// Takes the original scene data and modifies it based on the prompt
async function handleEditAnimation(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  const apiKey = process.env.GEMINI_API_KEY;
  if (!apiKey) {
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'GEMINI_API_KEY not configured' }));
    return;
  }

  try {
    const body = await parseBody(req);
    const { assetId, editPrompt, assets: availableAssets, v1Context, fps = 30, width = 1920, height = 1080 } = body;

    if (!assetId || !editPrompt) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'assetId and editPrompt are required' }));
      return;
    }

    // Get the original animation asset
    const originalAsset = session.assets.get(assetId);
    if (!originalAsset) {
      res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'Animation asset not found' }));
      return;
    }

    if (!originalAsset.aiGenerated) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'Asset is not an AI-generated animation' }));
      return;
    }

    // Get the original scene data
    let originalSceneData = originalAsset.sceneData;
    if (!originalSceneData && originalAsset.sceneDataPath && existsSync(originalAsset.sceneDataPath)) {
      originalSceneData = JSON.parse(readFileSync(originalAsset.sceneDataPath, 'utf-8'));
    }

    if (!originalSceneData) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'Original scene data not found - cannot edit this animation' }));
      return;
    }

    const jobId = randomUUID();
    // IMPORTANT: Reuse the same asset ID to replace in-place (no asset creep)
    const outputPath = originalAsset.path; // Overwrite existing video file
    const thumbPath = originalAsset.thumbPath || join(session.assetsDir, `${assetId}_thumb.jpg`);
    const propsPath = join(session.dir, `${jobId}-props.json`);
    // Reuse existing scene data path or create one with original asset ID
    const existingSceneDataPath = originalAsset.sceneDataPath || join(session.dir, `${assetId}-scenes.json`);

    console.log(`\n[${jobId}] ========================================`);
    console.log(`[${jobId}] === EDIT AI ANIMATION (IN-PLACE) ===`);
    console.log(`[${jobId}] ========================================`);
    console.log(`[${jobId}] IMPORTANT: Reusing SAME asset ID: ${assetId}`);
    console.log(`[${jobId}] Output path (overwriting): ${outputPath}`);
    console.log(`[${jobId}] Edit prompt: ${editPrompt}`);
    console.log(`[${jobId}] Original scene count: ${originalSceneData.scenes?.length || 0}`);
    console.log(`[${jobId}] Original scenes: ${originalSceneData.scenes?.map(s => s.type).join(', ') || 'none'}`);
    console.log(`[${jobId}] Original scene data being passed to Gemini:`);
    console.log(JSON.stringify(originalSceneData, null, 2));
    if (v1Context) {
      console.log(`[${jobId}] V1 context: ${v1Context.filename} (${v1Context.type})`);
    }

    // Build transcript context from source video if available
    // Try V1 context first, but fall back to any non-AI-generated video in session
    let transcriptContext = '';
    let sourceVideoAsset = null;

    // First, try the V1 clip if it's a real video (not AI-generated animation)
    if (v1Context && v1Context.assetId && v1Context.type === 'video') {
      const v1VideoAsset = session.assets.get(v1Context.assetId);
      if (v1VideoAsset && v1VideoAsset.type === 'video' && !v1VideoAsset.aiGenerated) {
        sourceVideoAsset = v1VideoAsset;
        console.log(`[${jobId}] ðŸ“ Using V1 source video for transcript: ${v1VideoAsset.filename}`);
      }
    }

    // If V1 is an animation, find any source video in the session
    if (!sourceVideoAsset) {
      for (const asset of session.assets.values()) {
        if (asset.type === 'video' && !asset.aiGenerated) {
          sourceVideoAsset = asset;
          console.log(`[${jobId}] ðŸ“ Using session source video for transcript: ${asset.filename}`);
          break;
        }
      }
    }

    // Fetch transcript from the source video
    if (sourceVideoAsset) {
      try {
        const transcription = await getOrTranscribeVideo(session, sourceVideoAsset, jobId);
        if (transcription.text) {
          // Get first 1500 chars of transcript for context
          const transcriptText = transcription.text.substring(0, 1500);
          transcriptContext = `

VIDEO TRANSCRIPT CONTEXT (what's being said in the video "${sourceVideoAsset.filename}"):
"${transcriptText}"${transcription.text.length > 1500 ? '...' : ''}

This is what the viewer is hearing. Use this context to make the animation content relevant and synchronized with the video's message. Consider:
- Key topics and themes being discussed
- Important words, phrases, or concepts that could be visualized
- The tone and style of the content (educational, entertaining, promotional, etc.)
- Specific facts, numbers, or quotes that could be highlighted`;
          console.log(`[${jobId}] âœ… Transcript context added (${transcriptText.length} chars)`);
        }
      } catch (transcriptError) {
        console.log(`[${jobId}] âš ï¸ Could not get transcript: ${transcriptError.message}`);
        // Continue without transcript - not a fatal error
      }
    } else {
      console.log(`[${jobId}] â„¹ï¸ No source video found for transcript context`);
    }

    // Build asset context for Gemini
    let assetContext = '';

    // Include V1 context if provided (primary clip in the edit tab)
    if (v1Context) {
      assetContext += `\n\nPRIMARY V1 CLIP CONTEXT (currently on the timeline):
- ${v1Context.type}: "${v1Context.filename}" (id: ${v1Context.assetId})${v1Context.duration ? `, duration: ${v1Context.duration}s` : ''}
This clip is currently being used in the animation timeline. You can reference it for visual coherence or incorporate it into scenes.`;
    }

    if (availableAssets && availableAssets.length > 0) {
      assetContext += `\n\nAVAILABLE ASSETS you can use in the animation:
${availableAssets.map(a => `- ${a.type}: "${a.filename}" (id: ${a.id})${a.type === 'video' ? `, duration: ${a.duration}s` : ''}`).join('\n')}

To include an asset in a scene, use:
{
  "type": "asset",
  "assetType": "image" | "video",
  "assetId": "<asset id>",
  "duration": <frames>,
  "content": { "title": "optional overlay text" }
}`;
    }

    // Use Gemini to modify the scene data
    console.log(`[${jobId}] Modifying scenes with Gemini...`);

    const ai = new GoogleGenAI({ apiKey });

    const prompt = `You are editing an EXISTING Remotion animation. The user wants to make a SPECIFIC change.

## YOUR TASK
Make ONLY the change the user requested. Do NOT change anything else.

## EXISTING ANIMATION (copy this exactly, then apply ONLY the requested change):
${JSON.stringify(originalSceneData, null, 2)}

## USER'S REQUESTED CHANGE:
"${editPrompt}"
${assetContext}${transcriptContext}

## SCENE STRUCTURE REFERENCE:
Scene types and their content properties:
- "title": { "title": "text", "subtitle": "optional text", "color": "#hex", "backgroundColor": "#hex" }
- "text": { "title": "main text", "subtitle": "optional" }
- "steps" / "features": { "title": "optional heading", "items": [{"icon": "emoji", "label": "text", "description": "optional"}] }
- "stats": { "stats": [{"value": "10K+", "label": "Users", "numericValue": 10000}] }
- "transition": { "color": "#hex" }

## ADDING EMOJIS/ICONS:
To add emojis or icons, use scene types that support "items" array:
{
  "type": "features",
  "duration": 90,
  "content": {
    "title": "Optional heading",
    "items": [
      {"icon": "ðŸ’¯", "label": "100% Satisfaction"},
      {"icon": "ðŸ”¥", "label": "Hot Feature"},
      {"icon": "â­", "label": "5-Star Quality"}
    ]
  }
}

To add a SINGLE large emoji/icon, use a "title" scene with the emoji IN the title:
{
  "type": "title",
  "duration": 60,
  "content": {
    "title": "ðŸ’¯",
    "subtitle": "Perfect Score"
  }
}

## CAMERA MOVEMENTS (IMPORTANT - add to make scenes dynamic):
Camera movements make scenes more engaging. Add a "camera" object INSIDE the scene's "content":

Available camera types:
- "zoom-in": Slowly zoom into the content (intensity 0.2-0.4 recommended)
- "zoom-out": Start zoomed in, pull back to reveal
- "pan-left" / "pan-right": Horizontal tracking movement
- "pan-up" / "pan-down": Vertical tilt movement
- "ken-burns": Classic documentary style (slow zoom + subtle pan)
- "shake": Camera shake for energy/impact (use low intensity 0.1-0.2)

EXAMPLE - Complete scene with camera movement:
{
  "id": "intro-scene",
  "type": "title",
  "duration": 90,
  "content": {
    "title": "Welcome",
    "subtitle": "Let's get started",
    "color": "#ffffff",
    "backgroundColor": "#1a1a2e",
    "camera": {
      "type": "zoom-in",
      "intensity": 0.3
    }
  }
}

WHEN TO ADD CAMERA MOVEMENTS:
- User says "add zoom", "zoom in", "zoom effect" â†’ Add camera with type "zoom-in"
- User says "add pan", "pan across", "tracking" â†’ Add camera with type "pan-left" or "pan-right"
- User says "ken burns", "documentary style" â†’ Add camera with type "ken-burns"
- User says "shake", "energy", "impact" â†’ Add camera with type "shake" (low intensity)
- User says "make it dynamic", "more movement", "cinematic" â†’ Add camera movements to multiple scenes

## STRICT RULES - FOLLOW EXACTLY:
1. Copy the ENTIRE existing animation structure above
2. Find ONLY the specific element the user mentioned
3. Change ONLY that element - nothing else
4. Keep ALL other text, colors, durations, and properties EXACTLY the same

## EXAMPLES OF CORRECT BEHAVIOR:
- User says "change the title to Hello World" â†’ Only change the title text field, keep all colors/styles
- User says "make it blue" â†’ Only change color values, keep all text the same
- User says "add a new scene" â†’ Keep all existing scenes, append the new one
- User says "add zoom effect" â†’ Add camera object with zoom-in to relevant scenes
- User says "add ken burns to the intro" â†’ Add camera object to intro scene only
- User says "make it more dynamic" â†’ Add camera movements and/or transitions to scenes
- User says "add a 100 emoji" â†’ Add a new scene with type "title" and title "ðŸ’¯" or add to items array
- User says "add fire emoji" â†’ Add "ðŸ”¥" to title or items depending on context
- User says "visualize the transcript" â†’ Create scenes that highlight key words, phrases, or concepts from the transcript
- User says "add kinetic typography" â†’ Create animated text scenes using words from the transcript

## TRANSCRIPT VISUALIZATION (if transcript context is provided):
When transcript context is available, you can use it to:
- Extract key quotes and display them with "title" or "text" scenes
- Identify statistics or numbers mentioned and create "stats" scenes
- Find key steps or points and create "steps" or "features" scenes
- Pull important concepts and visualize them with relevant emojis/icons
- Create word clouds or key phrase highlights

## EXAMPLES OF WRONG BEHAVIOR (DO NOT DO THIS):
- Changing colors when user only asked about text
- Changing text when user only asked about colors
- Removing or reordering scenes
- Changing durations unless specifically asked

Return ONLY the complete JSON structure with your minimal change applied. No markdown, no explanation.`;

    // Use Gemini 3.0 Pro for better instruction following on edits
    const result = await ai.models.generateContent({
      model: 'gemini-3-pro-preview',
      contents: [{ role: 'user', parts: [{ text: prompt }] }],
    });

    let newSceneData;
    try {
      const responseText = result.candidates[0].content.parts[0].text;
      const cleanedResponse = responseText
        .replace(/```json\n?/g, '')
        .replace(/```\n?/g, '')
        .trim();
      newSceneData = JSON.parse(cleanedResponse);
    } catch (parseError) {
      console.error(`[${jobId}] Failed to parse Gemini response:`, parseError);
      throw new Error('Failed to parse AI-modified scene data');
    }

    console.log(`[${jobId}] Modified to ${newSceneData.scenes.length} scenes`);

    // Log camera movements for debugging
    const scenesWithCamera = newSceneData.scenes.filter(s => s.content?.camera?.type);
    if (scenesWithCamera.length > 0) {
      console.log(`[${jobId}] ðŸŽ¥ Camera movements: ${scenesWithCamera.map(s => `${s.id}: ${s.content.camera.type}`).join(', ')}`);
    } else {
      console.log(`[${jobId}] âš ï¸ No camera movements in any scene`);
    }

    const totalDuration = newSceneData.totalDuration || newSceneData.scenes.reduce((sum, s) => sum + s.duration, 0);
    const durationInSeconds = totalDuration / fps;

    // Store scene data for future editing (overwrite existing)
    writeFileSync(existingSceneDataPath, JSON.stringify(newSceneData, null, 2));

    // Write props for Remotion
    writeFileSync(propsPath, JSON.stringify(newSceneData, null, 2));
    console.log(`[${jobId}] Props written to ${propsPath}`);

    // Render with Remotion
    console.log(`[${jobId}] Rendering with Remotion...`);

    const remotionArgs = [
      'remotion', 'render',
      'src/remotion/index.tsx',
      'DynamicAnimation',
      outputPath,
      '--props', propsPath,
      '--frames', `0-${totalDuration - 1}`,
      '--fps', String(fps),
      '--width', String(width),
      '--height', String(height),
      '--codec', 'h264',
      '--overwrite',
      '--gl=angle', // Use Metal GPU acceleration on macOS
    ];

    await new Promise((resolve, reject) => {
      const proc = spawn('npx', remotionArgs, {
        cwd: process.cwd(),
        stdio: ['pipe', 'pipe', 'pipe'],
      });

      let stderr = '';
      proc.stderr.on('data', (data) => {
        stderr += data.toString();
        console.log(`[${jobId}] Remotion: ${data.toString().trim()}`);
      });

      proc.on('close', (code) => {
        if (code === 0) {
          resolve();
        } else {
          reject(new Error(`Remotion render failed with code ${code}: ${stderr}`));
        }
      });

      proc.on('error', (err) => {
        reject(new Error(`Failed to start Remotion: ${err.message}`));
      });
    });

    // Generate thumbnail
    await runFFmpeg([
      '-y', '-i', outputPath,
      '-vf', 'scale=160:90:force_original_aspect_ratio=decrease,pad=160:90:(ow-iw)/2:(oh-ih)/2',
      '-frames:v', '1',
      thumbPath
    ], jobId);

    // Clean up props file
    try {
      unlinkSync(propsPath);
    } catch (e) {}

    const { stat } = await import('fs/promises');
    const stats = await stat(outputPath);

    // Update the existing asset entry IN-PLACE (no new asset, prevents asset creep)
    originalAsset.duration = durationInSeconds;
    originalAsset.size = stats.size;
    originalAsset.thumbPath = existsSync(thumbPath) ? thumbPath : null;
    originalAsset.sceneCount = newSceneData.scenes.length;
    originalAsset.sceneDataPath = existingSceneDataPath;
    originalAsset.sceneData = newSceneData;
    originalAsset.lastEditedAt = Date.now();
    originalAsset.lastEditPrompt = editPrompt;
    // Keep original description but track edit history
    originalAsset.editCount = (originalAsset.editCount || 0) + 1;
    saveAssetMetadata(session); // Persist updated metadata to disk

    console.log(`[${jobId}] ========================================`);
    console.log(`[${jobId}] Animation updated IN-PLACE successfully!`);
    console.log(`[${jobId}] SAME asset ID: ${assetId}`);
    console.log(`[${jobId}] Duration: ${durationInSeconds}s`);
    console.log(`[${jobId}] Edit count: ${originalAsset.editCount}`);
    console.log(`[${jobId}] Total assets in session: ${session.assets.size}`);
    console.log(`[${jobId}] === EDIT COMPLETE ===`);
    console.log(`[${jobId}] ========================================\n`);

    const responseData = {
      success: true,
      assetId: assetId, // Same asset ID - no new asset created
      filename: originalAsset.filename,
      duration: durationInSeconds,
      sceneCount: newSceneData.scenes.length,
      editCount: originalAsset.editCount,
      thumbnailUrl: `/session/${sessionId}/assets/${assetId}/thumbnail?t=${Date.now()}`, // Cache bust
      streamUrl: `/session/${sessionId}/assets/${assetId}/stream?t=${Date.now()}`, // Cache bust
    };

    console.log(`[${jobId}] Sending response:`, JSON.stringify(responseData, null, 2));

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify(responseData));

  } catch (error) {
    console.error('Animation edit error:', error);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Generate image using fal.ai nano-banana-pro model (Picasso agent)
async function handleGenerateImage(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  const falApiKey = process.env.FAL_KEY || process.env.FAL_API_KEY;
  if (!falApiKey) {
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'FAL_KEY or FAL_API_KEY not configured in .dev.vars' }));
    return;
  }

  const geminiApiKey = process.env.GEMINI_API_KEY;

  try {
    const body = await parseBody(req);
    const {
      prompt,
      aspectRatio = '16:9',
      resolution = '1K',
      numImages = 1
    } = body;

    if (!prompt) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'prompt is required' }));
      return;
    }

    const jobId = sessionId.substring(0, 8);
    console.log(`\n[${jobId}] === PICASSO: GENERATE IMAGE ===`);
    console.log(`[${jobId}] User prompt: ${prompt}`);
    console.log(`[${jobId}] Aspect ratio: ${aspectRatio}, Resolution: ${resolution}`);

    // Enhance prompt using Gemini for better image generation results
    let enhancedPrompt = prompt;
    if (geminiApiKey) {
      try {
        console.log(`[${jobId}] Enhancing prompt with Picasso AI...`);
        const ai = new GoogleGenAI({ apiKey: geminiApiKey });

        const systemPrompt = `You are Picasso, an expert AI prompt engineer specializing in image generation. Your role is to transform simple user requests into detailed, visually compelling prompts that produce stunning images.

## Your Expertise
- Deep knowledge of photography, cinematography, art styles, and visual composition
- Understanding of lighting (golden hour, studio, dramatic, soft, etc.)
- Mastery of artistic movements (impressionism, surrealism, photorealism, etc.)
- Knowledge of camera perspectives, lenses, and depth of field
- Understanding of color theory and mood creation

## Prompt Enhancement Guidelines

1. **Visual Details**: Add specific visual elements - textures, materials, colors, patterns
2. **Lighting**: Specify lighting conditions that enhance the mood (soft diffused light, dramatic rim lighting, golden hour glow, neon accents)
3. **Composition**: Include framing, perspective, and focal points (close-up, wide shot, bird's eye view, rule of thirds)
4. **Style**: Add artistic style when appropriate (cinematic, photorealistic, digital art, oil painting, etc.)
5. **Atmosphere**: Include mood and atmosphere descriptors (ethereal, moody, vibrant, serene, dynamic)
6. **Quality Markers**: Add quality enhancers (highly detailed, 8K, professional photography, masterpiece)

## Rules
- Keep the enhanced prompt under 200 words
- Preserve the user's core intent - don't change WHAT they want, enhance HOW it looks
- Don't add text/words to appear in the image unless requested
- Output ONLY the enhanced prompt, no explanations or markdown
- Make every image feel premium, professional, and visually striking

## Examples

User: "a cat sitting on a windowsill"
Enhanced: "A majestic tabby cat lounging on a sun-drenched windowsill, soft golden hour light streaming through sheer curtains, dust particles floating in the warm light beams, cozy interior with potted plants, shallow depth of field, photorealistic, intimate portrait style, warm amber and cream color palette, highly detailed fur texture"

User: "cyberpunk city"
Enhanced: "Sprawling cyberpunk metropolis at night, towering neon-lit skyscrapers piercing through low-hanging smog, holographic advertisements reflecting off rain-slicked streets, flying vehicles with glowing thrusters, diverse crowd of augmented humans, pink and cyan neon color scheme, cinematic wide-angle shot, blade runner aesthetic, volumetric fog, raytraced reflections, 8K ultra detailed"

User: "a peaceful forest"
Enhanced: "Ancient moss-covered forest with towering redwood trees, ethereal morning mist weaving between massive trunks, soft dappled sunlight filtering through the dense canopy, ferns and wildflowers carpeting the forest floor, a gentle stream with crystal-clear water, mystical and serene atmosphere, nature photography style, rich greens and earth tones, depth and scale, photorealistic, National Geographic quality"`;

        const result = await ai.models.generateContent({
          model: 'gemini-2.0-flash',
          contents: [{
            role: 'user',
            parts: [{ text: `Enhance this image prompt:\n\n"${prompt}"` }]
          }],
          systemInstruction: systemPrompt,
        });

        const enhanced = result.candidates?.[0]?.content?.parts?.[0]?.text?.trim();
        if (enhanced && enhanced.length > 10) {
          enhancedPrompt = enhanced;
          console.log(`[${jobId}] Enhanced prompt: ${enhancedPrompt.substring(0, 100)}...`);
        }
      } catch (enhanceError) {
        console.warn(`[${jobId}] Prompt enhancement failed, using original:`, enhanceError.message);
      }
    } else {
      console.log(`[${jobId}] No GEMINI_API_KEY, using original prompt`);
    }

    // Call fal.ai nano-banana-pro API with enhanced prompt
    console.log(`[${jobId}] Sending to fal.ai...`);
    const falResult = await fal.run('fal-ai/nano-banana-pro', {
      input: {
        prompt: enhancedPrompt,
        num_images: Math.min(numImages, 4),
        aspect_ratio: aspectRatio,
        resolution,
        output_format: 'png',
      },
    });
    console.log(`[${jobId}] Generated ${falResult.data?.images?.length || 0} images`);

    // SDK returns { data, requestId }
    const images = falResult.data?.images;
    if (!images || images.length === 0) {
      throw new Error('No images generated');
    }

    // Download and save each generated image as an asset
    const generatedAssets = [];

    for (let i = 0; i < images.length; i++) {
      const imageData = images[i];
      const imageId = randomUUID();
      const imagePath = join(session.assetsDir, `${imageId}.png`);
      const thumbPath = join(session.assetsDir, `${imageId}_thumb.jpg`);

      console.log(`[${jobId}] Downloading image ${i + 1}...`);

      // Download image
      const imageResponse = await fetch(imageData.url);
      if (!imageResponse.ok) {
        throw new Error(`Failed to download image: ${imageResponse.status}`);
      }

      const buffer = await imageResponse.arrayBuffer();
      writeFileSync(imagePath, Buffer.from(buffer));

      // Generate thumbnail
      try {
        await runFFmpeg([
          '-y', '-i', imagePath,
          '-vf', 'scale=160:90:force_original_aspect_ratio=decrease,pad=160:90:(ow-iw)/2:(oh-ih)/2',
          '-frames:v', '1',
          thumbPath
        ], jobId);
      } catch (e) {
        console.warn(`[${jobId}] Thumbnail generation failed:`, e.message);
      }

      const { stat } = await import('fs/promises');
      const stats = await stat(imagePath);

      // Create short filename from prompt
      const shortPrompt = prompt.substring(0, 30).replace(/[^a-zA-Z0-9\s]/g, '').replace(/\s+/g, '-');

      const asset = {
        id: imageId,
        type: 'image',
        filename: `picasso-${shortPrompt}.png`,
        path: imagePath,
        thumbPath: existsSync(thumbPath) ? thumbPath : null,
        duration: 5, // Default 5 seconds for images on timeline
        size: stats.size,
        width: imageData.width || 1024,
        height: imageData.height || 1024,
        createdAt: Date.now(),
        aiGenerated: true,
        generatedBy: 'picasso',
        prompt: prompt, // Original user prompt
        enhancedPrompt: enhancedPrompt !== prompt ? enhancedPrompt : undefined, // Enhanced prompt if different
      };

      session.assets.set(imageId, asset);
      generatedAssets.push({
        id: imageId,
        filename: asset.filename,
        width: asset.width,
        height: asset.height,
        thumbnailUrl: `/session/${sessionId}/assets/${imageId}/thumbnail`,
        streamUrl: `/session/${sessionId}/assets/${imageId}/stream`,
      });

      console.log(`[${jobId}] Saved image: ${asset.filename} (${(stats.size / 1024).toFixed(1)} KB)`);
    }

    saveAssetMetadata(session); // Persist asset metadata to disk
    console.log(`[${jobId}] === PICASSO COMPLETE ===\n`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      images: generatedAssets,
      description: falResult.description,
    }));

  } catch (error) {
    console.error('Image generation error:', error);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Generate video from image using fal.ai (DiCaprio agent)
async function handleGenerateVideo(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  const falApiKey = process.env.FAL_KEY || process.env.FAL_API_KEY;
  if (!falApiKey) {
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'FAL_KEY or FAL_API_KEY not configured in .dev.vars' }));
    return;
  }

  const geminiApiKey = process.env.GEMINI_API_KEY;

  try {
    const body = await parseBody(req);
    const { prompt, imageAssetId, duration = 5 } = body;

    if (!prompt) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'prompt is required' }));
      return;
    }

    if (!imageAssetId) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'imageAssetId is required' }));
      return;
    }

    // Get the source image asset
    const imageAsset = session.assets.get(imageAssetId);
    if (!imageAsset || imageAsset.type !== 'image') {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'Image asset not found' }));
      return;
    }

    const jobId = sessionId.substring(0, 8);
    console.log(`\n[${jobId}] === DICAPRIO: GENERATE VIDEO ===`);
    console.log(`[${jobId}] User prompt: ${prompt}`);
    console.log(`[${jobId}] Source image: ${imageAsset.filename}`);
    console.log(`[${jobId}] Duration: ${duration}s`);

    // Enhance prompt using Gemini for better video generation
    let enhancedPrompt = prompt;
    if (geminiApiKey) {
      try {
        console.log(`[${jobId}] Enhancing prompt with DiCaprio AI...`);
        const ai = new GoogleGenAI({ apiKey: geminiApiKey });

        const systemPrompt = `You are DiCaprio, an expert AI prompt engineer specializing in image-to-video generation. Your role is to transform simple motion requests into detailed, cinematic prompts that produce stunning videos.

## Your Expertise
- Deep knowledge of cinematography, camera movements, and film techniques
- Understanding of timing, pacing, and motion dynamics
- Mastery of visual storytelling through movement
- Knowledge of video generation model capabilities

## Prompt Enhancement Guidelines

1. **Camera Movement**: Be specific about camera motion (dolly, pan, tilt, zoom, crane, tracking, handheld)
2. **Motion Direction**: Specify direction and speed (slow zoom in, gentle pan left, dynamic push forward)
3. **Subject Motion**: Describe how elements in the scene should move (hair flowing, leaves rustling, water rippling)
4. **Atmosphere**: Include atmospheric effects (light rays moving, dust particles, fog drifting)
5. **Timing**: Use terms like "gradual", "sudden", "rhythmic", "smooth", "cinematic"

## Response Format
Return ONLY the enhanced prompt text. No explanations, no quotes, no markdown.

## Example Input -> Output
Input: "make it move"
Output: "Cinematic slow zoom in with subtle parallax movement, gentle ambient motion with soft light rays drifting through the scene, atmospheric particles floating in the air, smooth and dreamlike camera drift"

Input: "zoom out"
Output: "Epic reveal shot with slow cinematic zoom out, camera gently pulling back to reveal the full scene, subtle atmospheric haze and soft light flares, smooth dolly movement with slight vertical lift"`;

        const result = await ai.models.generateContent({
          model: 'gemini-2.0-flash',
          contents: [
            { role: 'user', parts: [{ text: systemPrompt }] },
            { role: 'user', parts: [{ text: `Enhance this video motion prompt: "${prompt}"` }] }
          ],
        });

        enhancedPrompt = result.candidates[0].content.parts[0].text.trim();
        console.log(`[${jobId}] Enhanced prompt: ${enhancedPrompt.substring(0, 100)}...`);
      } catch (e) {
        console.log(`[${jobId}] Prompt enhancement failed, using original: ${e.message}`);
      }
    }

    // Upload image to fal.ai storage to get a URL (handles large files)
    console.log(`[${jobId}] Uploading image to fal.ai storage...`);
    const imageBuffer = readFileSync(imageAsset.path);
    const mimeType = imageAsset.filename.toLowerCase().endsWith('.png') ? 'image/png' : 'image/jpeg';
    const imageBlob = new Blob([imageBuffer], { type: mimeType });
    const uploadedImageUrl = await fal.storage.upload(imageBlob);
    console.log(`[${jobId}] Image uploaded: ${uploadedImageUrl.substring(0, 50)}...`);

    console.log(`[${jobId}] Calling fal.ai video generation...`);

    // Use fal.ai SDK with automatic queue handling
    const falResult = await fal.subscribe('fal-ai/kling-video/v1.5/pro/image-to-video', {
      input: {
        prompt: enhancedPrompt,
        image_url: uploadedImageUrl,
        duration: duration === 10 ? '10' : '5',
        aspect_ratio: '16:9',
      },
      logs: true,
      onQueueUpdate: (update) => {
        if (update.status === 'IN_QUEUE') {
          console.log(`[${jobId}] Queued at position ${update.position || '?'}`);
        } else if (update.status === 'IN_PROGRESS') {
          console.log(`[${jobId}] Processing...`);
        }
      },
    });

    console.log(`[${jobId}] Video generation complete!`);

    // Download the generated video - SDK returns { data, requestId }
    const videoUrl = falResult.data?.video?.url;
    if (!videoUrl) {
      throw new Error('No video URL in response');
    }

    const videoResponse = await fetch(videoUrl);
    if (!videoResponse.ok) {
      throw new Error('Failed to download generated video');
    }

    const videoBuffer = Buffer.from(await videoResponse.arrayBuffer());

    // Save to assets
    const videoId = randomUUID();
    const shortPrompt = prompt.substring(0, 30).replace(/[^a-zA-Z0-9]/g, '-').replace(/-+/g, '-');
    const videoPath = join(session.assetsDir, `${videoId}.mp4`);
    const thumbPath = join(session.assetsDir, `${videoId}_thumb.jpg`);

    writeFileSync(videoPath, videoBuffer);

    // Generate thumbnail
    await runFFmpeg([
      '-y', '-i', videoPath,
      '-vf', 'scale=320:180:force_original_aspect_ratio=decrease,pad=320:180:(ow-iw)/2:(oh-ih)/2',
      '-frames:v', '1',
      thumbPath
    ], jobId);

    // Get video duration using ffprobe
    let videoDuration = duration;
    try {
      const probeResult = await new Promise((resolve, reject) => {
        const proc = spawn('ffprobe', [
          '-v', 'error',
          '-show_entries', 'format=duration',
          '-of', 'json',
          videoPath
        ]);
        let output = '';
        proc.stdout.on('data', d => output += d.toString());
        proc.on('close', code => {
          if (code === 0) {
            try {
              const data = JSON.parse(output);
              resolve(parseFloat(data.format.duration) || duration);
            } catch { resolve(duration); }
          } else {
            resolve(duration);
          }
        });
        proc.on('error', () => resolve(duration));
      });
      videoDuration = probeResult;
    } catch (e) {
      console.log(`[${jobId}] Could not probe video duration, using default`);
    }

    const { stat } = await import('fs/promises');
    const stats = await stat(videoPath);

    // Create asset entry
    const asset = {
      id: videoId,
      filename: `dicaprio-${shortPrompt}.mp4`,
      originalFilename: `dicaprio-${shortPrompt}.mp4`,
      type: 'video',
      path: videoPath,
      thumbPath: existsSync(thumbPath) ? thumbPath : null,
      size: stats.size,
      duration: videoDuration,
      width: 1920,
      height: 1080,
      uploadedAt: Date.now(),
      generatedBy: 'dicaprio',
      sourcePrompt: prompt,
      enhancedPrompt: enhancedPrompt,
      sourceImageId: imageAssetId,
    };

    session.assets.set(videoId, asset);
    saveAssetMetadata(session);

    console.log(`[${jobId}] Saved video: ${asset.filename} (${(stats.size / 1024 / 1024).toFixed(1)} MB)`);
    console.log(`[${jobId}] === DICAPRIO COMPLETE ===\n`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      video: {
        id: videoId,
        filename: asset.filename,
        thumbnailUrl: `/session/${sessionId}/assets/${videoId}/thumbnail`,
        streamUrl: `/session/${sessionId}/assets/${videoId}/stream`,
        duration: videoDuration,
      },
    }));

  } catch (error) {
    console.error('Video generation error:', error);
    console.error('Error stack:', error.stack);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Restyle video using LTX-2 video-to-video (DiCaprio agent)
async function handleRestyleVideo(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  const falApiKey = process.env.FAL_KEY || process.env.FAL_API_KEY;
  if (!falApiKey) {
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'FAL_KEY or FAL_API_KEY not configured in .dev.vars' }));
    return;
  }

  const geminiApiKey = process.env.GEMINI_API_KEY;

  try {
    const body = await parseBody(req);
    const { prompt, videoAssetId } = body;

    if (!prompt) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'prompt is required' }));
      return;
    }

    if (!videoAssetId) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'videoAssetId is required' }));
      return;
    }

    // Get the source video asset
    const videoAsset = session.assets.get(videoAssetId);
    if (!videoAsset || videoAsset.type !== 'video') {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'Video asset not found' }));
      return;
    }

    const jobId = sessionId.substring(0, 8);
    console.log(`\n[${jobId}] === DICAPRIO: RESTYLE VIDEO ===`);
    console.log(`[${jobId}] User prompt: ${prompt}`);
    console.log(`[${jobId}] Source video: ${videoAsset.filename}`);

    // Enhance prompt using Gemini for better style transfer
    let enhancedPrompt = prompt;
    if (geminiApiKey) {
      try {
        console.log(`[${jobId}] Enhancing style prompt with AI...`);
        const ai = new GoogleGenAI({ apiKey: geminiApiKey });

        const result = await ai.models.generateContent({
          model: 'gemini-2.0-flash',
          contents: [{
            role: 'user',
            parts: [{
              text: `You are an expert at writing prompts for AI video style transfer. Transform this simple style request into a detailed, cinematic prompt that will produce stunning results.

User request: "${prompt}"

Write a detailed prompt describing the visual style. Include:
- Color grading and mood
- Texture and grain quality
- Lighting style
- Overall aesthetic
- Any specific visual effects

Return ONLY the enhanced prompt, no explanations.`
            }]
          }],
        });

        enhancedPrompt = result.candidates[0].content.parts[0].text.trim();
        console.log(`[${jobId}] Enhanced prompt: ${enhancedPrompt.substring(0, 100)}...`);
      } catch (e) {
        console.log(`[${jobId}] Prompt enhancement failed, using original: ${e.message}`);
      }
    }

    // Compress video for upload (fal.ai has size limits)
    const compressedPath = join(TEMP_DIR, `${jobId}-compressed.mp4`);
    console.log(`[${jobId}] Compressing video for upload...`);

    // Compress to 720p max, lower bitrate for faster upload
    await runFFmpeg([
      '-y', '-i', videoAsset.path,
      '-vf', 'scale=-2:720',  // Max 720p height, maintain aspect
      '-c:v', 'libx264',
      '-preset', 'fast',
      '-crf', '28',  // Lower quality but smaller file
      '-c:a', 'aac',
      '-b:a', '128k',
      '-t', '10',  // Max 10 seconds for API limits
      compressedPath
    ], jobId);

    // Upload compressed video to fal.ai storage
    console.log(`[${jobId}] Uploading compressed video to fal.ai storage...`);
    const videoBuffer = readFileSync(compressedPath);
    const fileSizeMB = videoBuffer.length / (1024 * 1024);
    console.log(`[${jobId}] Compressed size: ${fileSizeMB.toFixed(1)} MB`);

    const videoBlob = new Blob([videoBuffer], { type: 'video/mp4' });
    const uploadedVideoUrl = await fal.storage.upload(videoBlob);
    console.log(`[${jobId}] Video uploaded: ${uploadedVideoUrl.substring(0, 50)}...`);

    // Clean up compressed file
    try { unlinkSync(compressedPath); } catch (e) {}

    console.log(`[${jobId}] Calling fal.ai LTX-2 video-to-video...`);

    // Use fal.ai SDK with automatic queue handling
    const falResult = await fal.subscribe('fal-ai/ltx-2-19b/video-to-video', {
      input: {
        prompt: enhancedPrompt,
        video_url: uploadedVideoUrl,
        num_inference_steps: 40,
        guidance_scale: 3,
        video_strength: 0.7,
        generate_audio: false,
        video_quality: 'high',
      },
      logs: true,
      onQueueUpdate: (update) => {
        if (update.status === 'IN_QUEUE') {
          console.log(`[${jobId}] Queued at position ${update.position || '?'}`);
        } else if (update.status === 'IN_PROGRESS') {
          console.log(`[${jobId}] Processing...`);
        }
      },
    });

    console.log(`[${jobId}] Video restyle complete!`);

    // Download the restyled video - SDK returns { data, requestId }
    const outputVideoUrl = falResult.data?.video?.url;
    if (!outputVideoUrl) {
      throw new Error('No video URL in response');
    }

    const videoResponse = await fetch(outputVideoUrl);
    if (!videoResponse.ok) {
      throw new Error('Failed to download restyled video');
    }

    const outputBuffer = Buffer.from(await videoResponse.arrayBuffer());

    // Save to assets
    const newVideoId = randomUUID();
    const shortPrompt = prompt.substring(0, 20).replace(/[^a-zA-Z0-9]/g, '-').replace(/-+/g, '-');
    const outputPath = join(session.assetsDir, `${newVideoId}.mp4`);
    const thumbPath = join(session.assetsDir, `${newVideoId}_thumb.jpg`);

    writeFileSync(outputPath, outputBuffer);

    // Generate thumbnail
    await runFFmpeg([
      '-y', '-i', outputPath,
      '-vf', 'scale=320:180:force_original_aspect_ratio=decrease,pad=320:180:(ow-iw)/2:(oh-ih)/2',
      '-frames:v', '1',
      thumbPath
    ], jobId);

    // Get video duration
    let videoDuration = videoAsset.duration || 5;
    try {
      const probeResult = await new Promise((resolve) => {
        const proc = spawn('ffprobe', ['-v', 'error', '-show_entries', 'format=duration', '-of', 'json', outputPath]);
        let output = '';
        proc.stdout.on('data', d => output += d.toString());
        proc.on('close', code => {
          if (code === 0) {
            try { resolve(parseFloat(JSON.parse(output).format.duration)); }
            catch { resolve(videoDuration); }
          } else resolve(videoDuration);
        });
        proc.on('error', () => resolve(videoDuration));
      });
      videoDuration = probeResult;
    } catch (e) { /* use default */ }

    const { stat } = await import('fs/promises');
    const stats = await stat(outputPath);

    const asset = {
      id: newVideoId,
      filename: `restyled-${shortPrompt}.mp4`,
      originalFilename: `restyled-${shortPrompt}.mp4`,
      type: 'video',
      path: outputPath,
      thumbPath: existsSync(thumbPath) ? thumbPath : null,
      size: stats.size,
      duration: videoDuration,
      width: falResult.video?.width || 1280,
      height: falResult.video?.height || 720,
      uploadedAt: Date.now(),
      generatedBy: 'dicaprio-restyle',
      sourcePrompt: prompt,
      sourceVideoId: videoAssetId,
    };

    session.assets.set(newVideoId, asset);
    saveAssetMetadata(session);

    console.log(`[${jobId}] Saved restyled video: ${asset.filename}`);
    console.log(`[${jobId}] === DICAPRIO RESTYLE COMPLETE ===\n`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      video: {
        id: newVideoId,
        filename: asset.filename,
        thumbnailUrl: `/session/${sessionId}/assets/${newVideoId}/thumbnail`,
        streamUrl: `/session/${sessionId}/assets/${newVideoId}/stream`,
        duration: videoDuration,
      },
    }));

  } catch (error) {
    console.error('Video restyle error:', error);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Remove video background using Bria (DiCaprio agent)
async function handleRemoveVideoBg(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  const falApiKey = process.env.FAL_KEY || process.env.FAL_API_KEY;
  if (!falApiKey) {
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'FAL_KEY or FAL_API_KEY not configured in .dev.vars' }));
    return;
  }

  try {
    const body = await parseBody(req);
    const { videoAssetId } = body;

    if (!videoAssetId) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'videoAssetId is required' }));
      return;
    }

    // Get the source video asset
    const videoAsset = session.assets.get(videoAssetId);
    if (!videoAsset || videoAsset.type !== 'video') {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'Video asset not found' }));
      return;
    }

    const jobId = sessionId.substring(0, 8);
    console.log(`\n[${jobId}] === DICAPRIO: REMOVE VIDEO BACKGROUND ===`);
    console.log(`[${jobId}] Source video: ${videoAsset.filename}`);

    // Compress video for upload (fal.ai has size limits)
    const compressedPath = join(TEMP_DIR, `${jobId}-bg-compressed.mp4`);
    console.log(`[${jobId}] Compressing video for upload...`);

    // Compress to 720p max, lower bitrate for faster upload
    await runFFmpeg([
      '-y', '-i', videoAsset.path,
      '-vf', 'scale=-2:720',  // Max 720p height, maintain aspect
      '-c:v', 'libx264',
      '-preset', 'fast',
      '-crf', '28',  // Lower quality but smaller file
      '-c:a', 'aac',
      '-b:a', '128k',
      '-t', '10',  // Max 10 seconds for API limits
      compressedPath
    ], jobId);

    // Upload compressed video to fal.ai storage
    console.log(`[${jobId}] Uploading compressed video to fal.ai storage...`);
    const videoBuffer = readFileSync(compressedPath);
    const fileSizeMB = videoBuffer.length / (1024 * 1024);
    console.log(`[${jobId}] Compressed size: ${fileSizeMB.toFixed(1)} MB`);

    const videoBlob = new Blob([videoBuffer], { type: 'video/mp4' });
    const uploadedVideoUrl = await fal.storage.upload(videoBlob);
    console.log(`[${jobId}] Video uploaded: ${uploadedVideoUrl.substring(0, 50)}...`);

    // Clean up compressed file
    try { unlinkSync(compressedPath); } catch (e) {}

    console.log(`[${jobId}] Calling fal.ai Bria video background removal...`);

    // Use fal.ai SDK with automatic queue handling
    const falResult = await fal.subscribe('fal-ai/ben/v2/video', {
      input: {
        video_url: uploadedVideoUrl,
        output_format: 'webm',  // WebM for transparency support
      },
      logs: true,
      onQueueUpdate: (update) => {
        if (update.status === 'IN_QUEUE') {
          console.log(`[${jobId}] Queued at position ${update.position || '?'}`);
        } else if (update.status === 'IN_PROGRESS') {
          console.log(`[${jobId}] Processing...`);
        }
      },
    });

    console.log(`[${jobId}] Background removal complete!`);

    // Download the processed video - SDK returns { data, requestId }
    const outputVideoUrl = falResult.data?.video?.url;
    if (!outputVideoUrl) {
      throw new Error('No video URL in response');
    }

    const videoResponse = await fetch(outputVideoUrl);
    if (!videoResponse.ok) {
      throw new Error('Failed to download processed video');
    }

    const outputBuffer = Buffer.from(await videoResponse.arrayBuffer());

    // Save to assets (webm for transparency support)
    const newVideoId = randomUUID();
    const baseName = videoAsset.filename.replace(/\.[^/.]+$/, '');
    const outputPath = join(session.assetsDir, `${newVideoId}.webm`);
    const thumbPath = join(session.assetsDir, `${newVideoId}_thumb.jpg`);

    writeFileSync(outputPath, outputBuffer);

    // Generate thumbnail
    await runFFmpeg([
      '-y', '-i', outputPath,
      '-vf', 'scale=320:180:force_original_aspect_ratio=decrease,pad=320:180:(ow-iw)/2:(oh-ih)/2',
      '-frames:v', '1',
      thumbPath
    ], jobId);

    // Get video duration
    let videoDuration = videoAsset.duration || 5;
    try {
      const probeResult = await new Promise((resolve) => {
        const proc = spawn('ffprobe', ['-v', 'error', '-show_entries', 'format=duration', '-of', 'json', outputPath]);
        let output = '';
        proc.stdout.on('data', d => output += d.toString());
        proc.on('close', code => {
          if (code === 0) {
            try { resolve(parseFloat(JSON.parse(output).format.duration)); }
            catch { resolve(videoDuration); }
          } else resolve(videoDuration);
        });
        proc.on('error', () => resolve(videoDuration));
      });
      videoDuration = probeResult;
    } catch (e) { /* use default */ }

    const { stat } = await import('fs/promises');
    const stats = await stat(outputPath);

    const asset = {
      id: newVideoId,
      filename: `${baseName}-nobg.webm`,
      originalFilename: `${baseName}-nobg.webm`,
      type: 'video',
      path: outputPath,
      thumbPath: existsSync(thumbPath) ? thumbPath : null,
      size: stats.size,
      duration: videoDuration,
      width: videoAsset.width || 1920,
      height: videoAsset.height || 1080,
      uploadedAt: Date.now(),
      generatedBy: 'dicaprio-remove-bg',
      sourceVideoId: videoAssetId,
      hasTransparency: true,
    };

    session.assets.set(newVideoId, asset);
    saveAssetMetadata(session);

    console.log(`[${jobId}] Saved video: ${asset.filename}`);
    console.log(`[${jobId}] === DICAPRIO REMOVE BG COMPLETE ===\n`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      video: {
        id: newVideoId,
        filename: asset.filename,
        thumbnailUrl: `/session/${sessionId}/assets/${newVideoId}/thumbnail`,
        streamUrl: `/session/${sessionId}/assets/${newVideoId}/stream`,
        duration: videoDuration,
      },
    }));

  } catch (error) {
    console.error('Video background removal error:', error);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Generate batch animations across the timeline based on video content analysis
async function handleGenerateBatchAnimations(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  const apiKey = process.env.GEMINI_API_KEY;
  if (!apiKey) {
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'GEMINI_API_KEY not configured' }));
    return;
  }

  try {
    const body = await parseBody(req);
    const { count = 5, fps = 30, width = 1920, height = 1080 } = body;

    const jobId = sessionId.substring(0, 8);
    console.log(`\n[${jobId}] === GENERATE BATCH ANIMATIONS ===`);
    console.log(`[${jobId}] Requested count: ${count}`);

    // Find the first video asset in the session
    let videoAsset = null;
    for (const asset of session.assets.values()) {
      if (asset.type === 'video' && !asset.aiGenerated) {
        videoAsset = asset;
        break;
      }
    }

    if (!videoAsset) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'No video asset found in session' }));
      return;
    }

    console.log(`[${jobId}] Using video: ${videoAsset.filename} (${videoAsset.duration}s)`);

    // Step 1: Get or create transcription
    console.log(`[${jobId}] Step 1: Getting video transcription...`);
    const transcription = await getOrTranscribeVideo(session, videoAsset, jobId);

    if (!transcription.text) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'Could not transcribe video' }));
      return;
    }

    console.log(`[${jobId}] Transcription: ${transcription.text.substring(0, 200)}...`);

    // Step 2: Use Gemini to plan animations across the video
    console.log(`[${jobId}] Step 2: Planning ${count} animations with AI...`);

    const ai = new GoogleGenAI({ apiKey });
    const planResult = await ai.models.generateContent({
      model: 'gemini-2.0-flash',
      contents: [{
        role: 'user',
        parts: [{
          text: `You are a video editor planning motion graphics animations for a video. Analyze this transcript and plan exactly ${count} animations that would enhance the video.

VIDEO TRANSCRIPT:
"${transcription.text}"

VIDEO DURATION: ${videoAsset.duration} seconds

WORD TIMESTAMPS (for timing reference):
${transcription.words?.slice(0, 100).map(w => `[${w.start.toFixed(1)}s] ${w.text}`).join(' ') || 'Not available'}

Plan exactly ${count} animations. Each should:
1. Be placed at a strategic moment in the video (intro, key points, transitions, outro)
2. Have a specific purpose (introduce topic, highlight key point, transition, call-to-action, etc.)
3. Be relevant to the content being discussed at that timestamp

Return ONLY valid JSON (no markdown):
{
  "animations": [
    {
      "type": "intro" | "highlight" | "transition" | "callout" | "outro",
      "startTime": <seconds where animation should appear>,
      "duration": <animation duration in seconds, typically 3-5>,
      "title": "<short title for the animation>",
      "description": "<detailed description of what the animation should show, including specific text, colors, style>",
      "relevantContent": "<what the video is discussing at this point>"
    }
  ]
}

Guidelines:
- First animation should typically be an intro (startTime: 0)
- Last animation could be an outro or call-to-action
- Space animations throughout the video, not clustered together
- Each animation should enhance understanding or engagement
- Be specific about visual style, colors, and text content`
        }]
      }],
    });

    let animationPlan;
    try {
      const planText = planResult.candidates[0].content.parts[0].text;
      const cleanedPlan = planText.replace(/```json\n?/g, '').replace(/```\n?/g, '').trim();
      animationPlan = JSON.parse(cleanedPlan);
    } catch (parseError) {
      console.error(`[${jobId}] Failed to parse animation plan:`, parseError);
      throw new Error('Failed to parse AI animation plan');
    }

    console.log(`[${jobId}] Planned ${animationPlan.animations.length} animations`);
    animationPlan.animations.forEach((a, i) => {
      console.log(`[${jobId}]   ${i + 1}. ${a.type} at ${a.startTime}s: ${a.title}`);
    });

    // Step 3: Generate each animation
    console.log(`[${jobId}] Step 3: Generating animations...`);
    const generatedAnimations = [];

    for (let i = 0; i < animationPlan.animations.length; i++) {
      const plan = animationPlan.animations[i];
      console.log(`[${jobId}] Generating animation ${i + 1}/${animationPlan.animations.length}: ${plan.title}`);

      const assetId = randomUUID();
      const outputPath = join(session.assetsDir, `${assetId}.mp4`);
      const thumbPath = join(session.assetsDir, `${assetId}_thumb.jpg`);
      const propsPath = join(session.dir, `${jobId}-batch-${i}-props.json`);
      const sceneDataPath = join(session.dir, `${assetId}-scenes.json`);

      // Generate scene data with Gemini
      const sceneResult = await ai.models.generateContent({
        model: 'gemini-2.0-flash',
        contents: [{
          role: 'user',
          parts: [{
            text: `Create a Remotion animation for this video moment.

ANIMATION TYPE: ${plan.type}
TITLE: ${plan.title}
DESCRIPTION: ${plan.description}
CONTEXT: ${plan.relevantContent}
DURATION: ${plan.duration} seconds (${plan.duration * fps} frames)

Generate a scene-based animation. Return ONLY valid JSON:
{
  "scenes": [
    {
      "id": "scene-1",
      "type": "title" | "bullets" | "stats" | "quote" | "callToAction" | "transition",
      "duration": <frames>,
      "content": {
        "title": "optional title text",
        "subtitle": "optional subtitle",
        "items": [{"label": "item text", "icon": "optional emoji"}],
        "stats": [{"value": "100%", "label": "stat name"}],
        "quote": "quote text",
        "author": "quote author",
        "buttonText": "CTA text",
        "backgroundColor": "#hex",
        "textColor": "#hex",
        "accentColor": "#hex"
      }
    }
  ],
  "totalDuration": <total frames>,
  "backgroundColor": "#1a1a2e"
}

Make it visually engaging with good color choices. Use 2-4 scenes for variety.`
          }]
        }],
      });

      let sceneData;
      try {
        const sceneText = sceneResult.candidates[0].content.parts[0].text;
        const cleanedScene = sceneText.replace(/```json\n?/g, '').replace(/```\n?/g, '').trim();
        sceneData = JSON.parse(cleanedScene);
      } catch (parseError) {
        console.error(`[${jobId}] Failed to parse scene data for animation ${i + 1}, using fallback`);
        // Create a simple fallback animation
        sceneData = {
          scenes: [{
            id: 'scene-1',
            type: 'title',
            duration: plan.duration * fps,
            content: {
              title: plan.title,
              subtitle: plan.description.substring(0, 50),
              backgroundColor: '#1a1a2e',
              textColor: '#ffffff',
              accentColor: '#6366f1'
            }
          }],
          totalDuration: plan.duration * fps,
          backgroundColor: '#1a1a2e'
        };
      }

      // Save scene data
      writeFileSync(sceneDataPath, JSON.stringify(sceneData, null, 2));
      writeFileSync(propsPath, JSON.stringify(sceneData, null, 2));

      const totalDuration = sceneData.totalDuration || sceneData.scenes.reduce((sum, s) => sum + s.duration, 0);
      const durationInSeconds = totalDuration / fps;

      // Render with Remotion
      const remotionArgs = [
        'remotion', 'render',
        'src/remotion/index.tsx',
        'DynamicAnimation',
        outputPath,
        '--props', propsPath,
        '--frames', `0-${totalDuration - 1}`,
        '--fps', String(fps),
        '--width', String(width),
        '--height', String(height),
        '--codec', 'h264',
        '--overwrite',
        '--gl=angle', // Use Metal GPU acceleration on macOS
      ];

      await new Promise((resolve, reject) => {
        const proc = spawn('npx', remotionArgs, {
          cwd: process.cwd(),
          stdio: ['pipe', 'pipe', 'pipe'],
        });

        let stderr = '';
        proc.stderr.on('data', (data) => {
          stderr += data.toString();
        });

        proc.on('close', (code) => {
          if (code === 0) {
            resolve();
          } else {
            reject(new Error(`Remotion render failed: ${stderr.substring(0, 200)}`));
          }
        });

        proc.on('error', (err) => {
          reject(new Error(`Failed to start Remotion: ${err.message}`));
        });
      });

      // Generate thumbnail
      try {
        await runFFmpeg([
          '-y', '-i', outputPath,
          '-vf', 'scale=160:90:force_original_aspect_ratio=decrease,pad=160:90:(ow-iw)/2:(oh-ih)/2',
          '-frames:v', '1',
          thumbPath
        ], jobId);
      } catch (e) {
        console.warn(`[${jobId}] Thumbnail failed for animation ${i + 1}`);
      }

      // Clean up props file
      try { unlinkSync(propsPath); } catch (e) {}

      const { stat } = await import('fs/promises');
      const stats = await stat(outputPath);

      // Create asset entry
      const asset = {
        id: assetId,
        type: 'video',
        filename: `${plan.type}-${plan.title.replace(/[^a-zA-Z0-9]/g, '-').substring(0, 20)}.mp4`,
        path: outputPath,
        thumbPath: existsSync(thumbPath) ? thumbPath : null,
        duration: durationInSeconds,
        size: stats.size,
        width,
        height,
        createdAt: Date.now(),
        aiGenerated: true,
        sceneData,
        sceneDataPath,
        description: plan.description,
      };

      session.assets.set(assetId, asset);

      generatedAnimations.push({
        assetId,
        filename: asset.filename,
        duration: durationInSeconds,
        startTime: plan.startTime,
        type: plan.type,
        title: plan.title,
        thumbnailUrl: `/session/${sessionId}/assets/${assetId}/thumbnail`,
        streamUrl: `/session/${sessionId}/assets/${assetId}/stream`,
      });

      console.log(`[${jobId}] âœ“ Animation ${i + 1} complete: ${asset.filename}`);
    }

    console.log(`[${jobId}] === BATCH GENERATION COMPLETE ===`);
    console.log(`[${jobId}] Generated ${generatedAnimations.length} animations\n`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      animations: generatedAnimations,
      videoDuration: videoAsset.duration,
    }));

  } catch (error) {
    console.error('Batch animation generation error:', error);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Analyze video for animation concept (no rendering - for approval workflow)
// Returns transcript and proposed animation scenes for user approval
async function handleAnalyzeForAnimation(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  const apiKey = process.env.GEMINI_API_KEY;
  if (!apiKey) {
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'GEMINI_API_KEY not configured' }));
    return;
  }

  try {
    const body = await parseBody(req);
    const { assetId, type = 'intro', description, startTime, endTime } = body;

    // Debug: log received time range values
    console.log(`[DEBUG] Received analyze request - startTime: ${startTime} (${typeof startTime}), endTime: ${endTime} (${typeof endTime})`);

    // Get the video asset to analyze
    let videoAsset;
    if (assetId) {
      videoAsset = session.assets.get(assetId);
    } else {
      for (const [id, asset] of session.assets) {
        if (asset.type === 'video') {
          videoAsset = asset;
          break;
        }
      }
    }

    if (!videoAsset) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'No video asset found to analyze' }));
      return;
    }

    const jobId = randomUUID();
    const audioPath = join(TEMP_DIR, `${jobId}-audio.mp3`);

    // Determine if we're analyzing a specific time range or the whole video
    const hasTimeRange = typeof startTime === 'number' && typeof endTime === 'number';
    const segmentStart = hasTimeRange ? startTime : 0;
    const segmentDuration = hasTimeRange ? (endTime - startTime) : null;

    console.log(`\n[${jobId}] === ANALYZE VIDEO FOR ${type.toUpperCase()} ANIMATION ===`);
    console.log(`[${jobId}] Analyzing video: ${videoAsset.filename}`);
    if (hasTimeRange) {
      console.log(`[${jobId}] Time range: ${segmentStart.toFixed(1)}s - ${endTime.toFixed(1)}s (${segmentDuration.toFixed(1)}s segment)`);
    }

    // Step 1: Transcribe the video (or just the specified segment)
    console.log(`[${jobId}] Step 1: Transcribing ${hasTimeRange ? 'segment' : 'video'}...`);

    // Extract audio from video - optionally just from the specified time range
    const ffmpegArgs = ['-y', '-i', videoAsset.path];
    if (hasTimeRange) {
      // Use -ss for seeking and -t for duration to extract only the segment
      ffmpegArgs.push('-ss', segmentStart.toString());
      ffmpegArgs.push('-t', segmentDuration.toString());
    }
    ffmpegArgs.push('-vn', '-acodec', 'libmp3lame', '-ar', '16000', '-ac', '1', '-q:a', '9', audioPath);

    await runFFmpeg(ffmpegArgs, jobId);

    // Get video duration
    const durationOutput = await runFFmpegProbe([
      '-v', 'error',
      '-show_entries', 'format=duration',
      '-of', 'default=noprint_wrappers=1:nokey=1',
      videoAsset.path
    ], jobId);
    const totalDuration = parseFloat(durationOutput.trim()) || 60;
    const analyzedDuration = hasTimeRange ? segmentDuration : totalDuration;

    let transcription;
    const hasLocalWhisper = await checkLocalWhisper();
    const openaiKey = process.env.OPENAI_API_KEY;

    // Helper function to transcribe with Gemini (always available as fallback)
    const transcribeWithGemini = async () => {
      console.log(`[${jobId}]    Using Gemini for transcription...`);
      const ai = new GoogleGenAI({ apiKey });
      const audioBuffer = readFileSync(audioPath);
      const fileSizeKB = audioBuffer.length / 1024;
      console.log(`[${jobId}]    Audio file size: ${fileSizeKB.toFixed(1)}KB`);

      // Check if audio file is too small (likely no audio track in video)
      if (audioBuffer.length < 1000) {
        console.log(`[${jobId}]    Audio file too small, video may have no audio track`);
        return { text: '', words: [] };
      }

      const audioBase64 = audioBuffer.toString('base64');

      const result = await ai.models.generateContent({
        model: 'gemini-2.0-flash',
        contents: [{
          role: 'user',
          parts: [
            { inlineData: { mimeType: 'audio/mp3', data: audioBase64 } },
            { text: `Transcribe this audio. Return ONLY the text content. Duration: ${analyzedDuration.toFixed(1)}s` }
          ]
        }],
      });

      return {
        text: result.candidates?.[0]?.content?.parts?.[0]?.text || '',
        words: [],
      };
    };

    if (hasLocalWhisper) {
      try {
        console.log(`[${jobId}]    Using local Whisper...`);
        transcription = await runLocalWhisper(audioPath, jobId);
      } catch (whisperError) {
        console.log(`[${jobId}]    Local Whisper failed: ${whisperError.message}`);
        console.log(`[${jobId}]    Falling back to Gemini...`);
        transcription = await transcribeWithGemini();
      }
    } else if (openaiKey) {
      console.log(`[${jobId}]    Using OpenAI Whisper API...`);
      const FormData = (await import('node-fetch')).default.FormData || global.FormData;
      const formData = new FormData();
      formData.append('file', createReadStream(audioPath));
      formData.append('model', 'whisper-1');
      formData.append('response_format', 'verbose_json');
      formData.append('timestamp_granularities[]', 'word');

      const whisperResponse = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: { 'Authorization': `Bearer ${openaiKey}` },
        body: formData,
      });

      if (!whisperResponse.ok) {
        throw new Error(`Whisper API error: ${whisperResponse.status}`);
      }

      const whisperResult = await whisperResponse.json();
      transcription = {
        text: whisperResult.text || '',
        words: (whisperResult.words || []).map(w => ({
          text: w.word,
          start: w.start,
          end: w.end,
        })),
      };
    } else {
      // Use Gemini as fallback
      transcription = await transcribeWithGemini();
    }

    console.log(`[${jobId}] Transcription complete: ${transcription.text.substring(0, 100)}...`);

    // Clean up audio file
    try { unlinkSync(audioPath); } catch (e) {}

    // Step 2: Generate animation concept (scenes) without rendering
    console.log(`[${jobId}] Step 2: Generating animation concept...`);

    const genAI = new GoogleGenAI({ apiKey });

    const typePrompts = {
      intro: `Create an engaging INTRO animation that hooks viewers and introduces the video topic.
The intro should:
- Start with an attention-grabbing title or hook
- Tease what viewers will learn/see
- Build excitement for the content
- Be 4-8 seconds (120-240 frames at 30fps)`,

      outro: `Create a compelling OUTRO animation that wraps up the video.
The outro should:
- Summarize key takeaways
- Include a call-to-action (subscribe, like, etc.)
- Thank viewers
- Be 5-10 seconds (150-300 frames at 30fps)`,

      transition: `Create a smooth TRANSITION animation between sections.
The transition should:
- Be brief and visually interesting
- Match the video's tone
- Be 2-4 seconds (60-120 frames at 30fps)`,

      highlight: `Create a HIGHLIGHT animation that emphasizes a key moment.
The highlight should:
- Draw attention to an important point
- Use dynamic motion and colors
- Be 3-6 seconds (90-180 frames at 30fps)`,
    };

    // Build time context for the prompt
    const timeContext = hasTimeRange
      ? `\nNOTE: This transcript is from a SPECIFIC SEGMENT of the video (${segmentStart.toFixed(1)}s - ${endTime.toFixed(1)}s, duration: ${segmentDuration.toFixed(1)}s). Create an animation that relates ONLY to what is being discussed in this segment, not the entire video.`
      : '';

    const scenePrompt = `You are a motion graphics designer. Analyze this video transcript and create a contextual ${type} animation concept.

VIDEO TRANSCRIPT:
"${transcription.text}"
${timeContext}

${description ? `USER HINT: "${description}"` : ''}

${typePrompts[type] || typePrompts.intro}

Based on the video content above, return ONLY valid JSON (no markdown) with this structure:
{
  "scenes": [
    {
      "id": "unique-id",
      "type": "title" | "steps" | "features" | "stats" | "text" | "transition" | "gif" | "emoji",
      "duration": <frames at 30fps>,
      "content": {
        "title": "text derived from video content",
        "subtitle": "optional",
        "items": [{"icon": "emoji", "label": "text", "description": "optional"}],
        "stats": [{"value": "number", "label": "text", "numericValue": <integer for counting>}],
        "color": "#hex accent color",
        "backgroundColor": "#hex or null for transparent",
        // For gif scenes - use GIPHY search:
        "gifSearch": "keyword to search for GIF",
        "gifLayout": "fullscreen" | "scattered",
        // For emoji scenes:
        "emojis": [{"emoji": "ðŸ”¥", "x": 50, "y": 50, "scale": 0.2, "animation": "bounce"}]
      }
    }
  ],
  "backgroundColor": "#0a0a0a",
  "totalDuration": <sum of scene durations>,
  "contentSummary": "brief description of what the video is about",
  "keyTopics": ["topic1", "topic2", "topic3"]
}

Scene type notes:
- "gif": Use "gifSearch" to search GIPHY for GIFs (e.g., "mind blown", "celebration", "thumbs up")
- "emoji": Animated emoji scene with animations (pop, bounce, float, pulse)
- "stats": Use numericValue for counting animation (must be a NUMBER)

IMPORTANT: The animation content should directly relate to the video's actual topic and message.
Use specific terms, concepts, and themes from the transcript.
Feel free to add a GIF scene for reactions or emphasis when appropriate!`;

    const sceneResult = await genAI.models.generateContent({
      model: 'gemini-2.0-flash',
      contents: [{ role: 'user', parts: [{ text: scenePrompt }] }],
    });

    let sceneData;
    try {
      const responseText = sceneResult.candidates[0].content.parts[0].text;
      const cleanedResponse = responseText
        .replace(/```json\n?/g, '')
        .replace(/```\n?/g, '')
        .trim();
      sceneData = JSON.parse(cleanedResponse);
    } catch (parseError) {
      console.error(`[${jobId}] Failed to parse Gemini response:`, parseError);
      throw new Error('Failed to parse AI-generated scene data');
    }

    // Post-process GIF scenes - search GIPHY and inject actual URLs
    const giphyKeyForAnalysis = process.env.GIPHY_API_KEY;
    for (const scene of sceneData.scenes) {
      if (scene.type === 'gif' && scene.content) {
        const { gifSearch, gifSearches } = scene.content;
        const searchTerms = gifSearches || (gifSearch ? [gifSearch] : []);

        if (searchTerms.length > 0 && giphyKeyForAnalysis) {
          console.log(`[${jobId}] ðŸŽ¬ Fetching GIFs from GIPHY for concept: ${searchTerms.join(', ')}`);
          scene.content.gifs = [];

          for (const term of searchTerms) {
            try {
              const gifs = await searchGiphy(term, 1);
              if (gifs.length > 0) {
                const gif = gifs[0];
                const gifUrl = gif.images?.fixed_height?.url || gif.images?.original?.url;
                if (gifUrl) {
                  scene.content.gifs.push({
                    src: gifUrl,
                    width: parseInt(gif.images?.fixed_height?.width) || 400,
                    height: parseInt(gif.images?.fixed_height?.height) || 300,
                    title: gif.title || term,
                    searchTerm: term,
                  });
                  console.log(`[${jobId}]    âœ“ Found GIF for "${term}"`);
                }
              }
            } catch (err) {
              console.log(`[${jobId}]    âœ— GIPHY search failed: ${err.message}`);
            }
          }

          if (!scene.content.gifLayout && scene.content.gifs.length === 1) {
            scene.content.gifLayout = 'fullscreen';
          }
        }
      }
    }

    const animationTotalDuration = sceneData.totalDuration || sceneData.scenes.reduce((sum, s) => sum + s.duration, 0);
    const durationInSeconds = animationTotalDuration / 30; // 30 fps

    console.log(`[${jobId}] Analysis complete: ${sceneData.scenes.length} scenes, ${durationInSeconds}s total`);
    console.log(`[${jobId}] === ANALYSIS COMPLETE (awaiting approval) ===\n`);

    // Return the concept for user approval (NOT rendered yet)
    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      concept: {
        type,
        transcript: transcription.text,
        transcriptPreview: transcription.text.substring(0, 500) + (transcription.text.length > 500 ? '...' : ''),
        contentSummary: sceneData.contentSummary,
        keyTopics: sceneData.keyTopics || [],
        scenes: sceneData.scenes,
        totalDuration: animationTotalDuration,
        durationInSeconds,
        backgroundColor: sceneData.backgroundColor,
      },
      videoInfo: {
        filename: videoAsset.filename,
        duration: totalDuration,
        assetId: videoAsset.id,
      },
    }));

  } catch (error) {
    console.error('Animation analysis error:', error);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Render animation from pre-approved concept (skips analysis, uses provided scenes)
async function handleRenderFromConcept(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  try {
    const body = await parseBody(req);
    const { concept, fps = 30, width = 1920, height = 1080 } = body;

    if (!concept || !concept.scenes || concept.scenes.length === 0) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'concept with scenes is required' }));
      return;
    }

    const jobId = randomUUID();
    const assetId = randomUUID();
    const outputPath = join(session.assetsDir, `${assetId}.mp4`);
    const thumbPath = join(session.assetsDir, `${assetId}_thumb.jpg`);
    const propsPath = join(session.dir, `${jobId}-props.json`);

    console.log(`\n[${jobId}] === RENDER FROM APPROVED CONCEPT ===`);
    console.log(`[${jobId}] Type: ${concept.type}, Scenes: ${concept.scenes.length}`);

    const sceneData = {
      scenes: concept.scenes,
      backgroundColor: concept.backgroundColor || '#0a0a0a',
      totalDuration: concept.totalDuration,
      contentSummary: concept.contentSummary,
      keyTopics: concept.keyTopics,
    };

    // Post-process GIF scenes - search GIPHY for any unresolved gif searches
    const giphyKeyForRender = process.env.GIPHY_API_KEY;
    for (const scene of sceneData.scenes) {
      if (scene.type === 'gif' && scene.content) {
        const { gifSearch, gifSearches, gifs } = scene.content;
        const searchTerms = gifSearches || (gifSearch ? [gifSearch] : []);

        // Only search if we have search terms but no resolved GIFs
        if (searchTerms.length > 0 && (!gifs || gifs.length === 0) && giphyKeyForRender) {
          console.log(`[${jobId}] ðŸŽ¬ Resolving GIPHY searches: ${searchTerms.join(', ')}`);
          scene.content.gifs = [];

          for (const term of searchTerms) {
            try {
              const gifsResult = await searchGiphy(term, 1);
              if (gifsResult.length > 0) {
                const gif = gifsResult[0];
                const gifUrl = gif.images?.fixed_height?.url || gif.images?.original?.url;
                if (gifUrl) {
                  scene.content.gifs.push({
                    src: gifUrl,
                    width: parseInt(gif.images?.fixed_height?.width) || 400,
                    height: parseInt(gif.images?.fixed_height?.height) || 300,
                    title: gif.title || term,
                    searchTerm: term,
                  });
                  console.log(`[${jobId}]    âœ“ Resolved GIF for "${term}"`);
                }
              }
            } catch (err) {
              console.log(`[${jobId}]    âœ— GIPHY search failed: ${err.message}`);
            }
          }

          if (!scene.content.gifLayout && scene.content.gifs.length === 1) {
            scene.content.gifLayout = 'fullscreen';
          }
        }
      }
    }

    // Save scene data for future editing (reusable path based on asset ID)
    const sceneDataPath = join(session.dir, `${assetId}-scenes.json`);
    writeFileSync(sceneDataPath, JSON.stringify(sceneData, null, 2));
    console.log(`[${jobId}] Scene data saved to ${sceneDataPath} for future editing`);

    const animationTotalDuration = sceneData.totalDuration || sceneData.scenes.reduce((sum, s) => sum + s.duration, 0);
    const durationInSeconds = animationTotalDuration / fps;

    // Write props to JSON file for Remotion
    writeFileSync(propsPath, JSON.stringify(sceneData, null, 2));
    console.log(`[${jobId}] Props written to ${propsPath}`);
    console.log(`[${jobId}] Scene data:`, JSON.stringify(sceneData, null, 2));

    // Render with Remotion CLI
    console.log(`[${jobId}] Rendering with Remotion...`);

    const remotionArgs = [
      'remotion', 'render',
      'src/remotion/index.tsx',
      'DynamicAnimation',
      outputPath,
      '--props', propsPath,
      '--frames', `0-${animationTotalDuration - 1}`,
      '--fps', String(fps),
      '--width', String(width),
      '--height', String(height),
      '--codec', 'h264',
      '--overwrite',
      '--gl=angle', // Use Metal GPU acceleration on macOS
    ];

    console.log(`[${jobId}] Remotion command: npx ${remotionArgs.join(' ')}`);

    await new Promise((resolve, reject) => {
      const proc = spawn('npx', remotionArgs, {
        cwd: process.cwd(),
        stdio: ['pipe', 'pipe', 'pipe'],
      });

      let stdout = '';
      let stderr = '';

      proc.stdout.on('data', (data) => {
        stdout += data.toString();
        console.log(`[${jobId}] Remotion stdout: ${data.toString().trim()}`);
      });

      proc.stderr.on('data', (data) => {
        stderr += data.toString();
        const lines = data.toString().trim().split('\n');
        lines.forEach(line => {
          console.log(`[${jobId}] Remotion: ${line}`);
        });
      });

      proc.on('close', (code) => {
        if (code === 0) resolve();
        else {
          console.error(`[${jobId}] Remotion failed. stderr: ${stderr.slice(-1000)}`);
          reject(new Error(`Remotion render failed with code ${code}: ${stderr.slice(-500)}`));
        }
      });

      proc.on('error', (err) => reject(new Error(`Failed to start Remotion: ${err.message}`)));
    });

    // Generate thumbnail
    await runFFmpeg([
      '-y', '-i', outputPath,
      '-vf', 'scale=160:90:force_original_aspect_ratio=decrease,pad=160:90:(ow-iw)/2:(oh-ih)/2',
      '-frames:v', '1',
      thumbPath
    ], jobId);

    // Clean up props file
    try { unlinkSync(propsPath); } catch (e) {}

    const { stat } = await import('fs/promises');
    const stats = await stat(outputPath);

    // Create asset entry with scene data for future editing
    const asset = {
      id: assetId,
      type: 'video',
      filename: `${concept.type}-animation-${Date.now()}.mp4`,
      path: outputPath,
      thumbPath: existsSync(thumbPath) ? thumbPath : null,
      duration: durationInSeconds,
      size: stats.size,
      width,
      height,
      createdAt: Date.now(),
      aiGenerated: true,
      contextual: true,
      animationType: concept.type,
      contentSummary: concept.contentSummary,
      sceneCount: concept.scenes.length,
      sceneDataPath, // Store path to scene data for re-editing
      sceneData, // Also keep in memory for quick access
    };

    session.assets.set(assetId, asset);
    saveAssetMetadata(session); // Persist AI-generated flag to disk

    console.log(`[${jobId}] Animation rendered: ${assetId} (${durationInSeconds}s)`);
    console.log(`[${jobId}] === RENDER COMPLETE ===\n`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      assetId,
      filename: asset.filename,
      duration: durationInSeconds,
      type: concept.type,
      sceneCount: concept.scenes.length,
      thumbnailUrl: `/session/${sessionId}/assets/${assetId}/thumbnail`,
      streamUrl: `/session/${sessionId}/assets/${assetId}/stream`,
    }));

  } catch (error) {
    console.error('Render from concept error:', error);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Generate kinetic typography animation from video transcript
// Transcribes video, identifies key phrases, creates animated text scenes synced to audio
async function handleGenerateTranscriptAnimation(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  const apiKey = process.env.GEMINI_API_KEY;
  if (!apiKey) {
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'GEMINI_API_KEY not configured' }));
    return;
  }

  try {
    const body = await parseBody(req);
    const { fps = 30, width = 1920, height = 1080 } = body;

    // Find the first video asset
    let videoAsset = null;
    for (const asset of session.assets.values()) {
      if (asset.type === 'video') {
        videoAsset = asset;
        break;
      }
    }

    if (!videoAsset) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'No video asset found in session' }));
      return;
    }

    const jobId = sessionId.substring(0, 8);
    console.log(`\n[${jobId}] === GENERATE TRANSCRIPT ANIMATION ===`);
    console.log(`[${jobId}] Video: ${videoAsset.filename}`);

    // Step 1: Transcribe the video with word-level timestamps
    console.log(`[${jobId}] Step 1: Transcribing video...`);
    const audioPath = join(TEMP_DIR, `${jobId}-transcript-audio.mp3`);
    const totalDuration = await getVideoDuration(videoAsset.path);

    await runFFmpeg([
      '-y', '-i', videoAsset.path,
      '-vn', '-acodec', 'libmp3lame',
      '-ab', '64k', '-ar', '16000', '-ac', '1',
      audioPath
    ], jobId);

    // Check transcription method
    const hasLocalWhisper = await checkLocalWhisper();
    const openaiKey = process.env.OPENAI_API_KEY;

    // Helper for Gemini fallback
    const transcribeWithGeminiForAnimation = async () => {
      console.log(`[${jobId}]    Using Gemini for transcription...`);
      const audioBuffer = readFileSync(audioPath);
      const audioBase64 = audioBuffer.toString('base64');
      const ai = new GoogleGenAI({ apiKey });
      const geminiResponse = await ai.models.generateContent({
        model: 'gemini-2.0-flash',
        contents: [{ role: 'user', parts: [
          { inlineData: { mimeType: 'audio/mp3', data: audioBase64 } },
          { text: `Transcribe this audio with word timestamps. Duration: ${totalDuration}s. Return JSON: {"text": "...", "words": [{"text": "word", "start": 0.0, "end": 0.5}]}` }
        ]}]
      });
      const respText = geminiResponse.text || '';
      try {
        return JSON.parse(respText);
      } catch {
        const match = respText.match(/\{[\s\S]*\}/);
        return match ? JSON.parse(match[0]) : { text: respText, words: [] };
      }
    };

    let transcription;
    if (hasLocalWhisper) {
      try {
        console.log(`[${jobId}]    Using local Whisper...`);
        transcription = await runLocalWhisper(audioPath, jobId);
      } catch (whisperError) {
        console.log(`[${jobId}]    Local Whisper failed: ${whisperError.message}`);
        console.log(`[${jobId}]    Falling back to Gemini...`);
        transcription = await transcribeWithGeminiForAnimation();
      }
    } else if (openaiKey) {
      console.log(`[${jobId}]    Using OpenAI Whisper API...`);
      const audioBuffer = readFileSync(audioPath);
      const FormData = (await import('formdata-node')).FormData;
      const { Blob } = await import('buffer');

      const formData = new FormData();
      formData.append('file', new Blob([audioBuffer], { type: 'audio/mp3' }), 'audio.mp3');
      formData.append('model', 'whisper-1');
      formData.append('response_format', 'verbose_json');
      formData.append('timestamp_granularities[]', 'word');

      const whisperResponse = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: { 'Authorization': `Bearer ${openaiKey}` },
        body: formData,
      });

      if (!whisperResponse.ok) {
        throw new Error(`Whisper API error: ${whisperResponse.status}`);
      }

      const whisperResult = await whisperResponse.json();
      transcription = {
        text: whisperResult.text || '',
        words: (whisperResult.words || []).map(w => ({
          text: w.word || '',
          start: w.start || 0,
          end: w.end || 0,
        }))
      };
    } else {
      transcription = await transcribeWithGeminiForAnimation();
    }

    try { unlinkSync(audioPath); } catch {}

    console.log(`[${jobId}]    Transcript: "${transcription.text.substring(0, 100)}..."`);
    console.log(`[${jobId}]    Words: ${transcription.words?.length || 0}`);

    // Step 2: Use Gemini to identify key phrases for animation
    console.log(`[${jobId}] Step 2: Identifying key phrases...`);
    const ai = new GoogleGenAI({ apiKey });

    const analysisPrompt = `Analyze this video transcript and identify 5-8 KEY PHRASES that would make great kinetic typography animations. These should be:
- Important or impactful statements
- Keywords or product names
- Emotional or emphatic moments
- Key points the speaker is making

Transcript: "${transcription.text}"

Word timestamps: ${JSON.stringify(transcription.words?.slice(0, 100) || [])}
(Total duration: ${totalDuration}s)

Return JSON array of phrases to animate:
[
  {
    "phrase": "the exact phrase from transcript",
    "startTime": 1.5,
    "endTime": 3.2,
    "emphasis": "high|medium|low",
    "style": "bold|explosive|subtle|typewriter",
    "reason": "why this phrase is important"
  }
]

Pick phrases that are spread throughout the video. Each phrase should be 2-6 words.`;

    const analysisResponse = await ai.models.generateContent({
      model: 'gemini-2.0-flash',
      contents: [{ role: 'user', parts: [{ text: analysisPrompt }] }]
    });

    let keyPhrases = [];
    try {
      const respText = analysisResponse.text || '';
      const jsonMatch = respText.match(/\[[\s\S]*\]/);
      if (jsonMatch) {
        keyPhrases = JSON.parse(jsonMatch[0]);
      }
    } catch (e) {
      console.error(`[${jobId}] Failed to parse key phrases:`, e.message);
    }

    if (keyPhrases.length === 0) {
      // Fallback: create basic phrases from transcript chunks
      const words = transcription.words || [];
      const chunkSize = Math.ceil(words.length / 6);
      for (let i = 0; i < words.length; i += chunkSize) {
        const chunk = words.slice(i, i + chunkSize);
        if (chunk.length > 0) {
          keyPhrases.push({
            phrase: chunk.map(w => w.text).join(' ').trim(),
            startTime: chunk[0].start,
            endTime: chunk[chunk.length - 1].end,
            emphasis: 'medium',
            style: 'typewriter'
          });
        }
      }
    }

    console.log(`[${jobId}]    Found ${keyPhrases.length} key phrases`);

    // Step 3: Generate Remotion scenes for each phrase
    console.log(`[${jobId}] Step 3: Generating animation scenes...`);
    const scenes = keyPhrases.map((phrase, index) => {
      const duration = Math.max(60, Math.round((phrase.endTime - phrase.startTime + 1) * fps)); // At least 2 seconds

      // Map emphasis to visual style
      const colors = {
        high: '#f97316', // orange
        medium: '#3b82f6', // blue
        low: '#22c55e', // green
      };

      return {
        id: `text-${index}`,
        type: 'text',
        duration,
        content: {
          title: phrase.phrase.toUpperCase(),
          subtitle: null,
          color: colors[phrase.emphasis] || '#ffffff',
          backgroundColor: '#0a0a0a',
          style: phrase.style || 'typewriter',
        }
      };
    });

    // Calculate total animation duration
    const animationTotalDuration = scenes.reduce((sum, s) => sum + s.duration, 0);
    const durationInSeconds = animationTotalDuration / fps;

    console.log(`[${jobId}]    Total animation: ${animationTotalDuration} frames (${durationInSeconds}s)`);

    // Step 4: Render with Remotion
    console.log(`[${jobId}] Step 4: Rendering with Remotion...`);

    const assetId = randomUUID();
    const outputPath = join(session.assetsDir, `${assetId}.mp4`);
    const thumbPath = join(session.assetsDir, `${assetId}_thumb.jpg`);
    const propsPath = join(session.dir, `${jobId}-transcript-props.json`);

    const sceneData = {
      scenes,
      backgroundColor: '#0a0a0a',
      totalDuration: animationTotalDuration,
      contentSummary: `Kinetic typography animation from transcript: "${transcription.text.substring(0, 100)}..."`,
      keyTopics: keyPhrases.map(p => p.phrase),
    };

    // Save scene data for future editing (persistent path based on asset ID)
    const sceneDataPath = join(session.dir, `${assetId}-scenes.json`);
    writeFileSync(sceneDataPath, JSON.stringify(sceneData, null, 2));
    console.log(`[${jobId}] Scene data saved to ${sceneDataPath} for future editing`);

    writeFileSync(propsPath, JSON.stringify(sceneData, null, 2));

    const remotionArgs = [
      'remotion', 'render',
      'src/remotion/index.tsx',
      'DynamicAnimation',
      outputPath,
      '--props', propsPath,
      '--frames', `0-${animationTotalDuration - 1}`,
      '--fps', String(fps),
      '--width', String(width),
      '--height', String(height),
      '--codec', 'h264',
      '--overwrite',
      '--gl=angle', // Use Metal GPU acceleration on macOS
    ];

    console.log(`[${jobId}] Remotion command: npx ${remotionArgs.join(' ')}`);

    await new Promise((resolve, reject) => {
      const proc = spawn('npx', remotionArgs, {
        cwd: process.cwd(),
        stdio: ['pipe', 'pipe', 'pipe'],
      });

      proc.stdout.on('data', (data) => {
        console.log(`[${jobId}] Remotion: ${data.toString().trim()}`);
      });

      proc.stderr.on('data', (data) => {
        const lines = data.toString().trim().split('\n');
        lines.forEach(line => console.log(`[${jobId}] Remotion: ${line}`));
      });

      proc.on('close', (code) => {
        if (code === 0) resolve();
        else reject(new Error(`Remotion render failed with code ${code}`));
      });

      proc.on('error', (err) => reject(new Error(`Failed to start Remotion: ${err.message}`)));
    });

    // Generate thumbnail
    await runFFmpeg([
      '-y', '-i', outputPath,
      '-vf', 'scale=160:90:force_original_aspect_ratio=decrease,pad=160:90:(ow-iw)/2:(oh-ih)/2',
      '-frames:v', '1',
      thumbPath
    ], jobId);

    try { unlinkSync(propsPath); } catch {}

    const { stat } = await import('fs/promises');
    const stats = await stat(outputPath);

    // Create asset entry with scene data for future editing
    const asset = {
      id: assetId,
      type: 'video',
      filename: `transcript-animation-${Date.now()}.mp4`,
      path: outputPath,
      thumbPath: existsSync(thumbPath) ? thumbPath : null,
      duration: durationInSeconds,
      size: stats.size,
      width,
      height,
      createdAt: Date.now(),
      aiGenerated: true,
      transcriptAnimation: true,
      phraseCount: keyPhrases.length,
      sceneCount: scenes.length,
      sceneDataPath, // Store path to scene data for re-editing
      sceneData, // Also keep in memory for quick access
    };

    session.assets.set(assetId, asset);
    saveAssetMetadata(session); // Persist AI-generated flag to disk

    console.log(`[${jobId}] Transcript animation created: ${assetId}`);
    console.log(`[${jobId}] === TRANSCRIPT ANIMATION COMPLETE ===\n`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      assetId,
      filename: asset.filename,
      duration: durationInSeconds,
      phraseCount: keyPhrases.length,
      phrases: keyPhrases.map(p => p.phrase),
      thumbnailUrl: `/session/${sessionId}/assets/${assetId}/thumbnail`,
      streamUrl: `/session/${sessionId}/assets/${assetId}/stream`,
    }));

  } catch (error) {
    console.error('Transcript animation error:', error);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Generate contextual animation based on video content
// This transcribes the video first, understands what it's about, then generates relevant animation
async function handleGenerateContextualAnimation(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  const apiKey = process.env.GEMINI_API_KEY;
  if (!apiKey) {
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'GEMINI_API_KEY not configured' }));
    return;
  }

  try {
    const body = await parseBody(req);
    const { assetId, type = 'intro', description, fps = 30, width = 1920, height = 1080 } = body;

    // Get the video asset to analyze
    let videoAsset;
    if (assetId) {
      videoAsset = session.assets.get(assetId);
    } else {
      // Find the first video asset
      for (const [id, asset] of session.assets) {
        if (asset.type === 'video') {
          videoAsset = asset;
          break;
        }
      }
    }

    if (!videoAsset) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'No video asset found to analyze' }));
      return;
    }

    const jobId = randomUUID();
    const outputAssetId = randomUUID();
    const outputPath = join(session.assetsDir, `${outputAssetId}.mp4`);
    const thumbPath = join(session.assetsDir, `${outputAssetId}_thumb.jpg`);
    const propsPath = join(session.dir, `${jobId}-props.json`);
    const audioPath = join(TEMP_DIR, `${jobId}-audio.mp3`);

    console.log(`\n[${jobId}] === GENERATE CONTEXTUAL ${type.toUpperCase()} ANIMATION ===`);
    console.log(`[${jobId}] Analyzing video: ${videoAsset.filename}`);
    console.log(`[${jobId}] Type: ${type}, Description hint: ${description || 'none'}`);

    // Step 1: Transcribe the video to understand content
    console.log(`[${jobId}] Step 1: Transcribing video...`);

    // Extract audio from video
    await runFFmpeg([
      '-y', '-i', videoAsset.path,
      '-vn', '-acodec', 'libmp3lame', '-ar', '16000', '-ac', '1', '-q:a', '9',
      audioPath
    ], jobId);

    // Get video duration
    const durationOutput = await runFFmpegProbe([
      '-v', 'error',
      '-show_entries', 'format=duration',
      '-of', 'default=noprint_wrappers=1:nokey=1',
      videoAsset.path
    ], jobId);
    const totalDuration = parseFloat(durationOutput.trim()) || 60;

    let transcription;
    const hasLocalWhisper = await checkLocalWhisper();
    const openaiKey = process.env.OPENAI_API_KEY;

    // Helper for Gemini fallback in contextual animation
    const transcribeWithGeminiContextual = async () => {
      console.log(`[${jobId}]    Using Gemini for transcription...`);
      const ai = new GoogleGenAI({ apiKey });
      const audioBuffer = readFileSync(audioPath);
      const audioBase64 = audioBuffer.toString('base64');

      const result = await ai.models.generateContent({
        model: 'gemini-2.0-flash',
        contents: [{
          role: 'user',
          parts: [
            { inlineData: { mimeType: 'audio/mp3', data: audioBase64 } },
            { text: `Transcribe this audio. Return ONLY the text content, no timestamps needed. Duration: ${totalDuration.toFixed(1)}s` }
          ]
        }],
      });

      return {
        text: result.candidates[0].content.parts[0].text || '',
        words: [],
      };
    };

    if (hasLocalWhisper) {
      try {
        console.log(`[${jobId}]    Using local Whisper...`);
        transcription = await runLocalWhisper(audioPath, jobId);
      } catch (whisperError) {
        console.log(`[${jobId}]    Local Whisper failed: ${whisperError.message}`);
        console.log(`[${jobId}]    Falling back to Gemini...`);
        transcription = await transcribeWithGeminiContextual();
      }
    } else if (openaiKey) {
      console.log(`[${jobId}]    Using OpenAI Whisper API...`);
      const FormData = (await import('node-fetch')).default.FormData || global.FormData;
      const formData = new FormData();
      formData.append('file', createReadStream(audioPath));
      formData.append('model', 'whisper-1');
      formData.append('response_format', 'verbose_json');
      formData.append('timestamp_granularities[]', 'word');

      const whisperResponse = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: { 'Authorization': `Bearer ${openaiKey}` },
        body: formData,
      });

      if (!whisperResponse.ok) {
        throw new Error(`Whisper API error: ${whisperResponse.status}`);
      }

      const whisperResult = await whisperResponse.json();
      transcription = {
        text: whisperResult.text || '',
        words: (whisperResult.words || []).map(w => ({
          text: w.word,
          start: w.start,
          end: w.end,
        })),
      };
    } else {
      transcription = await transcribeWithGeminiContextual();
    }

    console.log(`[${jobId}] Transcription complete: ${transcription.text.substring(0, 100)}...`);

    // Clean up audio file
    try { unlinkSync(audioPath); } catch (e) {}

    // Step 2: Analyze content and generate contextual scene data
    console.log(`[${jobId}] Step 2: Analyzing content and generating scenes...`);

    const genAI = new GoogleGenAI({ apiKey });

    const typePrompts = {
      intro: `Create an engaging INTRO animation that hooks viewers and introduces the video topic.
The intro should:
- Start with an attention-grabbing title or hook
- Tease what viewers will learn/see
- Build excitement for the content
- Be 4-8 seconds (120-240 frames at 30fps)`,

      outro: `Create a compelling OUTRO animation that wraps up the video.
The outro should:
- Summarize key takeaways
- Include a call-to-action (subscribe, like, etc.)
- Thank viewers
- Be 5-10 seconds (150-300 frames at 30fps)`,

      transition: `Create a smooth TRANSITION animation between sections.
The transition should:
- Be brief and visually interesting
- Match the video's tone
- Be 2-4 seconds (60-120 frames at 30fps)`,

      highlight: `Create a HIGHLIGHT animation that emphasizes a key moment.
The highlight should:
- Draw attention to an important point
- Use dynamic motion and colors
- Be 3-6 seconds (90-180 frames at 30fps)`,
    };

    const scenePrompt = `You are a motion graphics designer. Analyze this video transcript and create a contextual ${type} animation.

VIDEO TRANSCRIPT:
"${transcription.text}"

${description ? `USER HINT: "${description}"` : ''}

${typePrompts[type] || typePrompts.intro}

Based on the video content above, return ONLY valid JSON (no markdown) with this structure:
{
  "scenes": [
    {
      "id": "unique-id",
      "type": "title" | "steps" | "features" | "stats" | "text" | "transition",
      "duration": <frames at 30fps>,
      "content": {
        "title": "text derived from video content",
        "subtitle": "optional",
        "items": [{"icon": "emoji", "label": "text", "description": "optional"}],
        "stats": [{"value": "number", "label": "text"}],
        "color": "#hex accent color",
        "backgroundColor": "#hex or null for transparent"
      }
    }
  ],
  "backgroundColor": "#0a0a0a",
  "totalDuration": <sum of scene durations>,
  "contentSummary": "brief description of what the video is about"
}

IMPORTANT: The animation content should directly relate to the video's actual topic and message.
Use specific terms, concepts, and themes from the transcript.`;

    const sceneResult = await genAI.models.generateContent({
      model: 'gemini-2.0-flash',
      contents: [{ role: 'user', parts: [{ text: scenePrompt }] }],
    });

    let sceneData;
    try {
      const responseText = sceneResult.candidates[0].content.parts[0].text;
      const cleanedResponse = responseText
        .replace(/```json\n?/g, '')
        .replace(/```\n?/g, '')
        .trim();
      sceneData = JSON.parse(cleanedResponse);
    } catch (parseError) {
      console.error(`[${jobId}] Failed to parse Gemini response:`, parseError);
      throw new Error('Failed to parse AI-generated scene data');
    }

    console.log(`[${jobId}] Generated ${sceneData.scenes.length} scenes for ${type}`);
    console.log(`[${jobId}] Content summary: ${sceneData.contentSummary || 'N/A'}`);

    // Log camera movements for debugging
    const scenesWithCamera = sceneData.scenes.filter(s => s.content?.camera?.type);
    if (scenesWithCamera.length > 0) {
      console.log(`[${jobId}] ðŸŽ¥ Camera movements: ${scenesWithCamera.map(s => `${s.id}: ${s.content.camera.type}`).join(', ')}`);
    } else {
      console.log(`[${jobId}] âš ï¸ No camera movements in any scene`);
    }

    const animationTotalDuration = sceneData.totalDuration || sceneData.scenes.reduce((sum, s) => sum + s.duration, 0);
    const durationInSeconds = animationTotalDuration / fps;

    // Save scene data for future editing (persistent path based on asset ID)
    const sceneDataPath = join(session.dir, `${outputAssetId}-scenes.json`);
    writeFileSync(sceneDataPath, JSON.stringify(sceneData, null, 2));
    console.log(`[${jobId}] Scene data saved to ${sceneDataPath} for future editing`);

    // Step 3: Write props and render with Remotion
    console.log(`[${jobId}] Step 3: Rendering with Remotion...`);

    writeFileSync(propsPath, JSON.stringify(sceneData, null, 2));

    const remotionArgs = [
      'remotion', 'render',
      'src/remotion/index.tsx',
      'DynamicAnimation',
      outputPath,
      '--props', propsPath,
      '--frames', `0-${animationTotalDuration - 1}`,
      '--fps', String(fps),
      '--width', String(width),
      '--height', String(height),
      '--codec', 'h264',
      '--overwrite',
      '--gl=angle', // Use Metal GPU acceleration on macOS
    ];

    await new Promise((resolve, reject) => {
      const proc = spawn('npx', remotionArgs, {
        cwd: process.cwd(),
        stdio: ['pipe', 'pipe', 'pipe'],
      });

      let stdout = '';
      let stderr = '';

      proc.stdout.on('data', (data) => {
        stdout += data.toString();
        console.log(`[${jobId}] Remotion stdout: ${data.toString().trim()}`);
      });

      proc.stderr.on('data', (data) => {
        stderr += data.toString();
        const lines = data.toString().trim().split('\n');
        lines.forEach(line => {
          console.log(`[${jobId}] Remotion: ${line}`);
        });
      });

      proc.on('close', (code) => {
        if (code === 0) resolve();
        else {
          console.error(`[${jobId}] Remotion failed. stderr: ${stderr.slice(-1000)}`);
          reject(new Error(`Remotion render failed with code ${code}: ${stderr.slice(-500)}`));
        }
      });

      proc.on('error', (err) => reject(new Error(`Failed to start Remotion: ${err.message}`)));
    });

    // Step 4: Generate thumbnail
    await runFFmpeg([
      '-y', '-i', outputPath,
      '-vf', 'scale=160:90:force_original_aspect_ratio=decrease,pad=160:90:(ow-iw)/2:(oh-ih)/2',
      '-frames:v', '1',
      thumbPath
    ], jobId);

    // Clean up
    try { unlinkSync(propsPath); } catch (e) {}

    const { stat } = await import('fs/promises');
    const stats = await stat(outputPath);

    // Create asset entry
    // Create asset entry with scene data for future editing
    const asset = {
      id: outputAssetId,
      type: 'video',
      filename: `${type}-animation-${Date.now()}.mp4`,
      path: outputPath,
      thumbPath: existsSync(thumbPath) ? thumbPath : null,
      duration: durationInSeconds,
      size: stats.size,
      width,
      height,
      createdAt: Date.now(),
      // Metadata
      aiGenerated: true,
      contextual: true,
      animationType: type,
      contentSummary: sceneData.contentSummary,
      sceneCount: sceneData.scenes.length,
      sourceAssetId: videoAsset.id,
      sceneDataPath, // Store path to scene data for re-editing
      sceneData, // Also keep in memory for quick access
    };

    session.assets.set(outputAssetId, asset);

    console.log(`[${jobId}] Contextual ${type} animation rendered: ${outputAssetId} (${durationInSeconds}s)`);
    console.log(`[${jobId}] === CONTEXTUAL ANIMATION COMPLETE ===\n`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      assetId: outputAssetId,
      filename: asset.filename,
      duration: durationInSeconds,
      type,
      contentSummary: sceneData.contentSummary,
      sceneCount: sceneData.scenes.length,
      thumbnailUrl: `/session/${sessionId}/assets/${outputAssetId}/thumbnail`,
      streamUrl: `/session/${sessionId}/assets/${outputAssetId}/stream`,
    }));

  } catch (error) {
    console.error('Contextual animation generation error:', error);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Extract audio from video - creates separate audio asset and mutes the video
async function handleExtractAudio(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  try {
    const body = await parseBody(req);
    const { assetId } = body;

    if (!assetId) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'assetId is required' }));
      return;
    }

    const videoAsset = session.assets.get(assetId);
    if (!videoAsset) {
      res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'Asset not found' }));
      return;
    }

    if (videoAsset.type !== 'video') {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'Asset must be a video' }));
      return;
    }

    const jobId = sessionId.substring(0, 8);
    console.log(`\n[${jobId}] === EXTRACT AUDIO ===`);
    console.log(`[${jobId}] Source video: ${videoAsset.filename}`);

    // Generate IDs and paths
    const audioAssetId = randomUUID();
    const mutedVideoAssetId = randomUUID();
    const audioPath = join(session.assetsDir, `${audioAssetId}.mp3`);
    const mutedVideoPath = join(session.assetsDir, `${mutedVideoAssetId}.mp4`);
    const mutedThumbPath = join(session.assetsDir, `${mutedVideoAssetId}_thumb.jpg`);

    // Step 1: Extract audio from video
    console.log(`[${jobId}] Step 1: Extracting audio...`);
    await runFFmpeg([
      '-y', '-i', videoAsset.path,
      '-vn',                    // No video
      '-acodec', 'libmp3lame',  // MP3 codec
      '-q:a', '2',              // High quality
      audioPath
    ], jobId);

    // Step 2: Create muted version of video
    console.log(`[${jobId}] Step 2: Creating muted video...`);
    await runFFmpeg([
      '-y', '-i', videoAsset.path,
      '-an',                    // No audio
      '-c:v', 'copy',           // Copy video stream (fast)
      mutedVideoPath
    ], jobId);

    // Step 3: Generate thumbnail for muted video
    try {
      await runFFmpeg([
        '-y', '-i', mutedVideoPath,
        '-vf', 'scale=160:90:force_original_aspect_ratio=decrease,pad=160:90:(ow-iw)/2:(oh-ih)/2',
        '-frames:v', '1',
        mutedThumbPath
      ], jobId);
    } catch (e) {
      console.warn(`[${jobId}] Thumbnail generation failed:`, e.message);
    }

    // Get file stats
    const { stat } = await import('fs/promises');
    const audioStats = await stat(audioPath);
    const videoStats = await stat(mutedVideoPath);

    // Get audio duration
    let audioDuration = videoAsset.duration;
    try {
      const ffprobePath = process.env.FFPROBE_PATH || 'ffprobe';
      const durationStr = execSync(
        `"${ffprobePath}" -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 "${audioPath}"`,
        { encoding: 'utf-8' }
      ).trim();
      audioDuration = parseFloat(durationStr) || videoAsset.duration;
    } catch (e) {
      console.warn(`[${jobId}] Could not get audio duration:`, e.message);
    }

    // Create audio asset
    const audioAsset = {
      id: audioAssetId,
      type: 'audio',
      filename: `${videoAsset.filename.replace(/\.[^.]+$/, '')}-audio.mp3`,
      path: audioPath,
      thumbPath: null,
      duration: audioDuration,
      size: audioStats.size,
      createdAt: Date.now(),
      sourceAssetId: assetId,
    };
    session.assets.set(audioAssetId, audioAsset);

    // Create muted video asset
    const mutedAsset = {
      id: mutedVideoAssetId,
      type: 'video',
      filename: `${videoAsset.filename.replace(/\.[^.]+$/, '')}-muted.mp4`,
      path: mutedVideoPath,
      thumbPath: existsSync(mutedThumbPath) ? mutedThumbPath : videoAsset.thumbPath,
      duration: videoAsset.duration,
      size: videoStats.size,
      width: videoAsset.width || 1920,
      height: videoAsset.height || 1080,
      createdAt: Date.now(),
      sourceAssetId: assetId,
      isMuted: true,
    };
    session.assets.set(mutedVideoAssetId, mutedAsset);

    console.log(`[${jobId}] âœ“ Audio extracted: ${audioAsset.filename} (${audioDuration.toFixed(2)}s)`);
    console.log(`[${jobId}] âœ“ Muted video created: ${mutedAsset.filename}`);
    console.log(`[${jobId}] === EXTRACT AUDIO COMPLETE ===\n`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      audioAsset: {
        id: audioAssetId,
        filename: audioAsset.filename,
        duration: audioDuration,
        type: 'audio',
        streamUrl: `/session/${sessionId}/assets/${audioAssetId}/stream`,
      },
      mutedVideoAsset: {
        id: mutedVideoAssetId,
        filename: mutedAsset.filename,
        duration: mutedAsset.duration,
        type: 'video',
        streamUrl: `/session/${sessionId}/assets/${mutedVideoAssetId}/stream`,
        thumbnailUrl: `/session/${sessionId}/assets/${mutedVideoAssetId}/thumbnail`,
      },
      originalAssetId: assetId,
    }));

  } catch (error) {
    console.error('Extract audio error:', error);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// Process asset with FFmpeg command (for AI-suggested edits)
async function handleProcessAsset(req, res, sessionId) {
  const session = getSession(sessionId);
  if (!session) {
    res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: 'Session not found' }));
    return;
  }

  try {
    const body = await parseBody(req);
    const { assetId, command } = body;

    if (!assetId || !command) {
      res.writeHead(400, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'assetId and command are required' }));
      return;
    }

    const asset = session.assets.get(assetId);
    if (!asset) {
      res.writeHead(404, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({ error: 'Asset not found' }));
      return;
    }

    // Verify the asset file actually exists on disk
    if (!existsSync(asset.path)) {
      console.error(`[ProcessAsset] Asset file missing: ${asset.path}`);
      res.writeHead(410, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
      res.end(JSON.stringify({
        error: 'Asset file no longer exists. The session may have expired. Please re-upload your video.',
        code: 'ASSET_FILE_MISSING'
      }));
      return;
    }

    const jobId = randomUUID();
    const newAssetId = randomUUID();
    const outputPath = join(session.assetsDir, `${newAssetId}.mp4`);
    const thumbPath = join(session.assetsDir, `${newAssetId}_thumb.jpg`);

    console.log(`\n[${jobId}] === PROCESS ASSET WITH FFMPEG ===`);
    console.log(`[${jobId}] Source: ${asset.filename}`);
    console.log(`[${jobId}] Command: ${command}`);

    // Parse the FFmpeg command and replace input/output placeholders
    // Expected format: "ffmpeg -i input.mp4 [options] output.mp4"
    // We'll replace input.mp4 with actual path and output.mp4 with new path
    let ffmpegArgs = command
      .replace(/^ffmpeg\s+/, '') // Remove 'ffmpeg' prefix
      .replace(/input\.mp4|"input\.mp4"/gi, `"${asset.path}"`)
      .replace(/output\.mp4|"output\.mp4"/gi, `"${outputPath}"`)
      .split(/\s+/)
      .filter(arg => arg.length > 0);

    // If the command doesn't have proper input/output, construct a basic one
    if (!ffmpegArgs.some(arg => arg.includes(asset.path))) {
      // Reconstruct with proper input
      ffmpegArgs = ['-y', '-i', asset.path, ...ffmpegArgs.filter(a => a !== '-i'), outputPath];
    }

    // Ensure -y flag for overwrite
    if (!ffmpegArgs.includes('-y')) {
      ffmpegArgs.unshift('-y');
    }

    console.log(`[${jobId}] FFmpeg args:`, ffmpegArgs);

    await runFFmpeg(ffmpegArgs, jobId);

    // Generate thumbnail
    await runFFmpeg([
      '-y', '-i', outputPath,
      '-vf', 'scale=160:90:force_original_aspect_ratio=decrease,pad=160:90:(ow-iw)/2:(oh-ih)/2',
      '-frames:v', '1',
      thumbPath
    ], jobId);

    // Get video info
    const { stat } = await import('fs/promises');
    const stats = await stat(outputPath);

    // Get duration with ffprobe
    let duration = asset.duration;
    try {
      const ffprobePath = process.env.FFPROBE_PATH || 'ffprobe';
      const durationStr = execSync(
        `"${ffprobePath}" -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 "${outputPath}"`,
        { encoding: 'utf-8' }
      ).trim();
      duration = parseFloat(durationStr) || asset.duration;
    } catch (e) {
      console.warn(`[${jobId}] Could not get duration:`, e.message);
    }

    // Create new asset entry
    const newAsset = {
      id: newAssetId,
      type: 'video',
      filename: `edited-${asset.filename}`,
      path: outputPath,
      thumbPath: existsSync(thumbPath) ? thumbPath : null,
      duration,
      size: stats.size,
      width: asset.width || 1920,
      height: asset.height || 1080,
      createdAt: Date.now(),
      // Metadata
      sourceAssetId: assetId,
      ffmpegCommand: command,
    };

    session.assets.set(newAssetId, newAsset);

    console.log(`[${jobId}] Asset processed: ${newAssetId} (${duration.toFixed(2)}s)`);
    console.log(`[${jobId}] === PROCESSING COMPLETE ===\n`);

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({
      success: true,
      assetId: newAssetId,
      filename: newAsset.filename,
      duration,
      thumbnailUrl: `/session/${sessionId}/assets/${newAssetId}/thumbnail`,
      streamUrl: `/session/${sessionId}/assets/${newAssetId}/stream`,
    }));

  } catch (error) {
    console.error('Process asset error:', error);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// ============== SETTINGS HANDLERS ==============

// Get settings (API key status)
function handleSettingsGet(req, res) {
  const settings = {
    OPENAI_API_KEY: !!process.env.OPENAI_API_KEY,
    GEMINI_API_KEY: !!process.env.GEMINI_API_KEY,
    GIPHY_API_KEY: !!process.env.GIPHY_API_KEY,
    FAL_KEY: !!(process.env.FAL_KEY || process.env.FAL_API_KEY),
  };

  res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
  res.end(JSON.stringify(settings));
}

// Save settings
async function handleSettingsSave(req, res) {
  try {
    const body = await parseBody(req);
    // Only update keys that are explicitly provided in the request
    const updates = {};
    if (body.OPENAI_API_KEY !== undefined) updates.OPENAI_API_KEY = body.OPENAI_API_KEY;
    if (body.GEMINI_API_KEY !== undefined) updates.GEMINI_API_KEY = body.GEMINI_API_KEY;
    if (body.GIPHY_API_KEY !== undefined) updates.GIPHY_API_KEY = body.GIPHY_API_KEY;
    if (body.FAL_KEY !== undefined) {
      updates.FAL_KEY = body.FAL_KEY;
      updates.FAL_API_KEY = body.FAL_KEY; // Sync alias
    }

    // Update process.env
    for (const [key, value] of Object.entries(updates)) {
      if (value) {
        process.env[key] = value;
      } else {
        delete process.env[key];
      }
    }

    // Persist to .dev.vars in writable user data directory
    const envPath = join(USER_DATA_DIR, '.dev.vars');
    let envContent = '';

    if (existsSync(envPath)) {
      envContent = readFileSync(envPath, 'utf-8');
    }

    // Parse existing lines to preserve comments and structure
    let lines = envContent.split('\n');
    const existingKeys = new Set();
    const newLines = [];

    // Process existing lines
    for (const line of lines) {
      const match = line.match(/^\s*([^=#]+)\s*=(.*)$/);
      if (match) {
        const key = match[1].trim();
        existingKeys.add(key);

        // If this key is being updated
        if (updates[key] !== undefined) {
          if (updates[key]) {
            newLines.push(`${key}=${updates[key]}`);
          } else {
            // If value is empty/null, remove the key (don't push line)
          }
        } else {
          // Keep existing line
          newLines.push(line);
        }
      } else {
        // Keep comments/empty lines
        newLines.push(line);
      }
    }

    // Append new keys that weren't in the file
    for (const [key, value] of Object.entries(updates)) {
      if (value && !existingKeys.has(key)) {
        // Don't duplicate FAL_API_KEY if FAL_KEY is there (or vice versa) if desired,
        // but explicit saving is safer.
        newLines.push(`${key}=${value}`);
      }
    }

    // Remove trailing newlines and join
    const finalContent = newLines.join('\n').replace(/\n+$/, '') + '\n';
    writeFileSync(envPath, finalContent);
    console.log('[Settings] Updated .dev.vars');

    res.writeHead(200, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ success: true }));

  } catch (error) {
    console.error('Settings save error:', error);
    res.writeHead(500, { 'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*' });
    res.end(JSON.stringify({ error: error.message }));
  }
}

// ============== SERVER ==============

const server = http.createServer(async (req, res) => {
  // CORS headers
  res.setHeader('Access-Control-Allow-Origin', '*');
  res.setHeader('Access-Control-Allow-Methods', 'GET, POST, PUT, DELETE, OPTIONS');
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type');
  res.setHeader('Access-Control-Expose-Headers', 'Content-Disposition');

  if (req.method === 'OPTIONS') {
    res.writeHead(204);
    res.end();
    return;
  }

  const url = new URL(req.url, `http://localhost:${PORT}`);
  const path = url.pathname;

  // Session-based routes (new efficient API)
  const sessionMatch = path.match(/^\/session\/([^/]+)(\/(.+))?$/);
  if (sessionMatch) {
    const sessionId = sessionMatch[1];
    const action = sessionMatch[3] || '';

    if (req.method === 'POST' && sessionId === 'create') {
      await handleSessionCreate(req, res);
    } else if (req.method === 'POST' && sessionId === 'upload') {
      await handleSessionUpload(req, res);
    } else if (req.method === 'GET' && action === 'stream') {
      await handleSessionStream(req, res, sessionId);
    } else if (req.method === 'GET' && action === 'info') {
      await handleSessionInfo(req, res, sessionId);
    } else if (req.method === 'GET' && action === 'download') {
      await handleSessionDownload(req, res, sessionId);
    } else if (req.method === 'POST' && action === 'process') {
      await handleSessionProcess(req, res, sessionId);
    } else if (req.method === 'POST' && action === 'remove-dead-air') {
      await handleSessionRemoveDeadAir(req, res, sessionId);
    } else if (req.method === 'POST' && action === 'chapters') {
      await handleSessionChapters(req, res, sessionId);
    } else if (req.method === 'DELETE' && !action) {
      handleSessionDelete(req, res, sessionId);
    }
    // Multi-asset endpoints
    else if (req.method === 'POST' && action === 'assets') {
      await handleAssetUpload(req, res, sessionId);
    } else if (req.method === 'GET' && action === 'assets') {
      handleAssetList(req, res, sessionId);
    } else if (action.startsWith('assets/')) {
      const assetPath = action.substring(7); // Remove 'assets/'
      const [assetId, subAction] = assetPath.split('/');

      if (req.method === 'DELETE' && !subAction) {
        handleAssetDelete(req, res, sessionId, assetId);
      } else if (req.method === 'GET' && subAction === 'thumbnail') {
        await handleAssetThumbnail(req, res, sessionId, assetId);
      } else if (req.method === 'GET' && subAction === 'stream') {
        await handleAssetStream(req, res, sessionId, assetId);
      } else {
        res.writeHead(404, { 'Content-Type': 'application/json' });
        res.end(JSON.stringify({ error: 'Asset endpoint not found' }));
      }
    }
    // Project state endpoints
    else if (req.method === 'GET' && action === 'project') {
      handleProjectGet(req, res, sessionId);
    } else if (req.method === 'PUT' && action === 'project') {
      await handleProjectSave(req, res, sessionId);
    }
    // Render endpoints
    else if (req.method === 'POST' && action === 'render') {
      await handleProjectRender(req, res, sessionId);
    }
    // GIF creation
    else if (req.method === 'POST' && action === 'create-gif') {
      await handleCreateGif(req, res, sessionId);
    }
    // Simple transcription (for captions)
    else if (req.method === 'POST' && action === 'transcribe') {
      await handleTranscribe(req, res, sessionId);
    }
    // Transcription and keyword extraction
    else if (req.method === 'POST' && action === 'transcribe-and-extract') {
      await handleTranscribeAndExtract(req, res, sessionId);
    }
    // B-roll image generation
    else if (req.method === 'POST' && action === 'generate-broll') {
      await handleGenerateBroll(req, res, sessionId);
    }
    // Motion graphics rendering (placeholder - creates solid color video for now)
    else if (req.method === 'POST' && action === 'render-motion-graphic') {
      await handleRenderMotionGraphic(req, res, sessionId);
    }
    // AI-generated custom animation (uses Gemini + Remotion)
    else if (req.method === 'POST' && action === 'generate-animation') {
      await handleGenerateAnimation(req, res, sessionId);
    }
    // Analyze video for animation (returns concept for approval, no rendering)
    else if (req.method === 'POST' && action === 'analyze-for-animation') {
      await handleAnalyzeForAnimation(req, res, sessionId);
    }
    // Render from pre-approved concept (skips analysis)
    else if (req.method === 'POST' && action === 'render-from-concept') {
      await handleRenderFromConcept(req, res, sessionId);
    }
    // Contextual animation - analyzes video content first, then generates relevant animation
    else if (req.method === 'POST' && action === 'generate-contextual-animation') {
      await handleGenerateContextualAnimation(req, res, sessionId);
    }
    // Transcript animation - kinetic typography from speech
    else if (req.method === 'POST' && action === 'generate-transcript-animation') {
      await handleGenerateTranscriptAnimation(req, res, sessionId);
    }
    // Edit existing animation with new prompt
    else if (req.method === 'POST' && action === 'edit-animation') {
      await handleEditAnimation(req, res, sessionId);
    }
    // Generate image with fal.ai (Picasso agent)
    else if (req.method === 'POST' && action === 'generate-image') {
      await handleGenerateImage(req, res, sessionId);
    }
    // Generate batch animations across timeline
    else if (req.method === 'POST' && action === 'generate-batch-animations') {
      await handleGenerateBatchAnimations(req, res, sessionId);
    }
    // Process asset with FFmpeg command
    else if (req.method === 'POST' && action === 'process-asset') {
      await handleProcessAsset(req, res, sessionId);
    }
    // Extract audio from video (creates audio asset + muted video)
    else if (req.method === 'POST' && action === 'extract-audio') {
      await handleExtractAudio(req, res, sessionId);
    }
    // Generate video from image (DiCaprio agent)
    else if (req.method === 'POST' && action === 'generate-video') {
      await handleGenerateVideo(req, res, sessionId);
    }
    // Restyle video with AI (DiCaprio agent - LTX-2)
    else if (req.method === 'POST' && action === 'restyle-video') {
      await handleRestyleVideo(req, res, sessionId);
    }
    // Remove video background (DiCaprio agent - Bria)
    else if (req.method === 'POST' && action === 'remove-video-bg') {
      await handleRemoveVideoBg(req, res, sessionId);
    }
    // GIPHY search endpoints
    else if (req.method === 'GET' && action === 'giphy/search') {
      await handleGiphySearch(req, res, sessionId, url);
    }
    else if (req.method === 'GET' && action === 'giphy/trending') {
      await handleGiphyTrending(req, res, sessionId, url);
    }
    else if (req.method === 'POST' && action === 'giphy/add') {
      await handleGiphyAdd(req, res, sessionId);
    }
    else if (action.startsWith('renders/')) {
      const renderType = action.substring(8); // Remove 'renders/'
      if (req.method === 'GET') {
        await handleRenderDownload(req, res, sessionId, renderType);
      } else {
        res.writeHead(404, { 'Content-Type': 'application/json' });
        res.end(JSON.stringify({ error: 'Render endpoint not found' }));
      }
    }
    else {
      res.writeHead(404, { 'Content-Type': 'application/json' });
      res.end(JSON.stringify({ error: 'Session endpoint not found' }));
    }
    return;
  }

  // Settings API
  if (req.method === 'GET' && path === '/settings') {
    handleSettingsGet(req, res);
    return;
  } else if (req.method === 'POST' && path === '/settings') {
    await handleSettingsSave(req, res);
    return;
  }

  // Legacy routes (kept for backwards compatibility)
  if (req.method === 'POST' && path === '/process') {
    await handleProcess(req, res);
  } else if (req.method === 'POST' && path === '/remove-dead-air') {
    await handleRemoveDeadAir(req, res);
  } else if (req.method === 'POST' && path === '/generate-chapters') {
    await handleGenerateChapters(req, res);
  } else if (req.method === 'GET' && path === '/health') {
    res.writeHead(200, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({ status: 'ok', ffmpeg: 'native', sessions: sessions.size }));
  } else {
    res.writeHead(404, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({ error: 'Not found' }));
  }
});

server.listen(PORT, () => {
  console.log(`\nðŸŽ¬ Local FFmpeg server running at http://localhost:${PORT}`);
  console.log(`\n   Session API:`);
  console.log(`   POST /session/upload - Upload video, get sessionId`);
  console.log(`   GET  /session/:id/stream - Stream video for preview`);
  console.log(`   GET  /session/:id/info - Get video info`);
  console.log(`   POST /session/:id/process - Apply FFmpeg edit`);
  console.log(`   POST /session/:id/remove-dead-air - Remove silence`);
  console.log(`   POST /session/:id/chapters - Generate chapters`);
  console.log(`   GET  /session/:id/download - Download final video`);
  console.log(`   DELETE /session/:id - Clean up session`);
  console.log(`\n   Multi-Asset API:`);
  console.log(`   POST /session/:id/assets - Upload asset (video/image/audio)`);
  console.log(`   GET  /session/:id/assets - List all assets`);
  console.log(`   DELETE /session/:id/assets/:assetId - Delete asset`);
  console.log(`   GET  /session/:id/assets/:assetId/thumbnail - Get thumbnail`);
  console.log(`   GET  /session/:id/assets/:assetId/stream - Stream asset`);
  console.log(`\n   Project API:`);
  console.log(`   GET  /session/:id/project - Get project state`);
  console.log(`   PUT  /session/:id/project - Save project state`);
  console.log(`   POST /session/:id/render - Render project to video`);
  console.log(`   GET  /session/:id/renders/preview - Download preview`);
  console.log(`   GET  /session/:id/renders/export - Download export`);
  console.log(`\n   AI/Auto GIF API:`);
  console.log(`   POST /session/:id/transcribe-and-extract - Transcribe video, extract keywords, fetch GIFs`);
  console.log(`   POST /session/:id/generate-broll - Generate AI B-roll images from transcript`);
  console.log(`   POST /session/:id/generate-animation - AI-generated custom animation (Gemini + Remotion)`);
  console.log(`   POST /session/:id/analyze-for-animation - Analyze video, return concept for approval`);
  console.log(`   POST /session/:id/generate-contextual-animation - Content-aware animation (transcribes video first)`);
  console.log(`   POST /session/:id/process-asset - Apply FFmpeg command to an asset`);
  console.log(`\n   GET /health - Health check\n`);
});
